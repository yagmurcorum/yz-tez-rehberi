{"content": "T.C. EGE ÜNİVERSİTESİ Fen Fakültesi Matematik Anabilim Dalı Adı İzmir 2025 Yağmur ÇORUM YAPAY ZEKÂ DİL MODELLERİ Lisans Tezi", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 1, "page_end": 1}}
{"content": "T.C. EGE ÜNİVERSİTESİ Fen Fakültesi YAPAY ZEKÂ DİL MODELLERİ Yağmur ÇORUM Danışman: Prof. Dr. Burak ORDİN İzmir 2025 Matematik Anabilim Dalı Adı", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 2, "page_end": 2}}
{"content": "iii ÖZET YAPAY ZEKÂ DİL MODELLERİ ÇORUM, Yağmur Lisans Tezi, Matematik Anabilim Dalı Tez Danışmanı: Prof. Dr. Burak ORDİN Temmuz 2025, 114 sayfa Bu tezde, yapay zekâ ve doğal dil işleme alanındaki güncel gelişmeler ve dil modelleme yöntemleri kapsamlı bir şekilde incelenmektedir. Literatür taraması yöntemi kullanılarak, yapay zekânın tanımı, tarihçesi ve doğal dil işlemenin evrimi sistemli bir şekilde ele alınmıştır. Makine öğrenimi ve derin öğrenmenin temel prensipleri, denetimli, denetimsiz ve takviyeli öğrenme yaklaşımları açıklanmıştır. Dil modellerinin başarısında önemli rol oynayan yapay sinir ağları ve derin öğrenme mimarileri de incelenmiştir. Dil modellerinin kullanım alanları detaylandırılmış; Transformer mimarisinin önceki mimarilerin sınırlamalarını nasıl aştığı açıklanmıştır. Popüler dil modellerinden GPT ve BERT modellerinin hem mimari yapıları hem de belli görevlere özgü performansları karşılaştırılmıştır. Ayrıca, çok modlu büyük dil modelleri ve çok ajanlı sistemler gibi güncel yönelimler ile yapay zekâ dil modellerinin etik sorunları da ele alınmıştır. Sonuç olarak, tezde kapsamlı bir literatür incelemesiyle yapay zekâ ve dil modellerinin mevcut durumu ile gelecekteki araştırma ve uygulama fırsatları değerlendirilmiştir. Anahtar Sözcükler : Yapay zekâ, doğal dil işleme, dil modelleme, Transformer Mimarisi, çok modlu modeller,çok ajanlı sistemler", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 3, "page_end": 3}}
{"content": "iv ABSTRACT ARTIFICIAL INTELLIGENCE LANGUAGE MODELS ÇORUM, Yağmur BSc in Mathematics Supervisor: Prof. Dr. Burak ORDİN July 2025, 114 pages This thesis comprehensively examines recent developments in artificial intelligence, natural language processing, and language modeling methods. A literature review is used to systemically discuss the definition and history of artificial intelligence, as well as the evolution of natural language processing. The basic principles of machine learning, deep learning, and the supervised, unsupervised, and reinforcement learning approaches are explained. The significant role of neural networks and deep learning architectures in the success of language models is also analyzed. The thesis details the use cases of language models and explains how the Transformer architecture overcomes the limitations of previous architectures. Two popular language models, GPT and BERT, are compared in terms of their architectural structures and performance on specific tasks. Additionally, the thesis discusses current trends such as multimodal large language models and multi-agent systems, as well as the ethical issues surrounding AI language models. In conclusion, this thesis provides a comprehensive literature review assessing in the current state of AI and language models, as well as future research and application opportunities. Keywords: Artificial intelligence, natural language processing, language modeling, Transformer architecture, multimodal models, multi-agent systems", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 4, "page_end": 4}}
{"content": "v İÇİNDEKİLER Sayfa İÇ KAPAK ..................................................................................................................... ii ÖZET .............................................................................................................................. iii ABSTRACT ................................................................................................................... iv İÇİNDEKİLER .............................................................................................................. v ŞEKİLLER DİZİNİ ....................................................................................................... viii TABLOLAR DİZİNİ ....................................................................................................... x SİMGELER VE KISALTMALAR DİZİNİ ................................................................... xi 1.GİRİŞ ............................................................................................................................. 1 2.YAPAY ZEKÂ VE DOĞAL DİL İŞLEME ................................................................. 2 2.1 Yapay Zekâ Nedir? ........................................................................................... 2 2.2 Yapay Zekânın Tarihçesi .................................................................................. 3 2.3 Doğal Dil İşleme (Natural Language Processing-NLP) ve Evrimi .................. 7 2.3.1 Doğal Dil İşleme (Natural Language Processing-NLP) ................................... 7 2.3.2 Doğal Dil İşlemenin Evrimi ............................................................................. 8 2.3.3 Doğal Dil İşleme Teknikleri ........................................................................... 10 3.DİL MODELLEMEDE MAKİNE ÖĞRENİMİ VE DERİN ÖĞRENME ................. 15 3.1 Makine Öğrenimi: Temel Kavramlar ve Çalışma Prensipleri ........................ 15 3.1.1 Denetimli Öğrenme (Supervised Learning) ................................................... 17 3.1.2 Denetimsiz Öğrenme (Unsupervised Learning) ............................................. 20 3.1.3 Takviyeli Öğrenme (Reinforcement Learning) .............................................. 21 3.2 Derin Öğrenme: Yapay Sinir Ağları ve Derin Öğrenme Mimarileri .............. 22", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 5, "page_end": 5}}
{"content": "vi 3.2.1 Yapay Sinir Ağları ( Artificial Neural Networks- ANNs) Türleri ................. 23 3.2.1 Derin Öğrenme Mimarileri ............................................................................. 25 3.3 Makine Öğrenimi ve Derin Öğrenmenin Karşılaştırılması ............................ 29 4.DİL MODELLERİ ...................................................................................................... 31 4.1 Dil Modellemenin Tanımı ve Önemi.............................................................. 31 4.2 Dil Modelleri ve Kullanım Alanları ............................................................... 32 4.2.1 Dil Modelleri (Language Models) ............................................................... 33 4.2.2 Dil Modellerinin Kullanım Alanları ............................................................... 36 5. TRANSFORMER (DÖNÜŞTÜRÜCÜ) TABANLI MODELLER ........................... 41 5.1 RNN, LSTM ve GRU Mimarilerinde Karşılaşılan Sorunlar .......................... 41 5.2 Transformer Mimarisinin Temel Yapıtaşları ve Getirdiği Yenilikler ............ 43 5.2.1 Transformer Mimarisin Temel Yapıtaşları ..................................................... 45 5.2.2 Transformer Mimarisinin Getirdiği Yenilikler ............................................... 48 5.3 GPT ve BERT Modelleri ................................................................................ 51 5.3.1 GPT ve Geliştirilmiş Versiyonları ................................................................. 51 5.3.2 BERT ve Geliştirilmiş Versiyonları ............................................................... 54 5.3.3 GPT ve BERT Modellerinin Karşılaştırılması ............................................... 58 5.3.3.1 GPT ve BERT Modellerinin Eğitim ve Mimari Yaklaşımları ....................... 58 5.3.3.2 GPT ve BERT Modellerinin Görev Performansları ve Uygulama Alanları .. 60 5.3.3.3 GPT ve BERT Modellerinin Karşılaştırmalı Değerlendirilmesi ve Sonuç .... 68 6.DİL MODELLERİNDE GÜNCEL YÖNELİMLER VE ETİK BOYUTLAR ........... 71 6.1 Dil Modellerinde Güncel Yönelimler ............................................................. 71 İÇİNDEKİLER (devam) Sayfa", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 6, "page_end": 6}}
{"content": "vii 6.1.1 Çok Modlu Büyük Dil Modelleri (Multimodal Large Language Models) ..... 71 6.1.1.1 Öne Çıkan Çok Modlu Büyük Dil Modelleri ................................................. 75 6.1.2 Çok Ajanlı Sistemler (Multi-Agent Systems) ................................................ 79 6.1.2.1 Çok Ajanlı Sistemlerin Büyük Dil Modelleri ile Entegrasyonu ..................... 82 6.1.2.2 Büyük Dil Modelleri Tabanlı Ajanların Uygulama Alanları.......................... 84 6.2 Büyük Dil Modellerinde Etik Sorunlar .......................................................... 88 7.SONUÇ VE DEĞERLENDİRME .............................................................................. 92 KAYNAKLAR DİZİNİ .................................................................................................. 93 İÇİNDEKİLER (devam) Sayfa", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 7, "page_end": 7}}
{"content": "viii ŞEKİLLER DİZİNİ Şekil Sayfa Şekil 2.1 Turing testinin diyagramı .......................................................................... 3 Şekil 2.2 Yaygın kullanılan doğal dil işleme teknikleri ......................................... 10 Şekil 2.3 Mikolov’un çalışmasındaki kelime çiftlerinin vektör ofsetleri ............... 14 Şekil 3.1 Makine öğreniminde bir modelin eğitilmesi ........................................... 16 Şekil 3.2 Test verisi için karar verme süreci .......................................................... 16 Şekil 3.3 Yapay sinir ağı modelinin temel yapısı ................................................... 19 Şekil 3.4 Yapay sinir ağı modeli ........................................................................... 19 Şekil 3.5 Takviyeli öğrenme modeli ...................................................................... 21 Şekil 3.6 İleri beslemeli yapay sinir ağı modeli ..................................................... 23 Şekil 3.7 Geri beslemeli sinir ağı modeli ............................................................... 24 Şekil 3.8 Evrişimsel sinir ağı katmanları ............................................................... 26 Şekil 3.9 Tekrarlayan sinir ağı mimarisi ................................................................ 27 Şekil 3.10 Uzun kısa süreli bellek mimarisi .......................................................... 28 Şekil 3.11 Kapılı tekrarlayan birim mimarisi ......................................................... 29 Şekil 4.1 Doğal dil işlemede kullanılan derin öğrenme yöntemleri ...................... 37 Şekil 5.1 Temel Transformer mimarisinin yapısı ................................................... 45 Şekil 5.2 Encoder ve Decoder blokları ................................................................... 59", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 8, "page_end": 8}}
{"content": "ix ŞEKİLLER DİZİNİ (devam) Şekil Sayfa Şekil 5.3 GPT-3 ve BERT’in duygu tanıma görevindeki performansları ............. 61 Şekil 5.4 Duygu analizi için ChatGPT tarafından oluşturulan promptlar .............. 66 Şekil 6.1 Çok modlu büyük dil modeli mimarisi ................................................... 73 Şekil 6.2 Çok ajanlı sistemin genel mimarisi ......................................................... 83 Şekil 6.3 OpenAI’nın Swarm sistemi ve müşteri hizmetlerinde kullanımı ............ 86", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 9, "page_end": 9}}
{"content": "x TABLOLAR DİZİNİ Tablo Sayfa Tablo 4.1 Dil modellerinin kullanım alanları .......................................................... 3 Tablo 5.1 Derin öğrenme modellerinin karşılaştırılması ...................................... 63 Şekil 5.2 GPT ve BERT karşılaştırılması ............................................................... 70", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 10, "page_end": 10}}
{"content": "xi SİMGELER VE KISALTMALAR DİZİNİ Kısaltmalar AI Artificial Intelligence (Yapay Zekâ) ANN Artificial Neural Network (Yapay Sinir Ağı) BERT Bidirectional Encoder Representations from Transformers CNN Convolutional Neural Network (Evrişimsel Sinir Ağı) DL Deep Learning (Derin Öğrenme) DNN Deep Neural Network (Derin Sinir Ağı) GPT Generative Pre-trained Transformer GRU Gated Recurrent Unit (Kapılı Tekrarlayan Birim) LLM Large Language Model (Büyük Dil Modeli) LLM-MAS Large Language Model-Multi Agent System LSTM Long-Short Term Memory (Uzun Kısa Süreli Bellek) MAS Multi-Agent Systems (Çok Ajanlı Sistem) MLLM Multimodal Large Language Model (Çok Modlu Büyük Dil Modeli) ML Machine Learning (Makine Öğrenimi) NLP Natural Language Processing (Doğal Dil İşleme)", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 11, "page_end": 11}}
{"content": "xii SİMGELER VE KISALTMALAR DİZİNİ (devam) RLHF Reinforcement Learning with Human Feedback (İnsan Geri Bildirimiyle Takviyeli öğrenme) RLAIF Reinforcement Learning with Artificial Intelligence Feedback (Yapay Zekâ Geri Bildirimiyle Takviyeli Öğrenme) RNN Recurrent Neural Network (Tekrarlayan Sinir Ağı)", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 12, "page_end": 12}}
{"content": "1 1.GİRİŞ Yapay Zekâ Teknolojisi çağımızın en güncel ve en önemli araştırma konularından birisi olup birçok sektörde çığır açıcı gelişmelere imza atmaya devam etmektedir. Yapay zekânın temel yapıtaşlarından biri olan Büyük Dil Modelleri (Large Language Models- LLM), metin sınıflandırma, duygu analizi, metin özetleme, metin üretimi gibi birçok Doğal Dil İşleme (Natural Language Processing-NLP) görevinde kullanılmaktadır. Yapay Zekâ, insan benzeri düşünebilen, yorumlayabilen, konuşabilen sistemler geliştirmeye dayalıdır. Hepimiz son kullanıcı olarak bu teknolojileri kullanıyoruz ve sadece sonuçlarla ilgileniyoruz. Bu yüzden çoğumuz, arka planda bu sistemlerin nasıl işlediğini bilmiyoruz. Günümüzde bu teknolojiler, çocuktan yaşlıya herkes tarafından duyulan, benimsenen ve birçok alanda dikkat çekici değişimlere yol açan bir güç haline gelmiştir. Tüm bu değişimler bu alana olan ilgiyi artırmakla kalmayıp ciddi bir rekabet ortamı yaratmıştır. Peki işin arka planında ne oluyor ve bu süreç nasıl işliyor? Bu çalışmada, yapay zekâ dil modelleri ile ilgili yapılan akademik çalışmalar incelenmiş, literatür doğrultusunda ana fikir ve temel kavramlar anlatılmıştır. Literatür taraması yöntemiyle Doğal Dil İşleme alanında devrim yaratan Transformer Mimarisi ve gelişim süreci başta olmak üzere Generative Pre-trained Transformer (GPT) ve Bidirectional Encoder Representation from Tranformers (BERT) gibi iki önemli büyük dil modelinin farklı NLP görevlerinde ki performansları incelenmiş ve ilgili akademik çalışmalardan yararlanılarak karşılaştırılmıştır. Çalışma kapsamında doğal dil işleme alanında kullanılan makine öğrenimi ve derin öğrenme kavramlarına değinilmiş aynı zamanda literatürdeki akademik kaynaklardan yararlanılarak bu iki kavram arasındaki farklar açıkça belirtilmiştir. Çalışmada, insan dilinin karmaşıklığı ve geleneksel yöntemlerin yetersizliği gibi faktörler doğrultusunda dil modellemeye duyulan ihtiyaç ile, dil modellerinin kullanım alanları ele alınmıştır. Ayrıca bu çalışmada, kod uygulaması yapılmamış olup deneysel testler yerine literatür taramasına dayalı analizlere odaklanılmıştır. Çalışma GPT ve BERT ’in temel versiyonları ile gelişmiş sürümlerini kapsamakta olup güncel gelişmelerden olan çok modlu büyük dil modelleri ve ajan sistemlerini de içermektedir. Son olarak, bu gelişmelerle birlikte ortaya çıkan etik sorunlar ve bu sorunlara karşı çözüm olarak geliştirilen yasal düzenlemeler ve bilimsel gelişmeler ele alınmıştır.", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 13, "page_end": 13}}
{"content": "2 2.YAPAY ZEKÂ VE DOĞAL DİL İŞLEME Teknolojideki ilerlemeler doğrultusunda Doğal Dil İşleme (Natural-Language Processing-NLP) alanındaki en önemli gelişmelerden biri hiç şüphesiz Yapay zekâ alanı olmuştur. Yapay zekâ, NLP’nin önemli bir uygulama alanıdır. Bu bölümde, Yapay zekâ kavramı ve temel bileşenleri ile yapay zekâ tarihindeki önemli gelişmeler ele alınacak; ardından ise Doğal Dil İşleme ve evrimi incelenecektir. Kavramlar arasındaki ilişkiler ilerleyen süreçlerin daha iyi anlaşılabilmesi için oldukça önemlidir. 2.1 Yapay Zekâ Nedir? Yapay zekâ, ‘insan gibi düşünebilen’, ‘insan gibi yorumlayabilen’, ‘insan gibi konuşabilen’ sistemler geliştirmeyi amaçlayan ve insan benzeri görevleri yerine getirmeye çalışan bir alandır. Yapay zekâ kavramının ne olduğunu tam olarak anlayabilmek için ‘Yapay nedir?’ ,’Zekâ nedir?’, ‘Taklit nedir?’ gibi temel kavramlarında ne olduğunu anlamak gerekir. ‘Yapay’, Türk Dil Kurumu’na göre insan eliyle doğadaki örneklerine benzetilerek yapılan suni unsurlar şeklinde tanımlanır. Yapay zekâ bağlamında ise bu kavram, insanın sahip olduğu özelliklere benzetilerek sistemler geliştirmek anlamına gelir. ‘Zekâ’, Türk Dil Kurumu’na göre insanın düşünme, öğrenme ve akıl yürütme gibi becerilerinin tamamıdır. Yapay zekâ bağlamında ise bu kavram, makinelerin insan zekasına benzer olarak düşünmesi, verilerden öğrenmesi ve akıl yürütmesi süreçlerini ifade eder. ‘Taklit’, Türk Dil Kurumu’na göre belli bir örneğe benzemeye veya benzetilmeye çalışılan yapı şeklinde tanımlanır. Yapay zekâ bağlamında ise bu kavram, makinelerin insanın sahip oldukları özellikleri taklit ederek insan gibi davranması olarak açıklanır. ‘Yapay Zekâ’, Türk Dil Kurumu’na göre bilgisayar kontrolündeki bir aygıtın insan benzeri bir biçimde algılama, öğrenme, fikir yürütme ve karar verme süreci olarak tanımlanır. Sonuç olarak, yapay zekâ kavramı, öğrenme, algılama, karar verme ve ilişkileri anlama gibi süreçleri, insan fonksiyonlarını taklit ederek veya benzer şekilde yerine getiren bilgisayar tabanlı sistemler olarak tanımlanabilir. Temelde, insan beyninin çalışma biçiminin bir modellemesini baz alır.", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 14, "page_end": 14}}
{"content": "3 Şekil 2.1 Turing testinin diyagramı 2.2 Yapay Zekânın Tarihçesi Yapay zekânın tarihçesinden bahsederken, fikirsel ve isimsel olarak doğuşundan söz etmek yerinde olacaktır. Yapay zekâ alanında ki ilk çalışmalar Walter Pitts ve Warren McCulloch tarafından yapılmıştır. Bu çalışmalarda ele alınan konu ise yapay sinir hücreleri ile ilgili öneriler mantığı ve temel basit hesaplamalar yapabilen modellerdi. Bu modeller yapay zekâ teknolojisinin babası olarak görülen Alan Turing’in “Hesaplama Kuramı” isimli çalışmasına dayanıyordu (Aydın,2017). Alan Turing, 1950 yılında yayımladığı “Hesaplama Makineleri ve Zekâ” adlı makalesinde sorduğu “Makineler Düşünebilir mi?” sorusu ile ilk kez makinelerin düşünebileceklerini sorgulayan bir fikri öne sürmüştür. Turing, bu makalede bir makinenin düşünebilmesinin mümkün olup olmadığını gösterebilmek için bir düşünce deneyi olan ve literatürde “Turing testi” olarak bilinen bir test sunmuştur (Sarı,2021). Turing testi, üç kişi arasında yazışma yoluyla gerçekleşmektedir. Testte bir bilgisayar, bir insan ve bir sorgulayıcı bulunmaktadır. Sorgulayıcı, bilgisayarın ve insanın hangisi olduğunu bilmeden bir monitör aracılığıyla her ikisine de sorular sorar ve hangisinin bilgisayar hangisinin insan olduğunu tespit etmeye çalışır. Bilgisayar sorgulayıcıyı insan olduğuna ikna edebilirse yani insan gibi davranıp sorgulayıcıyı buna inandırırsa bilgisayar Turing testini geçmiş kabul edilir. Turing testinin diyagramı Şekil 2.1’de sunulmuştur (Aydın,2017).", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 15, "page_end": 15}}
{"content": "4 Yapay Zekâ’nın isimsel doğuşundan söz edecek olursak bu kavram 1956 yılında ilk kez Amerikalı bilgisayar bilimcisi John McCarthy tarafından kullanılmış ve literatüre geçmiştir. (Zorluel,2019). Yapay Zekâ (Artificial Intelligence-AI) kavramı McCarthy tarafından 1956 yılında düzenlenen Dartmouth Kolejindeki bir konferansta kullanılmıştır. McCarthy’nin yanı sıra bu konferansa katılan diğer bilim insanları Marvin L. Minsky, Nathaniel Rochester ve Claude Shannon yapay zekânın öncüleri olarak kabul görmektedir (Dick,2019). McCarthy, yapay zekâyı zeki insan davranışlarının makinelere öğretilmesi ve yaptırılması olarak tanımlamıştır (Balta,2020). Bu tanıma göre bir bilgisayarın fikir yürütme, ilişki kurma, genel bir sonuca ulaşma gibi insan benzeri davranışlar göstermesi ve bilişsel kabiliyetleri kullanması yapay zekâ olarak tanımlanabilir (Arslan,2020). Dartmouth konferansının ardından yapay zekâ alanındaki gelişmeler ivme kazanmıştır. Yapay zekâ ürünlerini bu gelişmelere örnek olarak verebiliriz. ELIZA, insanlar ve makineler arasındaki iletişimi keşfetmek amacıyla 1964-1967 yılında Joseph Weizenbaum tarafından geliştirilmiş bir doğal dil işleme bilgisayar programıdır (Vikipedi,2025). ELIZA, biriyle sohbet ederken karşısındaki kişinin söylediklerini anlıyormuş gibi gözükür ancak onun kurduğu cümleler karşısındaki kişinin söylediklerinin bir tekrarıdır. (Churchland,2018; Sarı’dan,2021). Bu nedenle ELIZA beklenilen başarıyı yakalayamamıştır çünkü kurduğu cümleler sadece anahtar kelimelerle sınırlıdır. Bununla birlikte günümüzdeki birçok sohbet robotu yazılımının temelini oluşturduğu gerçeği yadsınamazdır. Bu alanda ki gelişmeler hızla ilerlerken 1974-1980 yılında yapay zekâ çalışmalarını yavaşlatan birtakım gelişmeler meydana gelmiş ve bu döneme Yapay Zekâ Kışı (AI Winter) denmiştir. Bu dönemde yapay zekâ çalışmalarını eleştiren raporlar yayınlanmış ve devletin yapay zekâ çalışmalarına desteği azalmıştır.1980’li yıllardan itibaren İngiltere’nin Japonlarla yarışmak için tekrardan yapay zekâ alanına fon ayırmasıyla çalışmalar kaldığı yerden devam etmiş ve hareketlilik kazanmıştır (Lewis,2014; Öztürk ve Şahin’den,2018). 1980‘lerde uzman sistemler yapay zekâ çalışmalarında önemli bir rol oynamıştır. Uzmanların bilgi ve deneyimlerini modellemek amacıyla bilgisayarlar kullanılmıştır. 1990’larda derinlemesine öğrenme ve yapay sinir ağları gibi alanlarda çalışmalar popülerlik kazanmıştır. Bunun üzerine yapay zekâ çalışmaları daha büyük ve karmaşık veri setleri üzerinde yürütülmüştür (Köse vd.,2023).", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 16, "page_end": 16}}
{"content": "5 Tarihsel süreç içerisinde yaşanan önemli gelişmelerden biri hiç şüphesiz dünyanın en büyük teknoloji şirketlerinden biri olan Uluslararası İş Makineleri (International Business Machines-IBM) tarafından geliştirilen Deep Blue’nun 1997 yılında dünyaca ünlü satranç şampiyonu Gary Kasparov’u yenmeyi başarmasıdır. Bu gelişme ile birlikte bir bilgisayar programının satranç gibi stratejik bir oyunda bir insanı alt edebilmesinin mümkün olduğu görülmüştür ve bu alandaki diğer gelişmelerin önü açılmıştır. 1997 yılında Windows’ta kullanılan ve Dragon Systems tarafından geliştirilen ilk “konuşma tanıma yazılımı” piyasaya sunulduktan sonra sesli komutların kullanımı ile ilgili yeni bir çağ başlamıştır ( Temur,2024). Yapay zekâ 2000’li yıllardan itibaren büyük veri, derin öğrenme ve makine öğrenimi gibi teknolojilerin gelişmesi ile birlikte daha fazla dikkat çekmeye başladı. Bu dönemde yapay zekânın endüstride kullanımı ve uygulama alanları büyük ölçüde arttı. 2010’lar ve sonrasında derin öğrenme yapay zekâ alanında büyük bir sıçrama gösterdi. Gelişmiş algoritmalar ve daha büyük veri kullanımıyla birlikte yapay zekâ otomasyon, sağlık, otomotiv, finans ve birçok alanda aktif bir şekilde kullanılmaya başlandı (Köse vd.,2023). 2020 yılında GPT-3 dil modellerinin piyasaya sürülmesi ve AlphaFold protein katlama programının başarılı olması gibi iki tane önemli gelişme meydana gelmiştir (Temur,2024). GPT-3, OpenAI tarafından geliştirilen 175 milyar parametreli büyük bir dil modelidir. GPT-3 çok fazla uğraş gerektirmeyen bir makine öğrenimi yaklaşımı ile herhangi bir metin üzerinden öğrenir ve birçok farklı görevde kullanılır. GPT-3’ün başarılı olduğu görevlere örnek verecek olursak, bilgisayar kodu oluşturma, görüntüleri otomatik tanımlama, dil çevirisi yapma gibi birçok alanla beraber yaratıcılarının planlamadığı bazı özellikler de dahil olmak üzere matematiksel hesaplamaları yapabildiğini de söyleyebiliriz (Grossman,2020). DeepMind Technologies, 2010 yılında kurulmuş bir İngiliz yapay zekâ programı geliştirme şirketidir. DeepMind, 2014 yılında Google tarafından satın alınmıştır (Vikipedi, 2025). AlphaFold, DeepMind tarafından geliştirilmiş ve derin öğrenme tabanlı bir algoritmadır. Gelişim süreci 2015 yılında başlayan AlphaFold, protein katlanması problemi üzerine makine öğrenmesi algoritmaları kullanarak çalışmaya başlamıştır ve proteinlerin üç boyutlu yapısını dikkate değer bir doğrulukla tahmin edebilmektedir (Hey et al.,2020).", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 17, "page_end": 17}}
{"content": "6 2016 yılında DeepMind’ın AlphaGo isimli makine öğrenme algoritması, satranç gibi diğer oyunlara nazaran bilgisayarların yenmesinin çok daha zor olduğu Go isimli oyunda dünya şampiyonu Lee Sedol’u 4-1 mağlup ederek tarihi bir başarıya imza attı ve bu galibiyet yapay zekâ ve Go toplulukları üzerinde büyük etki yarattı (Li and Du,2018). AlphaGo’nun bu başarısı göz önüne alınarak aynı algoritmanın protein katlaması üzerinde de başarı gösterebileceği ön görülmüştür böylelikle AlphaGo’daki algoritma geliştirilerek protein katlaması problemi üzerinde çalışan AlphaFold geliştirilmiştir (Tekin ve Gurbanov,2023). 2021 yılında OpenAI tarafından geliştirilen ve ismini ressam Salvador Dali’den alan DALL-E, GPT-3 teknolojisi üzerine inşa edilmiştir. DALL-E, 12 milyar parametreden oluşan ve metin açıklamalarından görüntüler oluşturabilen bir yapay sinir ağıdır (Şen,2021). OpenAI geçmişte GPT-1, GPT-2, GPT-3 VE GPT-4 gibi farklı modeller yayınlamıştır. Yayınlanan her yeni model daha büyük veri setleri ve daha fazla parametre kullanılarak eğitilmiştir (Alawida et al.,2023). OpenAI, çıkardığı her yeni model için iddiali konuştuğu gibi en son piyasaya sürdüğü GPT-4.5 modelini de şimdiye kadar ki en büyük ve en iyi model olarak tanıttı. GPT-4.5 modelinin diğer modellere kıyasla daha az halüsinasyon görmesi ve daha zengin konuşmalar gerçekleştirmesinin yanı sıra diğer farklı görevlerde de daha güvenilir performans gösterdiği belirtilmiştir (OpenAI,2025). Sonuç olarak, yapay zekânın tarihsel süreci incelendiğinde bu alanda yapılan çalışmaların zaman zaman inişli çıkışlı ilerlese de önemli atılımlar yaptığı gözlemlenmektedir. Hayatımızın her alanında yer edinen ve oldukça hızlı gelişen yapay zekâ teknolojisindeki gelişmeleri adım adım takip etmenin giderek zorlaştığı açıktır. Bu hızlı gelişim yapay zekânın gelecekte daha da yaygınlaşması ile hayatımızın ayrılmaz bir parçası haline gelmesini sağlayacaktır. Ancak yapay zekâ teknolojisinin giderek yaygınlaşması ile birlikte etik, güvenlik ve şeffaflık konularına daha fazla dikkat edilmesi gerekmektedir. Bu doğrultuda, yapay zekanın bilinçli ve sorumlu kullanılması teşvik edilmeli ve gerekli önlemler alınmalıdır.", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 18, "page_end": 18}}
{"content": "7 2.3 Doğal Dil İşleme (Natural Language Processing-NLP) ve Evrimi Doğal Dil İşleme (Natural Language Processing-NLP), yapay zekânın önemli uygulama alanlarından biridir. Doğal Dil İşleme, bilgisayarların insan dilini anlamasını, yorumlamasını, analiz etmesini ve üretmesini amaçlayan bir yapay zekâ dalıdır. Bugün günlük hayatta kullandığımız hemen hemen yapay zekâ tabanlı çoğu cihaz NLP tekniklerine dayanarak üretilmiştir. Bu bölümde Doğal Dil İşlemenin tanımı, evrimi, ve yaygın kullanılan doğal dil işleme tekniklerinden bahsedilecektir. 2.3.1 Doğal Dil İşleme (Natural Language Processing-NLP) Doğal Dil İşleme (Natural Language Procesing-NLP), doğal dilin bilgisayarların anlayacağı şekilde sayısal verilere çevrilip ve sayısal verilerinde insanların anlayacağı doğal dile çevrilmesi işlemidir (Lane et al.,2019; Ateş’ten,2021). Doğal dil işleme, dilbilim, bilgisayar bilimi ve yapay zekâ alanlarının bir alt dalıdır. İnsanların günlük yaşamında kullandığı dil karmaşık bir yapıya sahiptir bu da dilin bilgisayarlar tarafından anlaşılmasını zorlaştırmaktadır. Doğal dil işleme dili anlama ve işleme süreçlerinde karşılaşılan bu zorlukları aşmayı hedeflemektedir. Gün geçtikçe doğal dil işlemenin önemi artmaktadır çünkü her geçen gün büyük miktarda metin verisi üretilmekte ve bu verilerin etkili bir şekilde işlenmesi gerekmektedir (Koç ve Sevli,2024). İnsan-bilgisayar etkileşiminin seviyesini belirleyen doğal dil işleme aslında yapay zekânın en karmaşık alanlarından biridir. Doğal dil işleme metin verilerine uygulanacak yöntemlerdir. Yapılacak göreve ve veriye göre doğal dil işleme adımları farklılık gösterir. Örneğin, web ortamında bulunan metin verilerine uygulanan doğal dil işleme adımları ile basılı bir belgeye uygulanan doğal dil işleme adımları birbirinden farklıdır. (Karaca ve Bayır,2024). Doğal dil işleme insanlar ve makineler arasında bir köprü görevi görmüştür ve birçok alanda kullanılmaktadır. Doğal dil işlemenin kullanım alanları; Duygu analizi, soru cevaplama, metin özetleme, makine çevirisi, sanal asistanlar, dil tanımlama, varlık tanımlama, intihal tespiti gibi alanlarda etkin bir şekilde kullanılmaktadır ve gelişmeye devam etmektedir (Özmutlu,2019; Güler ve Akgül’den,2022). Doğal dil işlemenin bu kadar geniş bir kullanıma sahip olması, yapay zekânın geleceği açısından oldukça büyük bir potansiyele sahip olduğunu göstermektedir. Gelişen NLP teknikleri ve yöntemleri, insan-bilgisayar etkileşimini çok daha verimli bir hale getirecektir. Bu gelişmeler doğrultusunda, dil modellerinin daha karmaşık ve zor görevleri yerine getirmesi NLP’nin gelecekteki rolünü çok daha önemli bir hale getirecektir.", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 19, "page_end": 19}}
{"content": "8 2.3.2 Doğal Dil İşlemenin Evrimi Doğal Dil İşleme (Natural Language Processing-NLP) alanındaki ilk denemeler, Almanlar tarafından II. Dünya Savaşı sırasında yapılmıştır. Enigma adı verilen makine, Almanların gizli mesajlarını şifrelemek ve bu mesajları Avrupa’da bulunan komutanlarına iletmek için kullanılmıştır. 1946 yılında ise Britanya tarafından Colossus adlı makine geliştirilmiştir ve bu makine Enigma tarafından üretilen şifreli mesajları başarıyla çözmüştür. Bu gelişme, II. Dünya Savaşı’nın dönüm noktalarından biri olmuştur. Britanya hükümetinin kriptografik kurumunun bulunduğu Bletchley Park’ta, Alan Turing ve diğer ajanlar Alman mesajlarını çözerek önemli askeri bilgileri elde etmiştir. Bu durum, modern bilgisayar biliminin temellerinin atılmasında önemli bir adımdır (Johri et al.,2021). Doğal dil işleme, yapay zekâ çalışmalarının başlangıcına kadar dayanmaktadır. 1950 yılında Alan Turing’in “Hesaplama Makineleri ve Zekâ” adlı makalesinde makinelerin zekâ sahibi olup olmamasını tartışması ve Turing testini önermesi makinelerin insan dilini anlaması üzerine ilk teorik adımları atmıştır. 1950 ve 1960 yılları, doğal dil işlemenin temel algoritmalarının ve temel programlarının geliştiği dönemdir. Bu dönemde, makine çevirisi gibi alanlar gelişmeye başlamıştır. Özellikle 1954’teki Georgetown-IBM deneyi ile İngilizce ’den Rusça ’ya yapılan makine çevirisi doğal dil işlemenin potansiyelini ortaya koymuştur (Koç ve Sevli,2024). Doğal dil işleme, ilk büyük gelişimini 1957 ile 1970 yılları arasında yaşamıştır ve bu dönemde Yapay Zekâ (Artificial Intelligence- AI) alanına dahil olmuştur. Bu dönemde hem olasılıksal hem de kurallara dayalı yöntemler üzerine yapılan araştırmalar önemli bir ivme kazanmıştır. NLP’nin ikinci gelişim aşaması ise 1971 yılında başlayıp 1993 yılında sona ermiştir. Bu dönem içerisinde ise NLP tabanlı uygulamaların makul bir süre içerisinde ele alınması, istatistiksel tekniklerin kullanımı ve dil veri kümelerinin oluşturulması gibi durumlarda zorluklarla karşılaşılmıştır.1990’ların ortasında bilgisayar hızı ve depolama kapasitesinin hızla artması, 1994 yılında internetin ticarileşmesi NLP araştırmalarının genişlemesine büyük katkı sağlamıştır (Abro et al.,2022). 1990’lı yıllarda Yoshua Bengio, dil modelleme için ileri beslemeli sinir ağlarının (feedforward neural networks) kullanılmasını önerdi. İleri beslemeli sinir ağları, veriler arasındaki karmaşık ilişkileri öğrenebilen sinir ağ türlerinden biridir. 2000’li yıllarda Tomas Mikolov tarafından kelime gömmelerini (word embeddings) öğrenmek amacıyla Word2Vec modeli geliştirildi. Kelime gömmeleri, kelimeleri yüksek boyutlu bir uzayda bir vektör olarak temsil etmek anlamına gelmektedir. Bu vektör temsili, dil modellerinin kelimeler arasındaki anlamsal ve sözdizimsel ilişkileri öğrenmesini sağlamaktadır. Kişisel bilgisayarların ve internetin gelişimi NLP alanında da devrim niteliğinde", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 20, "page_end": 20}}
{"content": "9 ilerlemelere yol açmıştır. Bu gelişmeler, büyük miktarda verinin depolanmasını ve kullanılmasını mümkün kılarken makine öğrenimi algoritmalarının eğitim sürecini de önemli ölçüde etkilemiştir. Teknolojideki bu ilerlemelerle birlikte NLP sistemleri daha geniş bir kitleye dağılmış ve bu alanda önemli ilerlemeler kaydedilmiştir (Hassan, 2023). Sinir ağları, NLP alanında önemli bir rol oynamıştır. 2000’lerin başında sinir ağları yaygın olarak kullanılmazken, 2013 yılına kadar NLP alanında sinir ağlarının kullanımı üzerine yeterince tartışma ve araştırma yapılmıştır. Bu durum, birçok değişikliğe yol açarak NLP alanında çeşitli sinir ağlarının uygulanmasına neden olmuştur. Başlangıçta, Evrişimsel Sinir Ağları (Convolutional Neural Networks-CNN), görüntü sınıflandırma ve görsel görüntüleri analiz etme gibi alanlarda kullanılmıştır. Daha sonra, CNN’ler cümle sınıflandırma, duygu analizi, metin sınıflandırma ve metin özetleme gibi NLP görevlerinde de kullanılmıştır. Ayrıca, doğası gereği tekrarlayan bir yapıya sahip olan ve her veri için aynı işlevi yerine getiren Tekrarlayan Sinir Ağları (Recurrent Neural Networks-RNNs) yapısıda NLP’de kullanılmıştır. RNN, metin, zaman serileri, finansal veriler, konuşma, ses ve video gibi sıralı ardışık veriler için ideal bulunmuştur. RNN’lerin değiştirilmiş versiyonlarından biri olan Uzun Kısa Süreli Bellek (Long Short-Term Memory-LSTM), istenen önemli bilgilerin çok daha uzun süre saklanmasını ve alakasız bilgilerin elenmesini sağladığı için oldukça kullanışlı bir yapıdır. Kapılı Tekrarlayan Birim (Gated Recurrent Unit-GRU), LSTM’nin daha gelişmiş halidir. Ayrıca, GRU’nun LSTM’den daha iyi sonuçlar gösterdiği kanıtlanmıştır. Transformer (Dönüştürücü) mimari, dikkat mekanizmasının (attention mechanism) kullanımı ile birlikte NLP alanında önemli gelişmeler sağlamıştır. Dikkat mekanizmaları geliştirilerek sinir ağlarının hangi bilgilere odaklanması gerektiği öğrenilmiştir ve bu mekanizmaya dayalı Transformer modeli tanıtılmıştır. Bu ilerlemeler doğrultusunda Transformer modelleri daha da geliştirilerek BERT (Bidirectional Encoder Representations from Transformer) gibi çift yönlü bağlam anlayışına sahip modeller NLP alanında büyük bir dönüşüm yarattı (Khurana et al.,2022). Doğal dil işlemenin tarihsel gelişimi ve günümüz teknolojisi göz önüne alındığında, insanların doğal dili anlama ve işleme çabasının bilgi iletimi, belge sınıflandırma, makine çevirisi gibi belli amaçlar doğrultusunda tarih boyunca var olduğunu söyleyebiliriz. Doğal dil işleme ve Büyük Dil Modelleri (Large Language Models-LLM) alanında her geçen gün yeni gelişmeler ortaya çıkmaktadır; bu gelişmelere paralel olarak internet kullanımının yaygınlaşması, sosyal medya platformlarında üretilen içeriklerin artması ve büyük veri (big data) kavramının ilgi görmeye başlaması gibi sebeplerle metin veri kümeleri de genişlemektedir. Tüm bu gelişmeler, NLP alanındaki yöntemlerin ve LLM’lerin daha büyük ve karmaşık veri setleri üzerinde çalışmasını mümkün kılmakta ve daha doğru, verimli çözümler geliştirilmesini sağlamaktadır.", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 21, "page_end": 21}}
{"content": "10 Şekil 2.2 Yaygın kullanılan doğal dil işleme teknikleri 2.3.3 Doğal Dil İşleme Teknikleri Doğal Dil İşleme (Natural Language Processing-NLP) teknikleri, insan dilinin zengin ve karmaşık yapısını göz önünde bulundurarak makinelerin dilin anlamını doğru şekilde kavrayıp, analiz etmesi için geliştirilmiş yöntemlerdir. İnsan dili deyimler, ironiler, sesteş kelimeler, mecaz anlamlar ve çeşitli dilbilgisel kurallara sahiptir. NLP’nin amacı makinelerin sadece kelimeleri doğru anlaması değil, aynı zamanda bu kelimelere ait bağlamı da doğru anlayıp analiz etmesidir. Günümüzde metin sınıflandırma, duygu analizi, metin özetleme, dijital asistanlar ve makine çevirisi gibi birçok alanda iyi performans gösteren uygulamaların temelinde güçlü ve etkili doğal dil işleme teknikleri bulunmaktadır. Doğal dil işleme çalışmalarının performansını etkileyen en önemli etkenlerden biri, kullanılan verinin kalitesidir. Son yıllarda hızla gelişen teknoloji ve pandeminin etkisiyle, internet ticareti, uzaktan eğitim, online hukuk görüşmeleri ve uzaktan toplantılar gibi yaşanan değişimler veri dünyasının genişlemesine büyük bir katkı sağlamıştır. Bu gelişmelerle birlikte veri bilimi ve doğal dil işleme alanlarına olan ilgi de artmıştır. Doğal dil işleme alanında kullanılan en yaygın dil işleme teknikleri Şekil 2.2’de sunulmuştur (Hatipoğlu ve Bilgin,2024).", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 22, "page_end": 22}}
{"content": "11 Kelime Gömme (Word Embeddings), metin verilerinin vektörlerle temsillerinin oluşturulmasıdır. Başka bir deyişle, birbirleriyle benzer anlam taşıyan kelimelerin benzer vektörlerle temsil edilmesidir. Bu teknik, kelimelerin birbirleri ile olan ilişkisini, kelimelerin kullanıldığı bağlamı ve metinde geçme sıklığını göz önünde bulundurarak doğal dil işleme görevlerinde kullanılmaktadır (Almeida and Xexéo, 2019; Hatipoğlu ve Bilgin’den,2024). Gövdeleme (Stemming), kelimenin çekim eklerinden arındırılarak en yalın hale getirilmesidir. Örnek verecek olursak,” kitaplıktan” kelimesinin “kitap” ve “geldi” kelimesinin “gel” şeklinde gösterilmesidir. Ancak bu yöntem bazı durumlarda doğru anlamı vermek için yetersiz kalabilmektedir. “Gözlükten” kelimesinin “göz” şeklinde ifade edilmesi bu duruma örnek olarak verilebilir. (Ateş,2021). Baş Sözcük Çıkarma (Lemmatization), kelimelerin doğru bir şekilde işlendiği, kelime bilgisi ve morfolojik analiz kullanılarak yapılan bir işlemi ifade eder. Bu yöntem, çekim eklerini kaldırmayı ve bir kelimenin sözlük anlamını döndürmeyi amaçlar. Gövdeleme (stemming) yöntemine benzer, ancak farkı anlamı korumaya odaklanmasıdır. Örneğin,” gözlükten” kelimesi gövdeleme (stemming) yönteminde “göz” şeklinde ifade edilirken, baş sözcük çıkarma (lemmatization) yönteminde “gözlük” olarak ifade edilir (Manning et al.,2009). Etkisiz Kelimelerin Çıkarılması (Stop Words Removal), NLP alanında durak kelimeler (stop words), hedef dilde konuşma ve yazı dilinde en yüksek frekansta geçen kelimeleri ifade eder. Evrensel olarak kabul edilmiş tek bir durak kelime listesi bulunmamaktadır. Durak kelimelerin cümlenin anlamına genellikle önemli bir etkisi yoktur. Dolayısıyla bu kelimelerin kaldırılması sözlük boyutunu azaltır ve analizi daha verimli bir hale getirir. Dildeki bağlaçlar, edatlar, zarflar ve sık kullanılan kelimeler durak kelime niteliğinde olup, cümlenin anlamını etkilemeyebilir. Örneğin, “Ali ve Ayşe bugün parka gitti.” cümlesindeki “ve” bağlacının cümlenin anlamına bir etkisi yoktur. Benzer şekilde, “Bu kalemi dün aldım.” cümlesindeki “bu” kelimesi kalem nesnesini işaret etse de cümlenin anlamına etki etmez. Ancak, bu tür kelimeleri cümleden kaldırırken cümlenin anlamını değiştirmeyeceğinden emin olmak gerekir (Juluru et al.,2021). Tokenleştirme, metin verilerini farklı boyutlarda tokenlere bölerek daha özel analizler yapmak için kullanılmaktadır. Örneğin,” Yarın hava çok güzel olacak” cümlesini tokenleştirdiğinizde: [“Yarın”,” hava”,” çok”,” güzel”,” olacak”] şeklinde olur. Makine öğrenimi modellerine veya doğal dil işleme algoritmalarına metin verilerini girdi olarak vermek kadar bu modeller veya algoritmaların metindeki kelimelerin anlamlarını doğru anlaması da oldukça önemlidir. Bu nedenle, Tokenleştirme işlemi, metin verilerinin sayısal temsillerini matematiksel bir yapıya getirmeden önceki adımdır. Bu sayede doğal", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 23, "page_end": 23}}
{"content": "12 dil işleme görevleri için kullanılan metin verileri daha etkili bir şekilde işlenebilir ve analiz edilebilir (Aslan,2023). Varlık İsmi Tanımlama (Named Entity Recognition-NER), metin içerisindeki kişi adları, kuruluşlar, konumlar, tarihler ve miktarlar gibi isimlendirilmiş varlıkları tanımlayan ve sınıflandıran bir NLP görevidir. NER teknikleri, kural tabanlı sistemler, makine öğrenimi yaklaşımları ve hibrit sistemler olmak üzere üç ana başlık altında incelenebilir (Khadake,2024). TF-IDF (Terim Frekansı-Ters Belge Frekansı), bir kelimenin bir metin veri kümesine (corpus) göre bir belgedeki önemini belirlemek için kullanılan istatistiksel ölçümdür. Belgede daha sık görünen kelimeleri ayarlayarak kelimenin öneminin daha doğru bir şekilde temsil edilmesini sağlar.TF-IDF, kelimeleri tüm metin veri kümesindeki genel frekanslarına yani sıklıklarına göre değil, belirli bir belge içerisinde bulunan frekanslarına göre puanlar bu da daha alakalı arama ve öneri sonuçları sunar. Kısaca, TF- IDF, bir kelimenin bir belgede ne sıklıkla yer aldığına bağlı olarak o kelimenin önemini ifade eder ve bu doğrultuda arama motorları ile öneri sistemlerinde kullanıcılara daha anlamlı öneriler ve sonuçlar sunar. (Gomez et al.,2023). Kelime Çantası Modeli (Bag of Words Model-BoW), özellik seçme ve sınıflandırma için kullanılan oldukça başarılı ve popüler bir yöntemdir. BoW modeli, araştırmacılar tarafından NLP alanında kullanılan basitleştirilmiş bir temsil olarak ifade edilmektedir. Bu modelde, cümleler veya belgeler gibi metinler, dilbilgisi ile kelimelerin sırasını göz ardı eden ve yalnızca kelimelerin tekrarını dikkate alan bir kelime torbası olarak temsil edilir. BoW modeli, daha çok belge sınıflandırma yönteminde kullanılır; bu yöntemlerde, bir sınıflandırıcıyı eğitmek için kullanılan özellikler her bir kelimenin frekansı yani oluşum sıklığından üretilir (Qader et al.,2019). Literatüre bakıldığında belirtilen doğal dil işleme tekniklerinin bir kısmı metin ön işleme bir kısmı metin temsili olarak kabul edilmektedir. Metin ön işlemenin amacı, bir metin girdisinin ham bir şekilde alınıp amacına uygun olarak dizgelerin çıkartılması işlemidir. Metin temsilindeki amaç ise, kelimelerin bağlamsal benzerliğini koruyarak, düşük boyutlu vektörler üzerinde, daha anlamlı temsiller sunulabilmesini sağlamaktır. Tokenleştirme, gövdeleme, kök çözümleme, durak kelimelerin kaldırılması ve varlık ismi tanıma gibi yöntemler, ön işleme aşamasına girerken; kelime gömme, TF-IDF, kelime çantası modeli gibi yöntemler ise metin temsil teknikleridir (Karaoğlan,2022). Derin öğrenme tekniklerinin gelişmesiyle birlikte Yapay Sinir Ağları (Artificial Neural Networks-ANN) tabanlı doğal dil işleme teknikleri de ortaya çıkmıştır. Doğal dil işleme problemlerinde, yapay sinir ağı tabanlı olan kelime gömme metotları kullanılmakta ve kelimelerin sabit boyutlu vektör temsillerinin oluşturulması", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 24, "page_end": 24}}
{"content": "13 sağlanmaktadır. Yaygın olarak en sık kullanılan ve bilinen kelime gömme yöntemi, Mikolov ve arkadaşları tarafından 2013 yılında geliştirilen Word2Vec modelidir. Bu yöntemde tek gizli katmana sahip bir ANN modeli kullanılmakta olup, girdi olarak verilen metinler, gradyan inişi (gradian descent) ve geriye yayılım (back propogation) yöntemleri kullanılarak vektörleri günceller. Bu yöntemin amacı, benzer kelimelerin birbirine yakın konumlarda olmasını sağlamaktır. Matematiksel olarak ele alındığında, bu tür konumlandırılmış vektörlerin arasındaki açının kosinüsü 1’e yakın, yani açı 0’a yakın olmalıdır. Word2Vec yöntemi, anlamsal olarak birbirine yakın olan kelimelerin vektör temsillerinin oluşturulması için kullanılan bir yöntemdir. Yeterli miktarda veri ve bağlam göz önünde bulundurulduğunda Word2Vec geçmiş bilgilere dayanarak bir kelimenin anlamı hakkında doğru tahminlerde bulunabilir. Bu tahminler, bir kelimenin diğer kelimelerle ilişkilerini belirlemek için kullanılır (örneğin, “erkek” sözcüğünün “oğlan” sözcüğü ile arasındaki anlam ilişkisi ve “kadın” sözcüğünün “kız” sözcüğü ile arasındaki anlam ilişkisi gibi). Ayrıca, belgeleri kümelemek ve konularına göre sınıflandırmak için de kullanılmaktadır (Taşkıran,2021). Word2Vec modeli, Google tarafından önerilen ve metin verilerini işleyen bir sinir ağıdır. Bu model, iki öğrenme modeli içerir: Sürekli Kelime Çantası (Continuous Bag of Words-CBOW) ve Skip-gram. CBOW, bağlamı verildiğinde kelimeyi tahmin ederken, Skip-gram, bir kelime verildiğinde bağlamını tahmin eder. Metin veri kümesi (corpus) bir öğrenme modeline verildiğinde, Word2Vec kelime vektörlerini oluşturur. Bu süreçte, Word2Vec metin veri kümesinden bir kelime darağacı oluşturur ve her kelimenin vektör temsillerini öğrenir. Ayrıca, bu modelin her kelime arasındaki kosinüs benzerliğini hesaplayabilmesi, kelimeleri aralarındaki benzerliklerine göre kümelendirmemizi sağlar. Böylelikle, benzer kelimeler gruplanarak, orijinal özellik boyutu daha düşük bir boyuta indirgenir (Ma and Zhang,2015). Word2Vec modelinden sonra gelişmiş bir versiyonu olan Glove modeli tanıtılmıştır. Bu model, soru-cevaplama,duygu analizi ve metin özetleme gibi doğal dil işleme görevlerinde başarılı olmuş ; ayrıca kelime benzerliği ve varlık isim tanımlama görevlerinde de popülerlik kazanmıştır. Ancak,Word2Vec ve Glove modelleri kelimenin farklı bağlamlarda nasıl kullanıldığını ayırt etmede yetersiz kalmıştır. Bu sorunu gidermek için CoVe modeli tanıtılmıştır. CoVe modelinin en büyük zorluğu, sözlük dışı kelimelerle karşılaşıldığında sıfır vektörleri kullanması olmuştur. ELMo(Embedding from Language Models) ve BERT (Bidirectional Encoder Representations from Transformers) gibi modeller NLP görevlerinde bağlamı dikkat mekanizması (attention mechanism) sayesinde daha iyi analiz edip üstün performans göstermiştir.Bu modeller,kelimelerin bağlamsal temsillerini başarılı bir şekilde oluşturarak sözdizimsel ve anlamsal bilgiyi yakalamada oldukça başarılı olmuşlardır (Marredy et al.,2019).", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 25, "page_end": 25}}
{"content": "14 Kelime Gömme(Word Embeddings) yönteminin temel özelliklerinden biri anlamsal benzerliğe sahip kelimelere ait vektörlerin çok boyutlu uzayda birbirine yakın konumlanmasıdır. Bu nedenle, kelime gömme yöntemleri, Kelime Analojisi (Word Analogy) adı verilen kelime çiftlerinin benzerlik işlemiyle doğrulanabilmektedir. Bu tür kelime gömme modelleri, birleştirme(+), çıkarma(-) gibi işlemleri sağlamakta ve kelime analojilerini yansıtmaktadır. Literatürde en çok kullanılan temel örneklerden biri, “Kral- Erkek+Kadın” işleminden “Kraliçe” vektörüne benzer bir vektör elde edilmesidir (Uçan ve Sezer,2019). Mikolov ve arkadaşları, kelime vektörleri ile yapılan cebirsel işlemlerin sadece kelimeler arasındaki benzerliği vermekle kalmayıp analoji adı verilen karmaşık semantik ilişkileri de verdiğini bulmuşlardır. Kelime çiftlerinin vektör uzayındaki dağılımı Şekil 2.3 ‘de sunulmuş olup bu dağılımdan anlamlı sonuçlar gözlemlenebilmektedir. Örneğin, şekilde cinsiyet ilişkisini gösteren kelime çiftleri için iki kelimenin vektörleri arasındaki uzaklıklar birbirine eşittir. Sağ tarafta ise, kelime çiftlerinin tekil çoğul ilişkisi vektörler arasındaki mesafeden bulunabilir. Man, woman ve uncle kelime vektörlerinin konumlarından aunt kelimesinin uncle kelimesi ile cinsiyet ilişkisi bulunabilir. Mikolov ve arkadaşlarının çalışmalarında oluşturduğu veri seti 2 kelime çiftinden oluşmaktadır ve araştırma sonuçlarını da bu veri setini kullanarak değerlendirmişlerdir. Mikolov, Vector(“King”) - Vector(“Man”)+Vector(“Woman”) cebirsel işleminin sonucunun Vector(“Queen”)’e oldukça yakın bir sonuc verdiğini ortaya koymuştur. Böylelikle, kelimelerin vektör temsillerinin vektör uzayında bulunduğu konumlar arasındaki mesafeye bakılarak analojileri bulunabilmektedir (Polat ve Köpre,2018). Şekil 2.3 Mikolov’un çalışmasındaki kelime çiftlerinin vektör ofsetleri", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 26, "page_end": 26}}
{"content": "15 3.DİL MODELLEMEDE MAKİNE ÖĞRENİMİ VE DERİN ÖĞRENME Günümüzde Makine Öğrenimi (Machine Learning-ML) ve Derin Öğrenme (Deep Learning-DL) kavramları çoğu zaman birbirinin yerine kullanılmakta ve ortak bir başlık altında incelenmektedir. Ancak, kullanılan yöntemler ve çalışma prensipleri açısından aralarında farklar bulunmaktadır. Bu bölümde, makine öğrenimi ve derin öğrenmenin temel kavramları, çalışma prensipleri ve aralarındaki farklar ele alınacaktır. Ayrıca, bu yöntemlerin dil modelleme (language modeling) süreçlerindeki rolü ve öneminden bahsedilecektir. 3.1 Makine Öğrenimi: Temel Kavramlar ve Çalışma Prensipleri Makine öğrenmesi, bilgisayarların bir problemi çözebilmeleri için her adımı öncesinde belirtmek yerine, algoritmalar sayesinde izlenecek adımları öğrenmelerini sağlayan ve yapay zekânın bir alt dalı olan disiplinler arası bir çalışma alanıdır. Makine öğrenme algoritmaları, verilerden bir desen veya model oluşturarak gelecekte karşılaşılabilecek yeni durumları tahmin etmeye dayanan yazılımlardır. Örneğin, müşterilerin geçmiş alışveriş alışkanlıklarını analiz eden bir sistemin, müşterinin gelecekteki alışveriş davranışlarına ilişkin analizler yapması, bir makine öğrenmesi örneği olarak değerlendirilebilir. Makine öğrenmesi, verileri analiz ederek bir model oluşturan ve bu modeli kullanarak tanıma adımlarını tekrarlı bir şekilde gerçekleştiren öğrenme-kullanma süreci olarak değerlendirilebilir (Arslan,2020). Genel olarak konuşursak, makine öğrenmesi, bir bilgisayar programının belirli bir görev sınıfı ve performans ölçütleri açısından deneyim kazandıkça performansının iyileşmesidir. Böylelikle nesne tespiti ve doğal dil çevirisi gibi bilişsel (cognitive) görevleri yerine getirmek için analitik model oluşturma aşamasını otomatikleştirmeyi amaçlar. Bu durum, probleme göre eğitim verilerinden yinelemeli olarak öğrenen algoritmaların uygulanmasıyla gerçekleşir. Özellikle, sınıflandırma regresyon ve kümeleme gibi görevlerde makine öğrenimi uygulanabilirlik açısından iyi bir performans göstermektedir çünkü bu görevler yüksek boyutlu verilerle ilgilidir (Janiesch et al.,2021). Makine öğrenmesi, yapay zekânın gelişimi için son derece önemli bir çalışma alanıdır. Yapay zekâ sistemlerinin gelişebilmesi için yüklenen denklemleri zaman içinde kendiliğinden değiştirebilmesi gereklidir; bu ise makine öğrenmesi ile mümkündür. Aksi halde yapay zekâ, kendisini geliştirmeyi reddeden verimsiz bir insan gibi olacaktır. Yapay zekânın sadece bizim gibi düşünmesi değil, bizden farklı bir şekilde düşünebilmesi, en önemli avantajlarından birini oluşturmaktadır. Makine öğrenmesi tekniklerini kullanan programlar, doğrudan ve insan kontrolünde olmadıkları için bir problem karşısında ki çözümleri ve yaklaşımları insanlarınkinden farklı olmaktadır (Ergün,2023).", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 27, "page_end": 27}}
{"content": "16 Yapay zekâ alanındaki Hesaplamalı Öğrenme Teorisi (Computational Learning Theory), ham verilerden öğrenen, sistemi eğiten ve bu eğitim verilerine göre tahminler yapan algoritmaları analiz edip incelemiştir ve bu doğrultuda, makine öğrenimi kavramı gelişerek ortaya çıkmıştır. Geleneksel makine öğrenimi algoritmalarının temeli bir eğitim kümesi kullanıp bu modeli eğitmektir (Şekil 3.1). Önceden eğitilmiş model, test veri kümesini sınıflandırmak ve tanımak için kullanılmaktadır (Şekil 3.2) (Chauhan and Singh,2018). Makine öğreniminin temeli aslında veriden anlam çıkarma ve veriyi yorumlama üzerine kuruludur. Burada dikkat edilmesi gereken nokta, modeli eğitmeden önce yapılması gereken ayarlamalardır. Verilerle çalışırken doğru ve kaliteli verilerle çalışmak önemlidir. Modeli eğitip karar verme sürecine geçmek önemli olsa da modelinizi eğitmek ve test etmek için kullandığınız verilerin kalitesinden de sorumlusunuz. Veri kalitesi tek ölçüt olmamakla beraber başlangıç adımı için en önemli faktördür. Çünkü veri, bilginin ham, işlenmemiş halidir. Bu nedenle, veriyi işleyip kullanılabilir hale getirmek, makine öğrenimi sürecinin ilk adımıdır. Makine öğrenimi, geçmiş veri kümesini eğiterek geleceğe yönelik tahminlerde bulunmak için kullanılan algoritmalardan oluşur. Teknolojinin ve internetin yükselmesiyle artan veri miktarı makinenin öğrenmesi ve analiz etmesi için çok büyük miktarda veri bulunduğu anlamına gelmektedir. Makine öğrenimi, Denetimli Öğrenme (Supervised Learning), Denetimsiz Öğrenme (Unsupervised Learning) ve Pekiştirmeli Öğrenme (Reinforcement Learning) olarak üç ana başlık altında incelenmektedir. Bu Şekil 3.1 Makine öğreniminde bir modelin eğitilmesi Şekil 3.2 Test verisi için karar verme süreci", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 28, "page_end": 28}}
{"content": "17 teknikler, makine öğrenimi probleminin türüne ve girdi verilerine göre değişmektedir (Alaskar and Saba,2021). 3.1.1 Denetimli Öğrenme (Supervised Learning) Denetimli Öğrenmede, algoritma hem girdileri hem de istenen çıktıları içeren bir veri kümesinden matematiksel bir model oluşturur. Bu algoritmaların eğitilmesi etiketlenmiş verileri kullanarak gerçekleştirilir; bu yüzden hem girdiler hem de istenen çıktılar bilinir. Denetimli öğrenmede, algoritma doğru çıktılarla birlikte girdileri alır ve kendi tahmin ettiği çıktıyı doğru çıktılarla karşılaştırarak hataları bulur. Daha sonra model buna göre değiştirilir (Chahal and Gulia,2019). Bu süreçte amaç, giriş verileri ile çıkış verileri arasında ilişkiyi öğrenmektir. Pratikte, makine öğreniminin büyük bir kısmı denetimli öğrenmeyi kullanır. Burada, giriş değişkenleri (x) ve çıkış değişkeni (Y) ile ifade edilir. Girdi değişkeninden çıktı değişkenine bir eşleme fonksiyonu (mapping function) öğrenmek için bir algoritma kullanılır. 𝑌=𝑓(𝑥) Amaç, giriş değişkenleri (x) verildiğinde, o verinin çıkış değişkenlerini (Y) gerçeğe en yakın şekilde tahmin edecek bir eşleme fonksiyonu (mapping function) bulmaktır. Bu yönteme denetimli öğrenme denmesinin sebebi, algoritmanın eğitim veri kümesinden öğrenme sürecinin bir öğretmenin öğrenme sürecini denetlemesine benzetilmesidir (Asongo et al.,2021). Çoğu makine öğrenimi algoritması, bazı girdileri aldığında, sınıflandırma görevleri için ayrık (discrete) değerler veya regresyon görevleri için sürekli (continuous) değerler üreten tahmin modelleri oluşturur. Bu algoritmalar, denetimli öğrenmenin bir parçasıdır (Morales and Escalente,2022). Denetimli öğrenme için kullanılan veri kümesinde, her bir örnek için n adet girdi özniteliği (𝑥ଵ,𝑥ଶ,𝑥ଷ, ⋯,𝑥௡)ve bir hedef özniteliği (y) bulunmaktadır. Bu veri kümelerinin “etiketli veri kümesi” şeklinde adlandırılmasının sebebi içerdiği örneklerin etiket olarak adlandırılan hedef özniteliğine sahip olmasıdır. Denetimli öğrenmede eğitim veri kümesi ile model eğitilir ve yeni girdiler için hedef özniteliğini değeri tahmin edilir (Turan ve Polat,2023).", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 29, "page_end": 29}}
{"content": "18 Denetimli öğrenme, Regresyon (Regression) ve Sınıflandırma (Classification) olmak üzere ikiye ayrılır. Regresyon probleminin çözümü her zaman sürekli (continuous) bir değer alır. Örneğin, bir aracın alması gerektiği yola bağlı olarak ne kadar hıza ulaşması gerektiğini tahmin etmek bir regresyon problemidir. Sınıflandırma ise her zaman kategorik bir değer üretir. E-postaları spam veya spam olmayan şeklinde kategorilere ayırmak bir sınıflandırma problemi örneğidir. Naive Bayes, Destek Vektör Makineleri (Support Vector Machines-SVM), K-En Yakın Komşu (K-Nearest Neighbor-KNN), Lojistik Regresyon (Logistic Regression) denetimli öğrenme yönteminde sınıflandırma problemleri için kullanılan algoritmalardır. Ayrıca, denetimli öğrenme yönteminde Karar Ağacı (Decision Tree) ve Sinir Ağı (Neural Network) gibi bilinen algoritmalarda geniş bir kullanım alanına sahiptir (Kapoor and Dimple,2022; Alaskar and Saba,2021).  Yapay Sinir Ağları (Artificial Neural Networks-ANN) Yapay Sinir Ağları (Artificial Neural Networks-ANN), bilgisayarların gözlemsel verilerden öğrenmesine olanak sağlayan ve beyin sinir ağlarının biyolojik modellemesinden esinlenerek oluşturulan bir tekniktir. Yapay sinir ağlarında gerçekleşen işlemler, ağırlıklı “bağlantılar” sistemiyle ve bir veya birden fazla “gizli katman” ile etkileşim kurup ağa veri sunulmasıyla gerçekleşmektedir. Yapay sinir ağları tahmin, sınıflandırma, veri ilişkilendirme ve filtreleme, veri yorumlama gibi alanlarda yaygın olarak kullanılmaktadır (Çapalı,2022). Yapay zekâ çalışmalarının temeli Turing makineleriyle atılmıştır ve bu zamandan beri yapay zekâ üzerinde en çok araştırma yapılan konu “Yapay Sinir Ağları” olmuştur. Temelde, insan beyninin işleyişi baz alınarak geliştirilmiş bir teknolojidir. Yapay sinir ağı, bir bilgiyi depolayıp onu kullanışlı hale getirmek için doğal eğilim gösteren basit birimlerden meydana gelen paralel dağıtılmış bir işlemcidir. Bilginin öğrenme süreci yoluyla ağ tarafından elde edilmesi ve sinaptik ağırlıklar olarak belirlenen nöronlar arası bağlantıların bilgiyi depolamak için kullanılması gibi yönleriyle insan beyninin işleyişine oldukça benzemektedir. Yapay sinir hücresi, biyolojik sinir hücresinin tanımından hareketle ortaya çıkan bir algoritmadır. Yapay sinir hücresi diğer sinir hücrelerinden aldığı sinyalleri toplar ve toplam sinyal birikimi belli bir eşik değerini aştığı anda, kendi sinyalini bir başka sinir hücresine iletir. Biyolojik sinir ağlarının sinir hücreleri olduğu gibi yapay sinir ağlarının da yapay sinir hücreleri bulunmaktadır ve 5 temel elemanı vardır. Bunlar; girdiler, ağırlıklar, toplama fonksiyonu, aktivasyon fonksiyonu ve çıktılardır (Ataseven,2013). Yapay sinir ağının temel yapısı Şekil 3.3 ‘te sunulmuştur (Öztürk ve Şahin,2018).", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 30, "page_end": 30}}
{"content": "19 En popüler yapay sinir ağı türü, Girdi Katmanı (Input Layer), Gizli Katman (Hidden Layer) ve Çıktı Katmanı (Output Layer) olmak üzere üç katmandan oluşmaktadır (Şekil 3.4). Girdi Katmanı (Input Layer), yapay girdi nöronlarından meydana gelen katmandır. Bu nöronlar, ilk nöron katmanlarından bilgiyi alarak sistemin işlemesi için iletir ve girdi katmanı sayesinde sinir ağının iş akışı başlamış olur. Gizli Katman (Hidden Layer), girdi ve çıktı katmanlarından oluşur ve yapay nöronların girdisi ve çıktısı, girdilerin sayısına göre ağırlıklandırılır. Çıktı Katmanı (Output Layer), yapay sinir ağındaki son nöron katmanı, programcıya belirli çıktılar sağlayan bir çıktı katmanıdır. Bu katmandaki nöronlar, ağın son “performans sergileyen” düğümleri olduğu için farklı şekilde oluşturulabilir ve ele alınabilir (Qamar and Zardari,2013). Şekil 3.4 Yapay sinir ağı modeli Şekil 3.3 Yapay sinir ağı modelinin temel yapısı", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 31, "page_end": 31}}
{"content": "20 Temel yapay sinir ağı modelini ele alacak olursak (Bkz.Şekil 3.3), girdilerden elde edilen veri ağırlık değerleri nörona iletilmekte ve nörona iletilen ağırlıkların etkisi ile girişin değeri belirlenmektedir. Nöronda oluşan bu değer ise her bir giriş ve girişe ait ağırlıkların çarpımının bir toplamıdır. Bu değer bir aktivasyon fonksiyonuna uygulanmakta ve nöronun çıkışını belirlemektedir. Aktivasyon fonksiyonu nöronun davranışını belirleyen en önemli etmenlerden biridir. Genel olarak en yaygın kullanılan aktivasyon fonksiyonları doğrusal (linear) sigmoid, bipolar sigmoid ve hiperbolik tanjant gibi aktivasyon fonksiyonlarıdır. Aktivasyon fonksiyonu genelde türevlenebilir ve doğrusal olmayan bir fonksiyondur (Duman vd.,2024; Çelik,2022). 3.1.2 Denetimsiz Öğrenme (Unsupervised Learning) Denetimsiz Öğrenme modellerinde, bir uzman tarafından etiketlenmiş veriler yoktur. Etiketli olmayan salt veriler bulunmaktadır ve model bu verilerle eğitilmektedir. Bu öğrenme yönteminde, sadece girdi verileri modele verilir ve veriler üzerinde herhangi bir işaretleme yapılmaz. Denetimsiz öğrenmede öğrenilen bilgi, modelin aldığı veriler içerisinde bulunan gizli örüntülerdir. Yani, model veya sistem otomatik olarak keşifler yaparak ilişki ağını ortaya koymaya çalışmaktadır. Denetimsiz öğrenme özellikle etiketli veriyle çalışmanın zor veya sınırlı olduğu durumlarda tercih edilmektedir (Kırat ve Aydın,2023; Kızılkaya ve Oğuzlar,2018). Denetimsiz öğrenme, büyük veri analizinde oldukça yaygın kullanılmaktadır fakat etiketlenmemiş veri kümeleriyle çalışıldığı için öğrenme sonrasında elde edilen sonucun değerlendirilmesi zordur. Ayrıca denetimsiz öğrenme, verilerin içerisindeki gizli örüntüleri ve yapıları belirleyerek verileri farklı kümelere ayırmak veya verilerin boyutlarını düşürmek için kullanılmaktadır. K-Ortalamalar (K-Means), Hiyerarşik Kümeleme (Hierarchical Clustering), DBSCAN (Density-Based Spatial Clustering of Applications with Noise) ve literatürde Temel Bileşen Analizi (Principal Component Analysis-PCA) olarak bilinen yöntem gibi birçok denetimsiz öğrenme algoritması bulunmaktadır (Zhang,2022). Denetimsiz öğrenmede kullanılan veri kümesinde sadece n adet girdi özniteliği (𝑥ଵ,𝑥ଶ,𝑥ଷ, ⋯,𝑥௡) bulunur ve hedef özniteliği bulunmaz. Bu yüzden, denetimsiz öğrenmede kullanılan veri kümesi “etiketsiz veri kümesi” olarak adlandırılmaktadır (Turan ve Polat,2023). Denetimsiz öğrenme yöntemleri, etiketsiz veri kümesi üzerinde Kümeleme (Clustering) ve Boyut İndirgeme (Dimensionality Reduction) problemlerini çözmek için kullanılmaktadır. Kümeleme, veri setinde bulunan etiketsiz verilerden benzer özelliklere", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 32, "page_end": 32}}
{"content": "21 sahip ve homojen dağılmış alt grupların tespit edilmesini hedeflemektedir. Boyut İndirgeme, verinin yüksek boyutlu bir özellik uzayından düşük boyutlu bir özellik uzayına indirgenmesi işlemidir. Çok sayıda özellik içeren veri kümeleri ile çalışıldığında, makine öğrenmesi yöntemleri ile yapılan çözümler uzun süreler aldığı ve karmaşık modeller gerektirdiği için boyut indirgenmesi yöntemi problemin çözümüne katkı sağlamaktadır. Ayrıca, literatürde Yarı Denetimli Öğrenme (Semi-Supervised Learning) olarak bilinen bir yöntem daha bulunmaktadır. Yarı Denetimli Öğrenme yöntemi, denetimli ve denetimsiz öğrenmenin bir birleşimi olarak ifade edilebilir. Bu yöntemde ise az sayıda etiketli veri bulunduğu için tahmin işleminde hem etiketlenmiş hem de etiketlenmemiş veriler kullanılmaktadır (Hatipoğlu vd.,2023). 3.1.3 Takviyeli Öğrenme (Reinforcement Learning) Takviyeli Öğrenme (Reinforcement Learning-RL), bir hedef tarafından yönlendirilen öğrenme türüdür. Takviyeli öğrenme sürecinde, ajan (agent) olarak adlandırılan bir kavram, deneme-yanılma yoluyla bilinmeyen bir çevre (environment) ile etkileşime girerek öğrenme sürecini gerçekleştirir. Ajanın çevre ile olan bu etkileşimi, küçük bir çocuğun bir davranışı yapıp yapmayacağına çevresi ile etkileşiminden sonra aldığı tepki doğrultusunda öğrenmesine benzetilebilir. Ajan, ortamdan iki şekilde geri bildirim alır; ödül veya ceza. Ortamdan alınan bu geri bildirimler sayesinde kendini eğiterek ortam hakkında bilgi ve tecrübe sahibi olur. Takviyeli öğrenme problemleri, her durum için en uygun eylemin ne olduğunu öğrenmeyle ilgili olup, ortamdan alınan geri bildirimler sayesinde toplam ödülü maksimize etmeyi yani en üst seviyeye çıkarmayı amaçlar. Takviyeli öğrenme ajanı, bir uzman veya dış etmen tarafından ona ne yapması gerektiği söylenmeden de seçim yaparak öğrenmek zorundadır. Aslında bu iki durum bize bu öğrenme türünün karakteristik özelliklerini ifade eder. Ajanın ortam hakkında ki mevcut bilgisini kullanıp daha önce o durum karşısında gerçekleştirilmiş eylemi gerçekleştirme yoluna gitmesine yararlanma (exploiatation), o durumda daha önce hiç denenmemiş eylemleri denemesine keşfetme(exploration) denmektedir. Ajan bu iki seçimden birini yapmak zorundadır. Takviyeli öğrenme modelinin temel yapısı Şekil 3.5’ te sunulmuştur (Naeem et al.,2020). Şekil 3.5 Takviyeli öğrenme modeli", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 33, "page_end": 33}}
{"content": "22 Son dönemde oldukça popüler olan ve güçlü yeteneklere sahip olan Büyük Dil Modellerinin (Large Language Models-LLM) neredeyse tamamı, eğitim sonrasında performanslarını daha da artırmak için takviyeli öğrenmeden yararlanmaktadır. Büyük dil modelleri tarafından benimsenen takviyeli öğrenme yöntemleri ikiye ayrılmaktadır: İnsan Geri Bildiriminden Takviyeli Öğrenme (Reinforcement Learning from Human Feedback- RLHF) ve Yapay Zekâ Geri Bildiriminden Takviyeli Öğrenme (Reinforcement Learning from Artificial Intelligence Feedback-RLAIF). RLHF, geleneksel takviyeli öğrenme yaklaşımlarını içermektedir. Bu yöntemler, bir ödül modelinin eğitilmesini gerektirmekte olup, genellikle karmaşık ve istikrarsız bir süreç içerirler. Bu doğrultuda, takviyeli öğrenmenin büyük dil modellerinin eğitim sonrası süreci için oldukça büyük rol oynadığı, modellerin daha tutarlı ve daha yüksek performans sergilemesini sağladığı söylenebilir (Wang et al.,2025). 3.2 Derin Öğrenme: Yapay Sinir Ağları ve Derin Öğrenme Mimarileri Derin Öğrenme (Deep Learning-DL), makine öğrenmesinin bir alt dalı olarak bilinmektedir. Derin öğrenme; sinir ağları, yapay zekâ, grafik modelleme, optimizasyon ve örüntü tanıma faaliyetlerinin sentezinden ortaya çıkmıştır. Derin öğrenme ağları, sinir ağlarının gelişmiş bir versiyonudur ve daha güçlü tahminler yapma amacıyla kullanılmaktadır. Makine öğreniminin bir alt dalı olan derin öğrenme, denetimli ve denetimsiz öğrenme yöntemlerini kapsar ve çok katmanlı makine öğrenme modellerini kullanır. Ayrıca, çok sayıda gizli katmana sahip gelişmiş bir sinir ağından oluşmaktadır. Metin sınıflandırma, el yazısı algılama, görüntü tanıma, bilgisayarla görme, çoklu sınıflandırma ve regresyon problemleri gibi uygulama alanları bulunmaktadır (Sakarya ve Yılmaz,2019). Başarılı bir derin öğrenme modellemesi için büyük bir veri havuzuna, verileri eğitecek bir algoritmaya ihtiyaç duyulmasının yanı sıra, verilerin özniteliklerinin sınıflandırılmasına, transfer fonksiyonunun seçilmesine, ağ yapısının ve gizli katman sayısının belirlenmesine de dikkat edilmelidir. Derin öğrenme modelleri sinir ağlarının gelişmiş bir versiyonları oldukları için, genel olarak girdi katmanı, gizli katman ve çıktı katmanından meydana gelmektedir. Derin öğrenme probleminde ilk olarak problemin tanımı ve derin öğrenmeye uygunluğu tespit edilir. Ardından, ilgili veri kümeleri belirlenerek analiz için hazır hale getirilir ve kullanılacak algoritma modeli seçilir. Tanımlı olan verilerin, kullanılan algoritmaya göre analitik modelleri oluşturulduktan sonra seçilen algoritma ile eğitilir. Son olarak, test skorları elde etmek için model çalıştırılır ve sonuca göre ileriye yönelik tahminlerde bulunulur. (Akın ve Şahin,2024). Bu bölümde yapay sinir ağlarının türleri ve gelişmiş derin öğrenme mimarileri ele alınacaktır.", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 34, "page_end": 34}}
{"content": "23 3.2.1 Yapay Sinir Ağları ( Artificial Neural Networks- ANNs) Türleri Yapay sinir ağları yapılarına göre İleri Beslemeli Sinir Ağları (Feedforward Neural Networks) ve Geri Beslemeli Sinir Ağları (Feedback Neural Networks) olmak üzere iki kategoriye ayrılırlar.  İleri Beslemeli Sinir Ağları (Feedforward Neural Networks) İleri Beslemeli yapay sinir ağlarında, giriş katmanından çıkış katmanına doğru tek yönlü bağlantılarla nöronlar arasında iletişim gerçekleşir. Nöronlar bir katmandan diğer katmana bağlantı kurduğunda, aynı katmandaki nöronlar birbirleriyle bağlantı kurmazlar. İleri beslemeli yapay sinir ağlarında, nöronlar arasındaki bağlantılar bir döngü oluşturmazlar. Ayrıca, girilen verilere karşılık hızlı bir şekilde çıktı üretebilirler. Bu sinir ağlarının eğitilmesinde Geriye Yayılım Öğrenme Algoritması (Backpropagation Algorithm) kullanılmaktadır. Şekil 3.6’da 3 katmanlı bir İleri Beslemeli Yapay Sinir Ağı modeli sunulmuştur (Bayır,2006).  Geri Beslemeli Sinir Ağları (Feedback Neural Networks) Geri beslemeli yapay sinir ağlarında, bir nöronun çıktısı kendinden sonra gelen nöronun katmanına girdi olarak verilmez. Kendinden önceki katmanda bulunan bir nörona veya kendi katmanında bulunan herhangi bir nörona girdi olarak bağlanabilir. Geri beslemeli yapay sinir ağları doğrusal olmayan dinamik bir davranışa sahiptir. Bu sinir ağlarında, geri besleme bir geciktirme elemanı üzerinden yapılır. Geri beslemenin yapılış şekline göre farklı yapıda ve davranışta geri beslemeli Yapay Sinir Ağı yapıları elde edilebilir. Şekil 3.7’ de bir Geri Beslemeli Sinir Ağ modeli sunulmuştur (Öztürk ve Şahin,2018; Çetin vd.,2006) Şekil 3.6 İleri beslemeli yapay sinir ağı modeli", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 35, "page_end": 35}}
{"content": "24 Sinir ağlarının yapısı ve kullanım alanları kadar, nasıl öğrendikleri de büyük önem taşımaktadır. Derin öğrenme, sinir ağı tabanlı bir makine öğrenme yaklaşımı olup geleneksel yöntemlerden bu yönüyle ayrılmaktadır. Bu yüzden, derin öğrenmeden bahsederken özellikle sinir ağlarının temel yapılarını ele aldık. Peki, bir sinir ağı nasıl öğrenir? Bu noktada, geri yayılım (backpropagation) ve gradyan tabanlı öğrenme (gradient-based learning) kavramları karşımıza çıkmaktadır.  Geri Yayılım Algoritması (Backpropagation Algorithm) Geri Yayılım Algoritması (Backpropagation Algorithm), sinir ağlarının eğitiminde kullanılan en popüler ve en yaygın öğrenme algoritmasıdır. Bu algoritma; hataları geriye doğru çıkıştan girişe azaltmaya çalışmaktadır. Geri yayılım algoritmasının kuralı, ağ çıkışındaki mevcut hata düzeyine göre her bir tabakadaki ağırlıkları yeniden hesaplamaktır. Geri yayılımlı bir ağ modelinde giriş katmanı, gizli katman ve çıkış katmanı olmak üzere 3 tane katman bulunur ve problemin özelliklerine göre gizli katman sayısı artırılır. Bu algoritmanın temel prensibi çıkış için hedef değerleri ile ağ çıkışındaki değerler arasındaki hatayı minimize etmeye çalışmaktır. Geriye yayılım algoritması çok katmanlı ağlar için ağırlıkların bulunmasında kullanılır ve tek katmanlı ağlara yönelik değildir (Keleşoğlu,2006). Şekil 3.7 Geri beslemeli sinir ağı modeli", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 36, "page_end": 36}}
{"content": "25  Gradyan Tabanlı Öğrenme (Gradient Based Learning) Bilgisayar bilimlerinde, bir fonksiyonun belli parametreler altında en küçük değere indirgenmesi problemi birçok algoritmanın temelini oluşturmaktadır. Gradyan Tabanlı Öğrenme (Gradient Based Learning), sürekli ve düzgün fonksiyonların, kesikli ve karmaşık fonksiyonlara kıyasla daha kolay minimize edilebilmesi fikrine dayanmaktadır. Kayıp Fonksiyonu (Loss Function), minimize edilirken parametre değerlerinde yapılan küçük değişimlerin fonksiyon üzerindeki etkisi tahmin edilir. Tahmin edilen bu etki, kayıp fonksiyonunun parametrelere göre türevi olan gradyan hesaplanarak ölçülür. Gradyan vektörünün matematiksel yolla analitik olarak elde edilebildiği durumlarda, türev alma gibi sayısal yaklaşımlar gerekmeksizin daha etkili öğrenme algoritmaları geliştirilebilir (LeCun, et al.,1998). Gradyan İnişi (Gradient Descent), bir optimizasyon algoritmasıdır. Kayıp (loss), dediğimiz kavram bir sinir ağının tahmin ettikleri ile ağın gerçek veya beklenen çıktıları arasındaki farktır. Gradyan inişi optimizasyon algoritması ise sinir ağının parametrelerinde düzenlemeler yaparak sinir ağının performansını artırmayı hedefler. Bu algoritma, parametrelerin başlangıç değerlerini alarak türev hesaplarına dayalı işlemler yapar. Elde edilen bu değerleri mümkün olduğunca ağın doğru çalışmasını sağlayacak değerlere göre ayarlamak için kullanır (Dilmurod et al.,2021). Gradyan inişi, derin öğrenme modellerinin eğitiminde önemli bir rol oynar. Son yıllarda gradyan inişi algoritmasının performansını daha da iyileştirmek için çeşitli varyantları geliştirilmiştir. Newton yöntemi gibi yüksek mertebeden türevlere dayalı algoritmalara kıyasla, gradyan inişi ve birinci dereceden türev bilgisine dayanan çeşitli varyant algoritmaları, derin öğrenme modelleri için oldukça verimli ve pratiktir (Zhang,2019). 3.2.1 Derin Öğrenme Mimarileri Teknolojinin hızla ilerlemesi ve katlanarak artan veri miktarı, makine öğrenmesi alanına özellikle derin öğrenmeye olan ilgiyi her geçen gün artırmıştır. Dijital çağın başlaması ve internetin yaygınlaşması bu değişimlerin öncüsü olmuştur. Geçmişte, veri işleme süreçleri yıllık veya aylık bir zamanla sınırlıyken, günümüzde bu süreçler saatlere, dakikalara ve hatta saniyelere kadar hızlanmıştır. Bu doğrultuda, büyük veri setleri ile çalışmalar yapılması, geleneksel algoritmalarla gerçekleştirilemeyecek kadar zor bir hale gelmiştir. Derin öğrenme teknolojileri büyük veri setlerini anlamlı bilgilere dönüştürebilme açısından oldukça büyük bir potansiyel kazanmıştır. Derin öğrenme mimarileri, büyük veri setlerindeki gizli desenleri keşfetme, örüntüleri ortaya çıkarma yetenekleri sayesinde çok daha doğru sonuçlar elde edilmesini mümkün kılmıştır. Son zamanlarda, derin öğrenme teknolojisi, görüntü işleme, doğal dil işleme, ses tanıma ve otonom araçlar gibi birçok alanda devrim niteliğinde yenilikler sunmuştur.", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 37, "page_end": 37}}
{"content": "26 Sonuç olarak, derin öğrenme mimarileri de sürekli olarak gelişmiştir. Büyük dil modellerinin gelişimi açısından başlıca kullanılan derin öğrenme mimarileri arasında Evrişimsel Sinir Ağları (Convolutional Neural Networks-CNN) Tekrarlayan Sinir Ağları (Recurrent Neural Networks-RNN), Uzun Kısa Süreli Bellek (Long-Short Term Memory-LSTM), Kapılı Tekrarlayan Birim (Gated Recurrent Units- GRU), Transformers tabanlı modellerden bahsedebiliriz.  Evrişimsel Sinir Ağları (Convolutional Neural Networks-CNN) Bilgisayar bir görüntüye baktığında nesneleri, nesnelerin renklerini ve şekillerini, duruş biçimlerini insanların algıladığı şekilde algılayamaz ve görüntüyü bir sayı matrisi olarak görür. Evrişimsel sinir ağları, görüntüyü birden fazla gizli katmandan geçirir ve kullanıcıya “Kırmızı arabanın üzerinde duran biri büyük diğeri yavru iki köpek” gibi basit cümlelerle görüntüdeki nesneleri ve bu nesnelerin özelliklerini aktarır. Evrişimsel sinir ağları görüntü sınıflandırma, nesne tanımlama, görüntü segmentasyonu gibi işlemleri başarılı bir şekilde gerçekleştirmekte ve yapay sistemlerde nesnelerin tanımlanıp sınıflandırılmasını amaçlamaktadır (Kayaalp ve Süzen,2018). Evrişimsel sinir ağları; evrişimsel(convolution), düzleştirme (rectified linear unit), havuzlama(pooling) ve tam bağlantı (fully connection) katmanlarından oluşur. Bu işlem basamakları elde edilecek görüntü netliğine göre farklı sayıda ve boyutta tasarlanabilmektedir. Şekil 3.8’ de işlem basamaklarının sıralaması sunulmuştur. (Akın ve Şahin,2024). Evrişimsel katmanlarda, her birim, belirli filtrelerle bir sonraki katmandaki özelliklerle bağlantı kurar. Bu katmanlardan elde edilen ağırlıklar doğrusal olmayan bir fonksiyon yardımıyla işlenir. Bu katmanın amacı, bir önceki katmandaki özelliklerin niteliklerini belirlemek olsa da anlam bakımından benzer özellikleri tek bir nesnede birleştirmek havuzlama katmanının görevidir. Havuzlama katmanları, belirli filtrelerle bir Şekil 3.8 Evrişimsel sinir ağı katmanları", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 38, "page_end": 38}}
{"content": "27 önceki katmandaki verileri, adım adım kaydırarak alır ve böylelikle özellik matrisinin boyutunu küçültür. CNN mimarisindeki evrişimsel katman, havuzlama katmanı ve tam bağlı katman birden fazla kez kullanılabilir (Tan ve Karaköse,2022).  Tekrarlayan Sinir Ağları (Recurrent Neural Networks-RNN) Tekrarlayan Sinir Ağlarında (Recurrent Neural Networks-RNN), birimler arasındaki bağlantılar yönlendirilmiş bir döngü oluşturur. Oluşturulan bu döngü, bir ağ iç durumu oluşturur ve bu durum dinamik zamansal davranış sergiler. Tekrarlayan sinir ağları, ileri beslemeli sinir ağlarından farklı olarak kendi giriş belleğini girdilerin rastgele dizilerini işlemek için kullanabilir. RNN’lerin temel amacı sıralı bilgileri kullanmak olup görüntü tabanlı verilerde tüm girdilerin veya çıktıların birbirinden bağımsız olduğunu varsaymaktır. Ancak, doğal dil işleme gibi zaman değişkeni olan durumlar için böyle bir bağımsızlık mümkün değildir. Örneğin, bir cümle içinde bir sonraki kelimeyi tahmin etmek için, o anki kelimeden önce hangi sözcüklerin geldiği bilinmelidir. Tekrarlayan sinir ağı mimarisinin yinelenen (recurrent) olarak adlandırılma sebebi, cümledeki kelimeler gibi bir dizinin her öğesi için aynı görevin, önceki çıktılara bağlı yerine getiriliyor olmasıdır (Şeker vd.,2017). RNN’nin genel yapısı Şekil 3.9’da sunulmuştur. İlk olarak ağa bir 𝑋଴ girdisi girer ve bu girdi işlemler sonucunda ℎ଴ olarak çıkar. Ardından, girdi olarak yeni bir bilgi olan 𝑋ଵ ve bir önceki adımın çıktısı olan ℎ଴ girer. Yeni adımda ise yeni girdi olarak 𝑋ଶ ve bir önceki adımın çıktısı olan ℎଵ girer. Bu işlemler bu şekilde kendini yineleyerek devam eder (Arslankaya ve Toprak,2020).  Uzun Kısa Süreli Bellek (Long-Short Term Memory-LSTM) Uzun Kısa Süreli Bellek (Long-Short Term Memory-LSTM), tekrarlayan sinir ağlarının gelişmiş bir versiyonu olarak ortaya çıkmıştır. RNN’lerin uzun dizilerdeki bilgileri etkili şekilde koruyup kullanamama sorununu çözen LSTM’ler ilk kez 1997 yılında tanıtılmış ve 2013 yılına kadar daha da geliştirilerek derin öğrenme alanında oldukça popüler olmuştur. Bir LSTM ağında, belir bir anın girdisi ve önceki anın çıktısı, Şekil 3.9 Tekrarlayan sinir ağı mimarisi", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 39, "page_end": 39}}
{"content": "28 LSTM birimine iletilir. Bu birim bir çıktı üretir ve bu çıktı bir sonraki adıma aktarılır. LSTM ağlarında, son zaman adımındaki gizli katman genellikle sınıflandırma amacıyla kullanılır. LSTM, üç ana kapıdan oluşur: giriş kapısı (input gate), unutma kapısı (forget gate) ve çıkış kapısı (output gate) LSTM’nin üst düzey mimarisi (a) ve iç yapısı (b) Şekil 3.10 ‘da sunulmuştur (Shiri et al.,2024). Mimari olarak LSTM, önemli bilgi ve önemsiz bilgilerin belirlendiği kapılara sahiptir. Bu kapılar önemli olan bilgilerin hatırlanmasını ve önemsiz olan bilgilerin ise unutulmasını sağlar. Unutma kapısı (Forget Gate), unutulacak olan bilgiyi belirler. Eğer ağ, bilginin önemsiz olduğunu öngördüyse ilgili olan girdinin ağırlığını 0 olarak belirler. Eğer önceki katmanda bulunan aktivasyon fonksiyonunun çıktısı 1’e yakınsa bilgi tutulacak, 0’a yakınsa bilgi unutulacaktır. Girdi kapısı (Input Gate), tıpkı kendisinden önce gelen unutma kapısında olduğu gibi önce bilginin tutulup tutulmayacağına karar verir ardından bu bilgi ağın güncellenmesinde kullanılır. LSTM mimarisinde bilginin taşınması görevi hücre durumu birimi tarafından yapılmaktadır. Hücre durumu birimi, tutulan bilginin diğer hücrelere taşınmasını sağlayarak veri akışını gerçekleştirir. Son olarak, çıktı kapısı (output gate) tahmin yaparak sıradaki katmana iletilecek olan değeri belirler (Dayan ve Yılmaz,2022).  Kapılı Tekrarlayan Birim (Gated Recurrent Unit- GRU) Kapılı Tekrarlayan Birim (Gated Recurrent Unit-GRU), bellek hücrelerine sahip başka bir tekrarlayan sinir ağı (RNN) türüdür. LSTM’lere benzerler fakat hücre yapıları daha basittir. Ayrıca GRU, hücre durumu üzerinden bilgi akışını kontrol etmek için bir geçit (kapı) mekanizmasına sahiptir ancak daha az parametre içerir ve bir çıkış kapısı yoktur. GRU, sıfırlama kapısı (reset gate) ve güncelleme kapısı (update gate) olmak üzere iki kapıdan oluşur. Sıfırlama Kapısı (Reset Gate), önceki belleğe yeni girişlerin ne Şekil 3.10 Uzun kısa süreli bellek mimarisi", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 40, "page_end": 40}}
{"content": "29 kadarının aktarılacağını düzenler. Güncelleme Kapısı (Update Gate), önceki belleğin ne kadarının korunacağını belirler. GRU’yu LSTM ile karşılaştırırsak, güncelleme kapısı giriş ve unutma kapılarının birleşimi gibidir ve çıktının üretilme şeklinde bir değişiklik yapmaz. Standart LSTM modellerine göre daha az karmaşıklığa sahiptir. LSTM daha büyük ve karmaşık veri setleri için verimliyken, GRU’nun daha küçük veri setlerinde daha verimli bir performans sergilediği gözlemlenmiştir. Şekil 3.11’de GRU mimarisi sunulmuştur (Shewalkar,2018; Özdağ,2019). 3.3 Makine Öğrenimi ve Derin Öğrenmenin Karşılaştırılması Makine Öğrenimi (Machine Learning-ML) ve Derin Öğrenme (Deep Learning- DL), sağlık, finans, otonom sürüş ve doğal dil işleme gibi birçok sektörde devrim yaratmıştır. Geleneksel makine öğrenme yöntemleri büyük ve karmaşık verileri analiz etme de yetersiz kalmıştır. Karmaşık veriler için doğru özellikleri elle belirlemek çok fazla alan bilgisi gerektiren ve zaman alan bir iş olduğundan geleneksel makine öğrenimi yöntemleri daha az gelişmiş mimariler üzerine kuruludur. Genellikle düşük boyutlu özelliklere sahip girişleri girdi olarak alırlar. Ön işleme adımlarının bir parçası olarak, özellik seçimi ve boyut indirgeme kullanılarak verilerin, az gelişmiş modellerin işleyebileceği daha az boyutlu bir uzaya indirgenmesi gerekmektedir. Ancak, bu tür sıkıştırma veri kaybına yol açar ve orijinal yüksek boyutlu uzaydaki verilerin birbirine nasıl bağlı olduğunu dikkate almaz. Derin öğrenme mimarileriyle birlikte birçok doğrusal olmayan dönüşüm kullanılabilir aynı zamanda ham verilerden hiyerarşik temsiller ve bilgilendirici özellikler otomatik olarak öğrenilebilir. Derin öğrenme, makine öğreniminin bir alt dalı olup karmaşık veri desenlerini modellemek, örüntüleri keşfetmek için çok katmanlı sinir ağlarını kullanmaktadır. Birçok işlem seviyesine sahip derin öğrenme modellerinin her katmandaki verilerin daha zengin ve daha soyut temsillerini elde etmesi mümkündür. Katmanlı özellik çıkarma süreci beynin bilgiyi soyutlamasına benzer. Derin Sinir Ağları (Deep Neural Networks-DNN), görüntü tanıma, doğal dil Şekil 3.11 Kapılı tekrarlayan birim mimarisi", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 41, "page_end": 41}}
{"content": "30 işleme ve konuşma tanıma gibi alanlarda önemli başarılar elde etmiştir. Evrişimsel Sinir Ağları (Convolutional Neural Networks-CNN), Tekrarlayan Sinir Ağları (Recurrent Neural Networks-RNN) gibi tekniklerin geliştirilmesi ise makinelerin daha önce yalnızca insan zekasına özgü görevleri yerine getirmeleri adına önemli ilerlemeler olmuştur (Patil et al.,2024; Hussain,2024). Makine öğrenmesi ve derin öğrenmeyi karşılaştırdığımızda, karmaşık ve çok boyutlu görevlerden biri olan dil modelleme görevi içi derin öğrenmenin daha avantajlı olduğu ortaya çıkmaktadır. Derin öğrenme tabanlı modeller, çok katmanlı yapıları sayesinde dilin karmaşık yapısını ve çok boyutlu ilişkilerini daha etkin bir şekilde öğrenir ve temsil eder. Ancak, geleneksel makine öğrenimi yöntemlerinin temsil etmede sınırlı yeteneğe sahip olması ve genellikle veri kaybına yol açması nedeniyle dilin karmaşık yapısını temsil etmekte yetersiz kalmaktadır. Derin öğrenme tabanlı yaklaşımlar dilin zengin yapısını öğrenmede ve temsil etmede önemli bir avantaj sağlar. Geleneksel yöntemler ise genellikle daha basit yapılar üzerine kuruludur ve dildeki ilişkileri yakalamakta zorlanır. Sonuç olarak, derin öğrenme modelleri daha doğru ve kapsamlı temsiller oluşturarak dil modelleme görevlerinde ki performansı artırmaktadır.", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 42, "page_end": 42}}
{"content": "31 4.DİL MODELLERİ İnsan-bilgisayar etkileşiminin önemli ve en göz önünde olan örneklerinden biri hiç şüphesiz yapay zekâ araçlarıdır. Her geçen gün daha fazla kitleye hitap eden GPT, Gemini, Copilot benzeri sohbet robotları (chatbotlar) bu alana olan ilgiyi arttırmış ve zamanla geliştiricileri tarafından daha büyük veri setleri üzerinde eğitilerek daha iyi versiyonları sunulmuştur. Hızla gelişen teknolojik gelişmeler doğrultusunda karşımızdaki yazılımın bizi nasıl anladığı, isteklerimizi nasıl yerine getirdiği ve bize nasıl cevap verdiği merak konusu olmuştur. Bu bölümde dil modellemenin tanımı ve önemi, dil modelleri ve kullanım alanları, dil modellerinin eğitim süreçlerine değinilecek ve bu konulara açıklık getirilmeye çalışılacaktır. 4.1 Dil Modellemenin Tanımı ve Önemi Dil, insanların iletişim kurması, kendini ifade etmesi ve makinelerle etkileşiminde temel bir rol oynamaktadır. Makinelerin çeviri, özetleme, bilgi erişimi, sohbet etkileşimleri gibi karmaşık dil görevlerini yerine getirmesine ilişkin artan talepler genelleştirilmiş modellere olan ihtiyacı da beraberinde getirmiştir (Naveed et al.,2024). Dil Modellemesi (Language Modeling), bir kelime dizisindeki bir sonraki kelimenin otomatik olarak tahmin edilmesine dayanır. Dil modellemenin temel amacı, önceki kelimelerin sırası göz önüne alındığında, bir dizideki bir sonraki kelimeyi tahmin etmektir. Girdi, genellikle “bağlam” veya “önceki bağlam” olarak adlandırılırken, çıktı ise tahmin edilen bir sonraki kelimeyi ifade etmektedir. Örneğin “Bu belge Doğal Dil _____ hakkında” cümlesi için eksik kelimeyi tahmini etmeniz istendiğinde fark etmeden zihinsel olarak dil modelleme görevini yerine getiriyorsunuz. Birçok mesajlaşma, e-posta ve kelime işlemci uygulamasında bulunan otomatik tamamlama bu görevin gerçek dünya uygulamalarına örnek olarak verilebilir. Dil modelleme, konuşma tanıma ve metin çeviri sistemlerinin temel bileşenlerinden biri olmuştur. Son zamanlarda, OpenAI’nın GPT ürünlerinde olduğu gibi, doğal dil girdisine karşılık gelen bir dizi “sonraki” kelimenin tahmin edilmesi genel amaçlı sohbet uygulamaları içinde kullanılmaya başlamıştır. İnsanlar dili kullanırken farkında olmadan birçok kurala uyarlar. Ayrıca, iletişimimizin büyük bir kısmı da tahmin edilmesi kolay günlük şeyler hakkındadır. Bir doğal dil işleme görevi olarak dil modelleme ise veri ve değerlendirme olmak üzere iki önemli kritere sahiptir. Metnin kendisi girdi ve çıktılardan oluşur. Dolayısıyla, dil modellerinin yalnızca metine ihtiyacı vardır. Ayrıca, büyük metin koleksiyonunda bulunan her kelime önceki kelimelerin bağlamıyla gelir (Serrano et al.,2023).", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 43, "page_end": 43}}
{"content": "32 Dil, insanlarda erken çocukluk döneminde gelişen ve bir yaşam boyu gelişmeye devam eden ifade etme ve iletişim kurma konusundaki yetenektir. Makineler, güçlü yapay zekâ algoritmaları ile donatılmadıkça, insan dili biçiminde anlama ve iletişim yeteneklerini kavramakta zorlanırlar. Makinelerin bu hedefe ulaşabilmeleri için insanlar gibi okumasını, yazmasını ve iletişim kurmasını sağlamak uzun süre araştırmaların konusu olmuştur. Teknik olarak, dil modelleme makinelerin dil zekasını geliştirmeye yönelik önemli bir yaklaşımdır (Zhao et al.,2025). Dil Modelleme, doğal dil işlemenin merkezinde yer alan temel bir görevdir. Cümleler üzerinde doğru olasılık dağılımları kurabilen modeller, yalnızca dilbilgisi gibi karmaşık yapısal özellikleri öğrenmekle kalmaz, aynı zamanda çalıştıkları metin koleksiyonlarında bulunan bilgileri de büyük ölçüde özetler. Dilbilgisi açısından doğru olan ama gerçek hayatta pek rastlanmayan cümlelere düşük olasılık atayabilen modeller çeşitli dil görevlerinde de büyük avantaj sağlar. Dil modelleri, konuşma tanıma, makine çevirisi ve metin özetleme gibi doğal dil işleme görevlerinde önemli bir rol oynamıştır. Daha iyi dil modelleri geliştirmek bu görevlerin performansını doğrudan artırdığı için dil modeli eğitimi tek başına da oldukça önemli bir araştırma konusu olarak kabul görmektedir (Jozefowicz et al.,2016). İlk başta, dil modelleri eğitim verilerine dayalı görevleri yerine getirmek için tasarlandığından, insana benzer bir anlayış kapasitesine sahip değildi. İnsan beynini model alan sinir ağlarının dil modelleriyle birleştirilmesi, dil modeli performansının artmasında önemli bir rol oynamıştır. Geçmişte, verilen bir cümledeki bir sonraki kelimeyi tahmin etmeye odaklanan istatistiksel dil modelleri kullanılmıştır. Bu modellerin sinir ağlarıyla desteklenmesi dil modellerinin performansında önemli bir gelişme olmuştur. Ayrıca, sinir ağları dil modellerinin doğal dil işleme görevlerini daha geniş bir alana yaymıştır. Bir sinir ağı tabanlı dil modeli, basitten karmaşığa kadar çeşitli doğal dil işleme görevlerini yerine getirebilse de henüz makinelerde insan bilişini kusursuz taklit etme yeteneği yoktur ve bu alandaki çalışmalar devam etmektedir (Meeradaavi,2023). 4.2 Dil Modelleri ve Kullanım Alanları Dil Modelleri, günlük yaşamın birçok alanında yaygın bir şekilde kullanılmaktadır. İnsan-makine etkileşiminde kullanıcı deneyimini daha iyi bir hale getirmek ve bilgiye erişimi hızlandırmak gibi konularda oldukça fayda sağlamaktadır. Dil Modelleri, geniş bir yelpazeye sahiptir. Günümüzde dil modellerinin kullanım yelpazesi çoğunlukla sadece sohbet robotları (chatbots) ile ilişkilendirilse de sadece bununla sınırlı değildir. Bu bölümde büyük dil modelleri kavramı ve temel dil modelleri hakkında bilgi verilecek ve ardından dil modellerinin kullanım alanları incelenecektir.", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 44, "page_end": 44}}
{"content": "33 4.2.1 Dil Modelleri (Language Models) Dil modelleri, daha önce de belirtildiği gibi, başlangıçta insan dilinin olasılıksal temsilleri olarak geliştirilmiş ve eğitildikleri metin verisine dayanarak kelime dizilerinin olasılıklarını tahmin etmişlerdir. Son yıllarda, hem eğitim verisinin hacminin oldukça büyümesi hem de modellerdeki kavramlar arasındaki ilişkileri temsil eden, ağırlık (parametre) sayısı büyük ölçüde artmış; bu gelişmelerin bir sonucu olarak dil modellerine “büyük” sıfatı eklenmiştir ve dil modelleri artık Büyük Dil Modelleri (Large Language Models) olarak anılmaya başlanmıştır (Bekker,2024). Büyük Dil Modelleri (Large Language Models-LLM) kavramı, insan dilini üst düzeyde anlamak ve üretmek amacıyla derin öğrenme tabanlı sinir ağları üzerine inşa edilen bir yapay zekâ sistemini ifade etmektedir. Yapay zekâ kullanımı, LLM’lerin ve insan konuşmasını taklit etmeyi amaçlayan ChatGPT gibi sohbet robotlarının ortaya çıkması nedeniyle son yıllarda hızlı bir artış göstermiştir (Ünveren,2024). LLM’ler, yapay zekâ ve doğal dil işleme alanlarında önemli bir gelişme olarak ortaya çıkmıştır. Büyük dil modelleri oldukça büyük veri kümesiyle eğitilir. Bu modeller, doğal dildeki karmaşıklıkları anlayarak bu karmaşayı çözmek ve metin oluşturmak için kullanılır. Derin öğrenme algoritmaları olan LLM’ler, doğal dil işleme görevlerinin yapıtaşı olarak kabul görür. Ayrıca, bu modellerin başarısı doğrudan derin öğrenme tekniklerinin gelişmesine dayanmaktadır. Google’a ait BERT (Bidirectional Encoder Representations from Transformers), OpenAI’nın GPT (Generative Pre-trained Transformer) serisi gibi büyük dil modelleri, son yıllarda öne çıkan başarılı ve popüler gelişmelerdendir. Özellikle BERT modelinin Google tarafından tanıtılması büyük dil modellerinin gelişmesinde ve ilerlemesinde önemli bir rol oynamıştır (Akıllı ve Şimşek,2024). Makine öğrenmesi ve derin öğrenme alanlarında parametrelerin en önemlisi veridir. Üzerinde çalışılan problemin çözümü için konuyla ilgili yeterli miktarda veriye sahip olunmalıdır. Zaman kaybının ve yanlış bilgilendirmenin önüne geçebilmek için metinlerin içindeki ham veriden bağlamı doğru ifade edebilecek yetkinlikte sistemlere ihtiyaç vardır. RNN, LSTM ve GRU gibi sıralı verilerin eğitilmesi için kullanılan derin öğrenme mimarilerinin yerine 2017 yılında Google tarafından yayımlanan “Attention Is All You Need” makalesinde önerilen Transformer mimarisi geliştirilmiştir. Bu mimari, doğal dil işleme alanındaki çalışmalarda en son teknoloji olarak kabul edilmektedir (Karaca ve Aydın,2023).", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 45, "page_end": 45}}
{"content": "34 Transformer modelleri bağlamdan ve anlamdan öğrenmenin yanı sıra, bir cümledeki sıralı veriler (kelimeler) arasındaki ilişkilerden öğrenebilen bir sinir ağıdır. Bu modeller, uzak veri ögelerinin birbirini nasıl etkilediğini ve birbirine nasıl bağımlı olduğunu algılamak için, dikkat(attention) veya öz-dikkat (self-attention) adı verilen bir dizi matematiksel tekniği uygular (Gomez et al.,2023). Büyük dil modelleri arasında, OpenAI tarafından geliştirilen GPT-4, Meta tarafından geliştirilen LLaMA, Google tarafından geliştirilen LaMDA gibi örnekler öne çıkmaktadır. Bu dil modelleri farklı derin öğrenme yöntemlerini kullanmaktadır. Literatürde yapılan deneyler, GPT-4’ün sahip olduğu parametre sayısının diğer dil modellerinden daha fazla olduğunu ve diğer dil modellerinden daha başarılı bir model olduğunu ortaya koymaktadır (Küçük ve Can,2023). İlk GPT versiyonu GPT-1 olup OpenAI tarafından 2018’de geliştirilmiştir. GPT-2 ve GPT-3 gibi geliştirilmiş versiyonlar, büyük dil modeli olarak adlandırılan bu yapay zekâ algoritmasını takiben geliştirilmiştir. 2019 yılında GPT-2, 2020 yılında ise GPT-3 tanıtılmıştır. GPT-3, 2023 yılında tanıtılan GPT-3.5 için bir temel oluşturmuştur. GPT, büyük miktardaki metin veri kümesi üzerinde önceden eğitilmiş ve doğal dil işleme görevlerini gerçekleştirmek için kullanılan yapay zekâ modelidir. GPT-4, GPT-4o modellerinden sonra OpenAI tarafından en son tanıtılan GPT-4.5 modeli ise şimdiye kadar ki en iyi ve en başarılı model olarak tanıtılmıştır (Demir,2024; OpenAI, 2025). GPT, işlenen kelime parçacığının (token) yalnızca sol bağlamına ulaşan tek yönlü bir dil modelidir. Bu tek yönlülük, akıl yürütme veya soru yanıtlama gibi görevlerde modelin performansını olumsuz etkileyebilir çünkü bu görevler için hem sol bağlamın hem de sağ bağlamın dikkate alınması gerekmektedir. Bu tek yönlü sınırlamayı aşmak için, Transformer tabanlı BERT modeli geliştirilmiştir. BERT, bir cümlenin hem sol bağlamını hem sağ bağlamını birleştirerek çift yönlü bir temsil sağlar. Böylelikle, BERT daha iyi bir bağlam çıkarımı yapar (Gillioz et al.,2020). Google tarafından geliştirilen bir diğer önemli dil modeli, metinden metine dönüşüm yapan T5 modelidir. Bu model, çeviri, özetleme ve metin sınıflandırma gibi birçok doğal dil işleme görevlerini yerine getirmek için tasarlanmıştır. T5 dil modeli, göreve özgü ince ayar (fine-tuning) sürecini kolaylaştırarak, farklı uygulamalarda daha iyi performansı hedefleyen bir metinden metine yaklaşımı kullanılarak eğitilmiştir. Bu metinden metine yaklaşımı sayesinde, T5 dil modeli büyük bir metin verisi üzerinde ön eğitim aşamasından geçirildikten sonra belli görevler için ince ayar yapılır. Örneğin, çeviri, özetleme, duygu analizi ve soru-cevaplama gibi doğal dil işleme görevlerinin tümü, girdi-çıktı şeklinde metin çiftleri olarak ele alınmaktadır. Böylece, T5 dil modeli çeşitli görevler için modele ince ayar yapma sürecini kolaylaştırır. Ayrıca, ince ayar", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 46, "page_end": 46}}
{"content": "35 yapma sürecinin kolaylaşması, bir görevden elde edilen bilginin diğerlerine uygulanabildiği transfer öğrenmeyi desteklemektedir (Geçici,2024). Ticari büyük dil modelleri haricinde bazı kurumlar tarafından paylaşılan büyük dil modelleri de bulunmaktadır. LLaMa modeli açık erişimli ve oldukça büyük bir dil modelidir. Meta şirketi tarafından yayımlanan LLaMa modelinin 7,13,33 ve 65 milyar parametre içeren versiyonları bulunmaktadır. Bu modeller 1 ve 1.4 trilyon token ile eğitilmiş modellerdir (Arslan,2023). 2022 yılında Google, “Language Model for Dialogue Applications “(Diyalog Uygulamaları için Dil Modeli) ifadesinin kısaltması olan LaMDA dil modelini tanıtmıştır. LaMDA dil modeli, önceki Google araştırmaları sayesinde, diyaloğa dayalı olarak eğitilen Transformer tabanlı dil modellerinin geniş bir konu yelpazesine sahip olduğunu göstermiştir. LaMDA sinir ağlarına dayalı gelişmiş bir derin öğrenme modeli kullanmaktadır ve çoğu dil modelinden farklı olarak, LaMDA diyaloğa dayalı şekilde öğrenir. Ayrıca, yanıtlarının özgünlüğü ve mantıksal tutarlılığı da yapılan ayarlamalarla önemli oranda geliştirilebilmektedir. LaMDA’ya erişim Google’ın AI Kitchen platformu üzerinden gerçekleştirilir (Demiröz,2025). Günümüzde yapay zekâ alanında en kritik unsurun veri olduğu kabul edilmektedir. Bununla birlikte, bazı kurumlar tarafından açık kaynak kodlu modeller geliştirilmekte ve kullanıma sunulmaktadır. Bu modellerin önceden eğitilmiş olmaları, modelin uzun süren eğitim süresini önemli ölçüde azaltarak zaman kaybının önüne geçmektedir. Modellerin ön eğitimli olması maliyet ve hesaplama gücü açısından önemli avantajlar sağlamaktadır. Bu modellere erişim sağlayan kurumlar veri paylaşımı konusunda daha temkinli davranmaktadırlar. Modelin eğitilmesini bir beslenme süreci gibi düşünürsek, aslında verinin bu süreçte temel besin kaynağı olduğu söylenebilir. Son yıllarda yapay zekâ sistemlerini eğitecek veri kaynaklarının sınırlı hale geldiği ve bu nedenle sentetik veri üretimine olan ilginin arttığı da ifade edilmektedir. Açık kaynak yazılımlar özel bir telif hakkı lisansı ile kaynak kodların herkes tarafından incelenmesine, kullanılmasına ve dağıtılmasına imkân tanıyan; böylece kullanıcının yazılımı değiştirebilmesine imkân sağlayan sistemlerdir. Açık kaynak yazılımların geçmiş yıllarda tercih edilme sebebi, kaynak ve zaman tasarrufunun sağlanmasıyken; günümüzde yüksek kaliteli, güvenilir ve güvenli olarak kabul edilmeleri etkili olmuştur. Bu yazılımlar, birçok uzman tarafından dikkatli incelendiği ve hatalardan arındırıldığı için güvenli olarak kabul edilmektedir (Yılmaz ve Tarhan,2018).", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 47, "page_end": 47}}
{"content": "36 Büyük dil modellerinin çoğu kaynak açısından zengin kuruluşlar tarafından geliştirilmekte ve genellikle kamuya açıklanmamaktadır. Transformer tabanlı, 176 milyar parametreli ve ROOTS veri kümesi üzerinde eğitilmiş BLOOM dil modeli, Sorumlu Yapay Zekâ Lisansı (Responsible AI License) kapsamında piyasaya sürülmüş açık kaynak erişimli dil modellerinden biridir. Önceden eğitilmiş dil modelleri, genellikle daha az etiketli veriden daha yüksek performans yakaladıkları için doğal dil işleme sistemlerinin yapıtaşlarından biri haline gelmiştir. Önceden eğitilmiş dil modellerinin herhangi bir ek eğitim gerektirmeden işe yarar görevleri yerine getirebildiği ve oldukça etkin olduğu gözlemlenmiştir. Çevresel kaygıların yanı sıra, büyük dil modellerinin eğitim maliyeti de yalnızca kaynak açısından güçlü kuruluşlar tarafından karşılanabilir seviyededir. Ayrıca, yakın zamana kadar büyük dil modellerinin çoğu açık kaynak kod şeklinde erişime sunulmadığı için araştırma topluluklarının büyük bir kısmı büyük dil modeli geliştirme sürecinden dışlanmıştır. Bu dışlanmanın sonuçlarından biri de çoğu LLM’in İngilizce metinler üzerinde eğitilmiş olmasıdır (BigScience WorkShop,2023). 4.2.2 Dil Modellerinin Kullanım Alanları Büyük Dil Modelleri, insan diline oldukça yakın ve yüksek bir doğrulukta metinler oluşturabilirler. Bu metinleri oluştururken de yapay zekânın potansiyelinden yararlanırlar. OpenAI tarafından geliştirilen GPT ve türevleri, yapay zekâ-insan etkileşimi üzerinde derin etkiler yaratmış, insan dilini anlama ve üretme konusunda etkileyici bir performans sergilemiştir. Derin öğrenme mimarileri tabanlı ve sinir ağları üzerine inşa edilen bu modeller, çeşitli doğal dil işleme görevlerini yerine getirebilirler. Metin üretimi, çeviri, özetleme ve yaratıcı yazma gibi karmaşık görevlerde de başarı sağlamışlardır. LLM’ler; müşteri hizmetleri, sağlık hizmetleri, eğitim gibi birçok alanda gerçek dünya uygulamalarında aktif olarak kullanılmaktadır (Pal,2023). Büyük dil modellerinin, derin öğrenme tabanlı sinir ağı mimarisi üzerine kurulu modeller olduğunu ve doğal dil işleme görevlerinde yüksek başarı sağladıklarını önceki bölümlerde ifade etmiştik. Şekil 4.1 ‘de çeşitli doğal dil işleme problemlerinde kullanılan başlıca derin öğrenme yöntemlerini özetleyen bir tablo sunulmuştur (Küçük ve Arıcı,2018).", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 48, "page_end": 48}}
{"content": "37 Bu görevlerde kullanılan derin öğrenme mimarileri aynı zamanda büyük dil modellerinin de temelini oluşturmaktadır. Bu yüzden, Şekil 4.1’de sunulan tablo geleneksel derin öğrenme yöntemlerini kapsamakla beraber, bu derin öğrenme yöntemleri tabanlı olan büyük dil modellerinin etkili olduğu doğal dil işleme problemlerini de kapsar. Yapay Zekâ, tıp, finans, eğitim gibi çeşitli alanlarda yaygın olarak kullanılmaktadır. Yapay zekânın bir alt dalı olan derin öğrenme, büyük veri kümeleri üzerinden öğrenme yeteneğine sahip yapay sinir ağlarını kullanır. Bu yöntem, görüntü tanıma, ses işleme ve doğal dil işleme gibi görevlerde insan performansının üstünde sonuçlar üretmektedir. Yapılan bir araştırma sonucunda katılımcıların ileride ki çalışma hayatlarında yazılım geliştirme yeteneklerini geliştirme konusunda farkındalığa sahip oldukları gözlemlenmiştir. Ancak, ChatGPT gibi popüler ve son kullanıcıya hitap eden uygulamaları bilmelerine rağmen arka planda yer alan Büyük Dil Modelleri ve Transformers gibi yapay öğrenme yöntemleri hakkında pek fikir sahibi olmadıkları sonucuna varılmıştır (Yazar vd.,2024). Büyük Dil Modelleri, yapay zekâ ve doğal dil işleme alanlarında önemli gelişmelere öncülük etmiştir. GPT ve benzeri mimariler, bağlama uygun ve tutarlı metinler üretebilmeleri sayesinde birçok sektörde vazgeçilmez araçlar haline gelmiştir. Eğitim alanında, kişiselleştirilmiş öğrenme deneyimlerinin önünü açarak, öğrencilere geri bildirime dayalı etkileşimli öğrenme ortamları sunmaktadırlar. Araştırmalar, LLM’lerin öğrenci ihtiyaçlarına uygun alıştırmalar sunarak katılımı ve motivasyonu artırabildiğini ortaya koymaktadır. GPT-4 gibi büyük dil modelleri, öğrencilerin öğrenme deneyimlerini önemli ölçüde iyileştirmiştir. LLM’ler öğrencilerle etkileşim kurma, karmaşık konuları özetleme, dil anlama ve üretme yetenekleri sayesinde onların sorularına gerçek zamanlı Şekil 4.1 Doğal dil işlemede kullanılan derin öğrenme yöntemleri", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 49, "page_end": 49}}
{"content": "38 yanıtlar verebilmektedir. Böylece, öğrencilerin ihtiyaçlarına göre şekil almaktadır. Örneğin, LLM’lerin tıp eğitimindeki öğrencilerin sorduğu sorulara etkili şekilde cevap verebildiği ve karışık konuları ele almada başarılı olduğu gözlemlenmiştir (Çetin ve Onan,2025). Bilişim teknolojisinde yaşanan hızlı büyümenin en dikkat çekici örneklerinden biri, 2022 yılının sonlarında üretken yapay zekâ uygulaması ChatGPT ‘nin piyasaya sürülmesi ve yalnızca beş günde 1 milyon kullanıcıya ulaşmış olmasıdır. ChatGPT’nin yanı sıra Google Bard ve BingAI gibi metin tabanlı, DALL-E ve Midjourney gibi görsel içerik üreten, Amper ile MuseNet gibi müzik alanında çalışan ve CodeStarter, Codex ile GitHub Copilot gibi kod üreten üretken yapay zekâ araçları da bulunmaktadır. Bilim ve tasarım alanında Ar-Ge amacıyla kullanılan uygulamalarda mevcuttur. İlaç geliştirme alanında kullanılan Moderna ve dünya Go şampiyonu olmayı başaran AlphaGo Zero, üretken yapay zekâ temelli sistemlerdir. 2022 yılında iş dünyasından uzmanların katılımı ile gerçekleştirilen bir araştırmada, üretken yapay zekânın en çok pazarlama ve reklamcılık alanında kullanıldığı tespit edilmiştir (Ünal ve Kılınç,2024). Duygu Analizi (Sentiment Analysis), doğal dil işlemenin bir alt dalıdır. Duyarlılık analizi, sosyal medyadaki görüşlerin çıkarılması ve belirlenmesi, ürünler hakkındaki görüşlerin analiz edilmesi gibi çeşitli alanlarda uygulanmaktadır. Ön eğitimli dil modellerinden biri olan BERT, büyük ölçekli Transformer tabanlı bir modeldir. BERT, çift yönlü ve maskelemeli dil modelleme (masked language modelling) ile sonraki cümle tahmini (next sentence prediction) teknikleriyle eğitilmektedir. 2018 yılında Google AI Language tarafından geliştirilen BERT, bir dil temsil modeli olup Twitter verileri üzerinde yapılan duygu analizinde yaygın olarak kullanılmaktadır. BERT ’in duygu analizi görevinde üstün sonuçlar elde ettiği yapılan çalışmalar doğrultusunda kanıtlanmıştır. Bu model, kelimelerin konumunu ve sırasını dikkate alır ardından cümleler arasındaki sıralamayı tahmin ederek kelimelerin bağlamını anlar. BERT modelinin bu yapısı duygu analiz görevlerinde özellikle tercih edilmesini sağlar (Yürütücü ve Demir,2023; Gazaz ve Ayvaz,2024). Arama motorları (search engines), kullanıcıların girdiği belirli anahtar kelimeler ile bilgiye erişmek için arama yapan ve ilgili web sayfalarının listelendiği uygulamalardır. Yapay zekâ teknikleri, arama motorlarında kullanıcıların arama sorgularının anlaşılması, sonuçların doğru bir şekilde sıralanması ve kişiselleştirilmiş sonuçların sunulması için kullanılmaktadır. BERT modeli, soru-cevap sistemleri, anlamsal analiz, duygu analizi ve kelime benzerliği gibi birçok doğal dil işleme görevinde gözle görülür başarılar elde etmiş çift yönlü bir dil modelidir. Özellikle uzun mesafedeki bağımlılıkları daha iyi modellemesi sayesinde, kullanıcıların arama sorgularını daha iyi anlamak amacıyla Google’ın sıralama algoritmasında kullanılmaktadır. BERT modeli, Google’ın arama", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 50, "page_end": 50}}
{"content": "39 sonuçlarını daha doğru hale getirebilmek için düzenli olarak güncellenmektedir. 2020 yılında OpenAI ile bir ortaklık kuran Microsoft, Bing Arama motoru için ChatGPT modelini kullanarak, kullanıcıların arama sorgularını daha iyi anlamayı ve daha doğru sonuçlar sunmayı hedeflemektedir. Ayrıca, Microsoft Bing ’in müşteri hizmetleri süreçlerinde de ChatGPT modeli aktif olarak kullanılmaktadır (Sevli,2023). ChatGPT, literatür tarama sürecini hızlandırması, nitelikli metinler yazılmasına olanak tanıması, araştırmalarda gelişmiş dil ve çeviri desteği sunması gibi imkânlarla akademik ve bilimsel yazı yazma alanında da önemli katkılar sağlamaktadır. Sağlık alanında ise, her ne kadar tıbbi kararlarda son sözün bir sağlık uzmanı tarafından söylenmesi gerekse de hastanın semptomları ve tıbbi geçmişi göz önünde bulundurularak tedaviye yönelik destek ve öneriler sunmak için ChatGPT kullanılmaktadır (Özdemir ve Güven 2025). Yapay zekâ araçlarının temelini oluşturan büyük dil modelleri, bu teknolojilere yönelik artan taleple birlikte kullanım alanlarını her geçen gün genişletmektedir. İnsan gücünün ve dikkatinin sınırlı olduğu durumlarda ya da iş akış süreçlerinin hızlandırılmasında, büyük dil modelleri tabanlı yapay zekâ araçları sıklıkla tercih edilmektedir. Bu modeller, sadece veri işleme hızlarıyla değil, aynı zamanda karmaşık ilişkileri kavrayıp tutarlı yanıtlar verebilme yetenekleri sayesinde de geleneksel yöntemlerin önüne geçmektedir. Büyük dil modelleri bilgiye erişim şeklini de değiştirdiği için yalnızca teknik bir yenilik değildir. Bu modeller, bilgiye erişim şeklini değiştirmekle kalmayıp; eğitimden sağlığa, hukuktan müşteri hizmetlerine kadar birçok alanda kişiselleştirilmiş deneyim sunmada önemli bir rol oynamaktadır. Yapay zekâ araçları sadece yazılı metin üretmek için değil, aynı zamanda ses ve görüntü işlemede de oldukça başarılıdır. Doğal dil işlemeyle başlayan süreç, görüntü hatta ses işlemeye kadar ilerlerken, günümüzde koku ve tat alma üzerine yapılan çalışmalar da bulunmaktadır. Yapay zekânın birçok alt dalı, insanı taklit etmeye dayalı olarak ortaya çıkmıştır; görev tanımları bile bu temel üzerinden şekillenmiştir. İnsan beyninin karmaşık ve oldukça büyüleyici bir parçası olan sinir ağlarının, yapay sinir ağı olarak modellenmesi de buna güçlü bir örnektir. Bilim insanları tüm bu gelişmelerden yola çıkarak geleneksel makine öğrenmesinden bu yana yaptığımız sınıflandırmaların tat veya koku sınıflandırma içinde mümkün olup olmadığını araştırmışlardır. Yakın geçmişte elektronik burun ve elektronik dili beraber kullanarak hibrit cihazlar geliştirilmiştir. Bu teknolojilerin gıda sektörü, sanayi, tıp ve askeri alan gibi sektörlerde önemli bir yer edinebileceği öngörülmektedir. (Çetin ve Beyhan,2019).", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 51, "page_end": 51}}
{"content": "40 Son yıllarda, büyük dil modelleri farklı mimariler ve ölçeklerde (modelin boyutu, parametre sayısı, hesaplama gücü) geliştirilmiş çeşitli uygulamalarla birlikte evrim geçirmiştir. Tablo 4.1’de popüler büyük dil modellerinin bazı önemli özellikleri göz önünde bulundurularak, çeşitli dil işleme görevlerinde nasıl kullanıldığı güçlü yönleriyle ele alınmıştır. Bu karşılaştırma tablosu, dil modelleri arasındaki çeşitliliği vurgulamakta ve bu modellerin pek çok görevde nasıl dağıtıldığını anlamaya yardımcı olmaktadır (Fahim et al.,2024). Tablo 4.1 Dil modellerinin kullanım alanları", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 52, "page_end": 52}}
{"content": "41 5. TRANSFORMER (DÖNÜŞTÜRÜCÜ) TABANLI MODELLER Son yıllarda, doğal dil işleme (NLP) alanında hem genel görevlerde hem de özel görevlerde büyük gelişmeler meydana gelmiştir. Bu gelişmeler, tarihsel süreç içerisinde her doğal dil işleme görevinde olduğu gibi vektör temsilleriyle başlamış; ardından Transformer (Dönüştürücü) mimarisinin ve Transfer öğrenimi (Transfer Learning) yönteminin tanıtılmasıyla , doğal dil işlemenin “Altın Çağı” başlamıştır. Bu gelişmeler yaşanırken, daha fazla şirket chatbot teknolojilerini benimseyip kullanmaya başlamışlardır. Basit desen eşleştirme (pattern matching) ve otomatik destek sistemleriyle başlayan bu süreç, giderek daha karmaşık hale gelmiştir. Makine öğrenmesi teknikleri ve modelleri benimsenerek bu sistemlerin daha insan benzeri davranışlar sergilemesi sağlanmakta ve örüntü tanımaya dayalı yapılardan gerçek öğrenmeye bir geçiş yaşanmaktadır. Bu doğrultuda, makine çevirisi de daha tutarlı hale gelmiştir. Kodlayıcı- Çözücü (Encoder-Decoder) modelleri ve bu modellerden türetilmiş olan Transformer mimarisi, doğal dil işleme alanında temel görevlerden biri olan makine çevirisi alanında da başarılı olmuş ve otomatik çeviri uygulamaları günümüzde her zamankinden daha doğru hale gelmiştir. Metin üreten modellerin gelişmesi ile birlikte otomatik makale üreten uygulamalar ortaya çıkmıştır. Bu uygulamalar, insan tarafından üretilen metin ile yapay zekâ tarafından oluşturulmuş metinler arasındaki farkları giderek azaltmıştır (Galanis et al.,2021). Bu bölümde, öncelikle RNN,LSTM ve GRU gibi önceki mimarilerin dil modellemede karşılaştığı sorunlar ele alınacaktır. Ardından, Transformer mimarisinin temel bileşenleri açıklanarak bu mimarinin getirdiği yenilikler değerlendirilecektir. Bu kapsamda, Transformer tabanlı geliştirilen en başarılı ve popüler dil modellerinden GPT ve BERT modellerinin temel yapıları, geliştirilmiş versiyonları ile birlikte ele alınacak; son olarak ise bu modeller karşılaştırılarak aralarındaki farklar incelenecektir. 5.1 RNN, LSTM ve GRU Mimarilerinde Karşılaşılan Sorunlar Geleneksel makine öğrenme algoritmaları özel ve sınırlı problemlerin çözümünde etkilidir. Ancak, konu karmaşık ve büyük problemler olduğunda etkili bir çözüm sunamamaktadır. Özellikle, sürekli değişkenlik gösteren çok boyutlu veriler işlendiğinde bu sorunla daha çok karşılaşılır. Bu doğrultuda, yapay sinir ağları modeli üzerine kurulu olan derin öğrenme, karmaşık ve yüksek boyuta sahip bilgileri işlemede etkin şekilde kullanılır. Günümüzde derin öğrenme yöntemleri birçok alanda kullanıldığı gibi zaman serisi problemlerinde de kullanılmaktadır. Zaman verisi problemleri, zaman içinde geçmiş verilerin değişimlerinin analiz edilerek geleceğe yönelik eğilimlerini tahmin etmeyi konu alır.", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 53, "page_end": 53}}
{"content": "42 Bu problemlerde yaygın olarak RNN,LSTM ve GRU gibi mimariler kullanılır. Bu yöntemler, veri setlerindeki uzun mesafeli bağımlılıkları (long range dependencies ) ve karmaşıklıkları modellemekte oldukça etkilidir (Çelik,2024). Doğal dil işlemede uzun mesafeli bağımlılık kavramı, bir cümlede sözcükler birbirinden çok uzak olsa bile anlamsal olarak ilişkili olan sözcüklerin arasındaki ilişkiyi ifade etmektedir. Bir tekrarlayan sinir ağı mimarisinin (RNN) amacı, uzun mesafeli bağımlılıkları koruyarak bağlam bilgisini zaman içinde sürdürebilmektir. Aşağıdaki iki cümleyi ele alalım : Cümle 1: “ Ayşe odaya girdi. Leyla da içeri girdi. Ayşe ____’ya selam verdi.“ Cümle 2: “ Ayşe odaya girdi. Leyla da içeri girdi. Günün sonuydu ve herkes oldukça yorgun görünüyordu. Ayşe ____’ya selam verdi.” RNN’nin, her iki cümlede de birkaç zaman adımı önce ortaya çıkan ikinci kişi olan “Leyla” ismini bir sonraki kelime olarak tahmin etmesi önemlidir. RNN’lerin ilk cümledeki boşluğu doğru tahmin etme olasılığının ikinci cümledekinden daha yüksek olduğunu söyleyebiliriz. Bunun nedeni, geriye yayılım (back-propagation) aşamasında, gradyan değerlerinin katkısının zaman adımları ilerledikçe giderek azalmasıdır. Bu nedenle, uzun cümlelerde “Leyla” ismini kelime olarak tahmin etme olasılığı bağlamın boyutuna bağlı olarak azalır. Kaybolan Gradyan Problemi (Vanishing Gradient Problem), gradyan değeri sıfıra yaklaştığında ve bu durum fark edilmediğinde modelin birbirinden uzak kelimeler için öğrenme kalitesinin düşmesidir. Bunun bir sonucu olarak, model bağlamı doğru bir şekilde öğrenmede sorun yaşar. (EITCA Academy, 2023; Mohammadi et al.,2019). Dil modelleme, uzun mesafeli bağımlılıkların modellenmesini gerektiren önemli problemler arasında bulunmaktadır. Ancak, sinir ağlarına sıralı verilerdeki uzun mesafeli bağımlılıkları öğrenme yeteneğini kazandırmak pek kolay değildir. Tekrarlayan sinir ağları (RNN), yaygın kullanılmasına rağmen optimize edilmesi ne yazık ki zordur. RNN’lerin uzun mesafeli bağımlılıkları öğrenmesinin zor olma nedeni ise kaybolan gradyan (vanishing gradient) ve gradyan taşması ( gradient exploding) gibi sorunlardır. Bu nedenle yaygın bir yaklaşım olarak zaman adımları boyunca bilgi akışını daha iyi kontrol etmek amacıyla kapı mekanizmaları (gating mechanisms) içeren LSTM ve GRU gibi mimariler geliştirilmiştir. Bu mimari ailesinde, gradyanların birçok zaman adımını atlayabildiği etkili kısayollar oluşturularak kaybolan gradyan sorununun etkileri hafifletilmiştir. Kapılı RNN mimarilerinden en yaygın kullanılan modellerden biri Uzun Kısa Süreli Bellek (Long Short Memory-LSTM) ağları, özellikle konuşma tanıma gibi çeşitli makine öğrenimi görevlerinde başarılı performans sergilemektedir. LSTM, zaman içindeki bilgiyi taşıyan bellek hücrelerini kullanır ve bu bellek hücreleri unutma kapısı", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 54, "page_end": 54}}
{"content": "43 (forget gate), giriş kapısı(input gate) ve çıkış kapısıyla (output gate) kontrol edilir. Ancak bu gelişmiş kapılı RNN türü, modelin karmaşık hale gelmesine sebep olabilir. Ayrıca, RNN’ler için hesaplama verimliliği de oldukça kritik olduğundan son yıllarda alternatif mimarilerin daha sade olmasına yönelik çalışmalar yürütülmektedir. Deneysel olarak, önceki çalışmalar LSTM dil modellerinin ortalama olarak 200 bağlam kelimesini kullandığını göstermiştir. Bu da daha fazla gelişme alanının önünü açmıştır. LSTM modelinin daha basit bir versiyonu olan Kapılı Tekrarlayan Birim (Gated Recurrent Unit- GRU) modeli, bu kapsamda önerilen yapılardan biridir. GRU’da bulunan sıfırlama kapısı, özellikle dizide anlamlı kopmaların yaşandığı durumlarda faydalı olabilir. Örneğin, dil modellemede, anlamsal olarak birbirleriyle ilişkisi olmayan iki metin arasında geçiş yapıldığında avantajlıdır. Ancak, bazı özel görevlerde bu durumunda önemli bir katkı sağlamadığı görülmüştür (Dai et al.,2019;Ravanelli et al.,2017; Ravanelli et al.,2018). Bu çerçevede toparlamak gerekirse, RNN ve RNN tabanlı LSTM,GRU modelleri, dil modelleme gibi sıraya dayalı dediğimiz yani zamana bağımlı görevlerde uzun süre çok iyi performans göstermiştir ve bu alanda birçok araştırma yapılmıştır. Bu modellerde, her kelime sırası geldiğinde hesaplanır. Ancak bu işlem sırayla yapıldığından yani her bir işlem bir öncekine bağlı gerçekleştirildiğinden paralel şekilde işlemleri gerçekleştirmek mümkün değildir. Bu durum uzun metinlerde ve büyük veri setlerinde eğitim sürecini hem yavaşlatır hem de zorlaştırır. Hesaplama verimliliğini artırmak adına, faktörizasyon teknikleri ve koşullu hesaplama gibi yöntemler kullanılarak bazı araştırmalar yapılmıştır. Özellikle koşullu hesaplama yöntemi model başarısını artırmıştır. Ancak, sıralı hesaplamaya dayalı temel kısıtlama halen devam etmektedir. Dikkat mekanizmaları (attention mechanism), giriş ve çıkış dizilerindeki ögeler arasındaki bağımlılıkları, aralarındaki mesafeden bağımsız olarak modelleme imkanı sunsa da birkaç istisna dışında, bu tür mekanizmalar genellikle tekrarlayan ağ ile birlikte kullanılmaktadır. Transformer modeli ile tekrar eden yapıları tamamen terk eden ve yalnızca dikkat mekanizmasına dayalı olan yeni bir mimari model sunulmuştur. Transformer, giriş ve çıkışlar arasındaki tüm bağımlılıkları aynı anda modelleyebildiği için çok daha fazla paralel işlem yapılmasına imkan sağlar (Vaswani et al.,2017). 5.2 Transformer Mimarisinin Temel Yapıtaşları ve Getirdiği Yenilikler Transformer kavramı ilk kez Toronto Üniversitesi ve Google’daki araştırmacılar tarafından 2017 yılında geliştirildi ve “Attention Is All You Need” makalesinde tanıtıldı. Derin öğrenme modeli olan bu model doğal dil işleme alanında yaygın olarak, soru cevaplama, metin özetleme, metin sınıflandırma gibi görevlerde kullanılmaktadır. Büyük veri setlerini tek tek incelemek oldukça yavaş ve zor olduğundan Transformer bu gibi süreçlere büyük katkı sağlamıştır. Bu modeller, büyük veri setlerinde denetimsiz öğrenme", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 55, "page_end": 55}}
{"content": "44 ile eğitilir. Transformer modelleri son derece etkili olan ve doğal dil işlemede önemli bir ilerleme sağlayan yapay sinir ağı modelidir (Arzu ve Aydoğan,2023). Transformer modelinin geliştirilmesinde en önemli nokta dikkat mekanizması (attention mechanism) olmuştur. Dikkat mekanizması, girdi dizisine bakar ve dizinin her adımında önemini koruyan kısımları seçerek dizinin önemli olan bölümlerinin bilgisini saklar. Dikkat mekanizması, başka girdi verilerini de hesaba katarak bu girdilere farklı ağırlıklar verir. Bu ağırlıkların verilme sebebi, öncelik sırası belirlemektir. Tüm kodlayıcı katmanları, çıktıyı üretmek için uygun bilgileri alan bir dikkat mekanizması kullanır. Sonrasında, dikkat mekanizması sonucunda çıktı olarak gönderilen ağırlıkları ve kodlanmış şekilde olan diziyi girdi olarak alır. Çok başlı dikkat (Multi-Head Attention), dikkat mekanizmasındaki işlemleri paralel olarak çalıştırarak sonuçları birleştirir. Sonuç olarak, modelin farklı ilişkileri öğrenmesi sağlanmış olur (Tuncel vd.,2022). Transformer, sıralamadan bağımsız çalışan dikkat mekanizmalarıyla bilgiyi işler. Bu sayede model, sıralama ilişkilerine bağlı olmadan önemli bölümlere odaklanarak daha anlamlı temsiller oluşturulmasını sağlar. Ancak, doğal dil işleme gibi sıralı verilerde sırasal ilişkiler önemlidir. Bu yüzden, Transformer sıralama bilgisini modele kazandırmak için konumsal kodlama (positional encoding) yöntemini kullanır. Konumsal kodlama, her bir kelime parçacığına (token) sıralama bilgisi ekler ve dikkat mekanizmasının sırasal bağlamı öğrenmesine olanak sağlar. Bu yöntem, hem uzun mesafeli bağımlılıkların öğrenilmesine hem de kelimelerin sıralanış şeklinin öğrenilip etkin şekilde modellenmesini sağlar. Transformer mimarisi yalnızca dikkat mekanizması ve konumsal kodlamadan oluşmaz; aynı zamanda katmanlı mimari ve geleneksel derin öğrenme yöntemlerine göre daha fazla yapı barından bir modeldir. Bu yapılar; gömme boyutu (embedding dimension), ileri beslemeli ağ birimleri (feedforward units), artık bağlantılar (residual connections/skip connections), çok başlıklı dikkat (multi-head attention), maskeli dikkat (masked attention) sayısı ve normalizasyon (layer normalization) gibi bileşenlerden oluşur. Modelin öğrenme sürecinde bu bileşenlerin her biri önemli bir rol oynar ve her bir bileşen modelin genel performansını doğrudan etkiler (Katırcı ve Çelik,2024). Transformer modeli, bağlamdan ve anlamdan öğrenerek, cümledeki kelimeler gibi sıralı veriler arasında ilişki kuran bir sinir ağı yapısıdır. Bu modeller, birbirinden uzakta bulunan veri ögelerinin bile birbirini etkilediği ve birbiriyle bağ kurduğu çeşitli yolları tespit edebilmek için geliştirilen bir dizi matematiksel teknik uygular (Gomez et al.,2023).", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 56, "page_end": 56}}
{"content": "45 5.2.1 Transformer Mimarisin Temel Yapıtaşları Transformer mimarisi, verileri işlerken doğal dil işleme çalışmalarında kullanılan diğer mimarilerden farklı şekilde işlemektedir. RNN,LSTM ve GRU gibi önceki mimarilerde girdi dizileri, t-1 adımından gelen gizli durum ile t zamanındaki girdi vektörünün işleme sokulması üzerine tasarlanmıştır. Transformer mimarisindeki yaklaşım ise, bütün diziyi aynı zaman adımı içerisinde bir matris şeklinde modele girdi olarak almaktır. Doğal dil işleme çalışmalarında kullanılan kodlayıcı-kod çözücü (encoder-decoder) , yapısı Transformer mimarisinde de bulunmaktadır. RNN,LSTM ve GRU mimarilerindeki tekrarlayan sinir ağlarının yerine iki alt katmana sahip N adet Transformer kodlayıcı ve kod çözücü bloklar bulunmaktadır. Günümüzde, doğal dil işleme görevlerinde RNN,LSTM ve GRU gibi modellere kıyasla Transformer modeli daha çok tercih edilmektedir. Bunun sebebi, Transformer mimarisi kullanılarak yapılan çalışmalarda diğer mimarilerinin elde ettiği sonuçlardan çok daha iyi sonuçlar edilmesidir. Son zamanlarda, Transformer mimarisinin daha geliştirilmiş versiyonları bulunmaktadır. “Attention Is All You Need” makalesinde bahsedilen mimari Şekil 5.1‘ de sunulmuş olup temel mimarinin bileşenlerini ele alacağız (Karaca ve Aydın,2023; Vaswani et al.,2017). Şekil 5.1 Temel Transformer mimarisinin yapısı", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 57, "page_end": 57}}
{"content": "46  Kodlayıcı- Kod Çözücü (Encoder- Decoder) Transformer mimarisinin modeli için Kodlayıcı ( Encoder) ve Kod Çözücü (Decoder) şeklinde iki temel bileşen bulunmaktadır. Kodlayıcı Şekil 5.1 ‘ de genel yapısı verilen mimarinin sol tarafı ve kod çözücü ise sağ tarafıdır. Kodlayıcı, girdi verisini alarak dilin temsili olacak şekilde bir dizi vektör oluşturmaktadır. Oluşturulan bu vektörler, metindeki kelimelerin hem anlamlarını hem de bağlamlarını taşımaktadırlar. Kod Çözücü, bu vektörleri kullanarak yeni bir metin üretir veya mevcut metni işler. Kodlayıcı, girdi dizisini işleyerek bu girdinin anlamını temsil eden gizli bir temsil seti üretir ve bu set daha sonra kod çözücü tarafından kullanılır. Kod çözücü, kodlayıcıdan gelen temsil setleriyle birlikte önceki çıktıları kullanarak bir sonraki kelimeyi veya kelime parçacığını (token) tahmin eder. Tüm çıktı dizisi üretilene kadar bu işlem tekrarlanarak devam etmektedir. Her kodlayıcı ve kod çözücü, N defa tekrar edilir ve bu modelin daha derin öğrenme yapmasını mümkün hale getirir (Işık,2024). Kodlayıcı (Encoder), öz-dikkat (self-attention) ve ileri beslemeli sinir ağı (feed- forward neural network) olmak üzere iki bileşene ayrılırken, kod çözücü (decoder), ileri beslemeli ağ ile öz-dikkat katmanları arasına yerleştirilen ek bir kodlayıcı-kod çözücü (encoder-decoder) katmanına sahiptir (Klein and Nabi,2019).  Gömme ve Konumsal Kodlama ( Embedding and Positional Encoding) Gömme kavramı, özellikle kelime gömme (word embedding) olarak bilinir. Kelime gömme, kelime parçacıklarının anlamlarını, bağlamlarını ve aralarındaki ilişkileri matematiksel olarak ifade etmek için vektör temsillerini oluşturur. Transformer sıralı veri yapısına sahip olmadığından kelime gömme temsili tek başına yeterli olmaz. Bu nedenle, Transformer mimarisi kelime gömme (word embedding) ve konumsal kodlama (positional encoding) olmak üzere iki bileşen içerir. Konumsal kodlama modelin hem kelimenin anlamını hem de cümledeki konumunu öğrenmesini sağlar. Kelimenin cümledeki sırasını belirtmek için eklenen konumsal gömme olarak tanımlanabilir. Sinüs ve kosinüs fonksiyonlarıyla hesaplanan bu sinyaller, kelime gömmeye eklendiğinde model, sadece kelimenin bağlamına göre değil aynı zamanda kelimenin cümledeki sırasına göre de öğrenebilir. Modelin her kelime parçacığını temsil etmek için kullandığı Konumsal kodlama, Transformer mimarisinde her kelime parçasına (token) ait konumu belirtmek için kullanılır. Konumsal kodlamayı hesaplamak için iki denklem bulunmaktadır. Çift indeksli bileşenlerin değeri sinüs fonksiyonu kullanılarak hesaplanırken, aynı konum için tek indeksli bileşenlerin değeri kosinüs fonksiyonu kullanılarak hesaplanır. Her iki durum için denklemler sırasıyla 1 ve 2 numaralı denklem olarak sunulmuştur. Her bir pozisyon ve gömme boyutunun her bir boyutu i için konumsal kodlama PE(pos,i) şeklinde gösterilmektedir (Katırcı ve Çelik,2024).", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 58, "page_end": 58}}
{"content": "47 (1) 𝑃𝐸(𝑝𝑜𝑠, 2𝑖)= sinቆ ௞௢௡௨௠ ଵ଴଴଴ మ೔ ೏೘೚೏೐೗ ቇ (2) 𝑃𝐸(𝑝𝑜𝑠, 2𝑖 + 1)= cosቆ ௞௢௡௨௠ ଵ଴଴଴ మ೔ ೏೘೚೏೐೗ ቇ pos: Dizideki belirtecin konumu (0 tabanlı indeks) i: Boyut indeksi (0,1,2, …,d_model-1) d_model: Gömme boyutunun büyüklüğü  Dikkat Mekanizması (Attention Mechanism) Dikkat Mekanizması (Attention Mechanism), büyük miktardaki veriden az sayıda ama önemli olan bilgiyi seçer ve önemsiz bilgileri göz ardı ederek önemli bilgilere daha çok odaklanmayı hedefler. Öz-Dikkat (Self- Attention) mekanizması ise, dikkat mekanizmasının bir çeşidi olup verideki ilişkileri yakalamak konusunda oldukça başarılı ve etkilidir. Bu mekanizma, metin verisi için geçerli olan uzun mesafeli bağımlılıkları yakalama problemini ele alırken kelimeler arasındaki karşılıklı etkileşimi hesaplar. Ayrıca öz dikkat mekanizması, Transformer mimarisinin temelini oluşturmakla beraber modelin bir cümledeki belirli kelimeyi tahmin ederken diğer kelimelerin önemini ölçmesini sağlar. Bu sayede model, her kelimenin hedef kelimeyle olan ilgisine göre belirlenen ağırlıklarla beraber tüm kelimelerin değerlerinin (value) ağırlıklı toplamını hesaplar. Öz-dikkat mekanizması sorgu (query), anahtar (key) ve değer (value) vektörlerinin hesaplanması şeklinde üç temel adımdan oluşmaktadır. Sorgu vektörü, odaklanılmak istenen hedef kelimeyi temsil ederken, anahtar vektörleri cümledeki tüm kelimeleri temsil eder. Değer vektörleri ise her kelimeyle ilişkili bilgiyi saklar. Anahtar- değer (key-value) temelli dikkat formülü şu şekildedir: (3) Attention(Q, K, V)= softmax൬୕୏౐ ඥୢౡ ൰V Çok Başlı Dikkat ( Multi-Head Attention) , öz-dikkat mekanizmasının yaptığı işlemi birden fazla kez paralel olarak yaparak çalışır. Her dikkat başlığı, girdinin farklı bir yönüne odaklandığı için model, cümledeki kelimeler arasındaki farklı ilişkileri aynı anda öğrenebilir. Bu dikkat başlıklarının her biri kendi, sorgu (Q), anahtar (K), ve değer (V) ağırlıklarıyla çalışır ve her başlık için dikkat hesaplaması yapılır. Ardından hesaplamalar birleştirilerek elde edilen çıktı, bir ağırlık matrisine dönüştürülür. Çok başlı dikkat mekanizması sayesinde model, aynı anda hem aralarında kısa mesafe olan yerel ilişkileri hem de uzak ilişkileri öğrenebilir. Böylece bir cümledeki yakın ve uzak kelimeler arasındaki anlam ilişkileri daha iyi yakalanır (Liu et al.,2024).", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 59, "page_end": 59}}
{"content": "48 Matematiksel formülü şu şekildedir: (4) MultiHeadAttention(Q, K, V)= Concat[headଵ, … , head୦]W଴ Her bir başlık şu şekilde hesaplanır : (5) head୧ = Attention (QW୧ ୕, KW୧୏, VW୧୚) 5.2.2 Transformer Mimarisinin Getirdiği Yenilikler Doğal dil işleme alanında derin öğrenme ve transfer öğrenimi birçok görevde etkin bir şekilde kullanılmış ve başarılı sonuçlar elde edilmiştir. RNN tabanlı modellerin, verileri sıralı işleme prensibi nedeniyle paralel işlemeye yatkın olmaması ve uzun mesafeli bağımlılıkları yakalamada yetersiz kalmaları, bu modellerin hesaplama açısından darboğaza (bottleneck) girmesine neden olmuştur. 2017 yılında Google araştırmacıları tarafından yayımlanan “Attention Is All You Need” makalesinde tanıtılan Transformer modelleri bu sınırlamaları aşarak dikkat mekanizması (attention mechanism) ve kodlayıcı-kod (encoder-decoder) çözücü ağ mimarisini baz alarak tekrarlayan (recurrent) veya evrişimsel (convolutional) mimarilerin önüne geçmiştir. Yapılan çalışmalar, RNN tabanlı modellerin karmaşık ve yinelemeli yapıları nedeniyle Transformer mimarisine göre çok daha yavaş eğitildiğini ortaya koymuştur. Ayrıca, bazı makine çevirisi görevlerinde yapılan ilk deneyler, Transformer tabanlı modellerin kaliteli sonuç açısından daha başarılı olduğunu göstermiştir. Transformer mimarisi, paralel işlemeye uygun olduğu için eğitim süreleri önemli ölçüde azdır. Ancak, RNN tabanlı modellerin hem veriyi yalnızca sıralı işleyebilmesi hem de karmaşık katmanlı yapıya sahip olması eğitim sürelerini uzatmaktadır (Yıldırım,2024). Transformer’lar hızlı eğitim süreleri ve üstün performansları sayesinde ortaya çıktıkları ilk günden beri hızlı bir şekilde geleneksel sıralı (sequence) modellerin yerlerini almışlardır. Transformer’ın temel bileşenlerinden konumsal kodlama (positional encoding), bu hızlanmanın nedenlerinden biri olmuştur. Konumsal kodlama, girdi ögelerini (token veya kelime parçacığı) paralel olarak işlediği için konum bilgisi doğrudan kelime gömme (word embedding) vektörlerine kodlanır. Konumsal kodlamalar, ögeleri temsil eden vektörleri, dizideki konumlarına bağlı olarak öznitelik (feature) uzayında belirli bir yöne doğru hafifçe kaydırır ve bunu yaparken gömme vektöründeki bilgiyi bozmaz (Hè and Kabic,2023).", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 60, "page_end": 60}}
{"content": "49 Transformer, her giriş ögesi arasındaki ilişkiyi öz-dikkat mekanizması (self- attention mechanism) sayesinde yakalar. Transformer’ın bu özelliği, bir giriş dizindeki en ilgili ögelere odaklanmasını ve aynı zamanda genel bağlama dikkat ederek doğru çıktılar üretmesini sağlar. Bu mimarinin yapay zekâda önde gelen teknolojilerden biri olmasının temel nedeni, model mimarisindeki dikkat (attention) kavramıdır. Uzun bir giriş dizisiyle eğitilmiş transformer modeli , kısa giriş dizisiyle eğitilmiş bir modele göre çok daha fazla sayıda giriş ögesine dikkat edebilir. Böylece, uzun giriş verileriyle eğitim genellikle daha fazla bağlamsal bilgi yakalar. Örneğin, DNA dizisi analizi, uzun belge özetleme,, görüntü segmentasyonu gibi uzun mesafeli bağımlılıkları olan bir görev için daha yüksek ve doğru tahminler yapar (Wang et al.,2023). Transformer mimarisinin yapıtaşlarından biri olan öz-dikkat mekanizması (self- attention mechanism), geleneksel tekrarlamalı (reccurent) ve evrişimsel (convolutional) katmanlarıyla karşılaştırıldığında hesaplama açısından önemli avantajlar sunmaktadır. Bu mimarinin tercih edilme nedenlerini üç madde altında toparlayabiliriz: 1. Katman başına toplam hesaplama karmaşıklığı 2. Paralel olarak yürütülebilecek işlem miktarı 3. Uzun mesafedeki bağımlılıkların öğrenilmesi için yol uzunluğu Özellikle uzun mesafeli bağımlılıkları öğrenmek, sıralı veri işlemede oldukça hayati bir zorluktur. Girdi ve çıktı dizilerinin konumları arasında, ileri ve geri sinyallerin (bilgi) aldığı yollar ne kadar kısa olursa, bu tür bağımlılıkların öğrenilmesi de o kadar kolaylaşmaktadır. Bir öz-dikkat katmanı (self-attention layer), tüm konumları sabit sayıda ardışık işlemle bağlayabilirken, tekrarlamalı katmanlar O(n) ardışık işlem gerektirmektedir. Bu durum, temsil boyutunun (d) , sıralı uzunluktan (n) büyük olduğu doğal dil işleme görevlerinde öz-dikkat mekanizmasını daha avantajlı ve verimli hale getirir. Çok uzun dizilerle çalışırken hesaplama yükünü azaltmak için öz-dikkatin yalnızca belirli bir komşuluk (r) içinde sınırlandırılması önerilmektedir. Bu durumda maksimum yol uzunluğu ise O(nr) olmaktadır. Evrişimsel katmanların karmaşıklığı, kullanılan çekirdek boyutuna bağlı olarak arttığı için genellikle tekrarlamalı katmanlardan daha pahalıdır. Ancak, ayrık evrişimler (separable convolutions) kullanıldığında, bu karmaşıklık önemli ölçüde düşer. Yine de, bu yöntemden elde edilen karmaşıklık, öz-dikkat katmanları ile noktasal ileri besleme katmanlarının (point-wise feed-forward layers) birleşimiyle benzer bir karmaşıklığa sahiptir. Öz-dikkat mekanizması, yalnızca performans açısından değil, aynı zamanda modelin yorumlanabilirliği açısından da avantaj sağlar. Deneysel sonuçlar, farklı dikkat başlarının (attention heads) belirli görevlerde uzmanlaştığını ve bazılarının cümlelerin anlamsal yapısına duyarlı davranışlar sergilediğini ortaya koymuştur (Vaswani et al.,2017).", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 61, "page_end": 61}}
{"content": "50 Transformer, RNN’nin karşılaştığı kaybolan gradyan problemini (vanishing gradient problem) aşan bir problemdir. Transformer mimarisi teorik olarak, herhangi bir uzunluktaki diziyi girdi olarak işleyebilir. Ancak pratikte, girdi uzunluğu (n) ve temsil boyutu (d) olmak üzere, 𝑂(𝑛ଶ.𝑑) karmaşıklığa sahiptir. Bu durum, hesaplama açısından uzun dizilerle çalışmayı pahalı hale getirmektedir. Bu nedenle, Transformer modellerininin eğitimi sırasında kullanılan yaklaşımda genellikle, girdi keyfi uzunlukta dizilere bölünür ve mimari bu diziler arasında herhangi bir bilgi akışı olmadan eğitilir. Bu yaklaşım, verinin ardışık bölümlerinden gelen bağlam bilgisinin göz ardı edilmesine sebep olur bu da birtakım sınırlamaları beraberinde getirir : 1. Önceden tanımlanmış girdi uzunluğunu aşan uzun mesafeli bağımlılıkların yakalanamaması 2. Dizilerin herhangi bir anlamsal ilişki gözetilmeden bölünmesiyle ortaya çıkan bağlam parçalanması problemi (context fragmentation problem) Bağlam parçalanması, dizilerin ilk birkaç ögesinin sınırlı bağlam bilgisi ile işlenmesine neden olur, bu da modelin etiketleri doğru şekilde çıkaramamasına sebep olur. Sonuç olarak verimsiz bir optimizasyon ve düşük bir performansla karşılaşılır (Atzeni,2021). Transformer mimarileri, uzun mesafeli bağımlılıkları öğrenme potansiyeline sahip olsa da, dil modellemesi bağlamında sabit uzunluktaki bağlam penceresiyle sınırlıdır. Bu sınırlamayı aşmak amacıyla, Transformer-XL adlı yeni bir sinir ağı mimarisi önerilmiştir. Bu mimari; segment düzeyinde yinelemeli (recurrence-based) bir mekanizma ve yeni bir konumsal kodlama (positional encoding) yöntemi içermektedir. Önerilen yöntem, yalnızca uzun mesafeli bağımlılıkları öğrenmekle kalmaz, aynı zamanda bağlam parçalanması (context fragmentation) problemini de ortadan kaldırır. Böylece, Transformer-XL, RNN’lere kıyasla %80 , klasik Transformer (Vanilla Transformer) modeline kıyasla %450 daha uzun mesafeli bağımlılıkları öğrenebilmektedir. Ayrıca, hem kısa hem de uzun dizilerde daha iyi performans göstermekte ve değerlendirme aşamasında Vanilla Transformer’lara göre 1.800 kata kadar daha hızlı çalışmaktadır (Dai et al.,2019). Son yıllarda Transformer model mimarileri, dil işleme, görsel analiz ve pekiştirmeli öğrenme gibi birçok alanda gösterdiği başarılarla büyük ilgi görmüş ve dikkat çekmiştir. Özellikle doğal dil işleme alanında Transformer mimarisi, modern derin öğrenme yaklaşımlarında vazgeçilmez bir hale gelmiştir. Yakın dönemde, orijinal Transformer mimarisi üzerine çeşitli iyileştirmeler getiren çok sayıda “X-former” modeli önerilmiştir. Reformer, Linformer, Performer ve Longformer gibi modellerin büyük çoğunluğu, hesaplama ve bellek verimliliğini artırmayı hedeflemektedir (Tay et al.,2022).", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 62, "page_end": 62}}
{"content": "51 5.3 GPT ve BERT Modelleri Transformer (Dönüştürücü) tabanlı dil modelleri, doğal dil işleme alanında son yıllarda çığır açan bir gelişme olmuştur. Bu modeller arasında özellikle öne çıkan iki model bulunmaktadır: Üretken Ön Eğitimli Dönüştürücü anlamına gelen GPT (Generative Pre-trained Transformer) ve Dönüştürücülerden Çift Yönlü Kodlayıcı Temsilleri anlamına gelen BERT (Bidirectional Encoder Representations from Transformers).Transformer mimarisine dayalı bu iki modelin, eğitim yaklaşımları, kullanım amaçları ve performansları açısından aralarında farklar vardır. Bu bölümde, öncelikle GPT ve BERT modellerinin temel yapıları ve geliştirilmiş versiyonları açıklanacak; ardından, aralarındaki farklar ele alınacak, son olarak ise bu iki dil modeli doğal dil işleme görevleri kapsamında incelenecektir. 5.3.1 GPT ve Geliştirilmiş Versiyonları OpenAI tarafından 2018 yılında tanıtılan transformer tabanlı GPT modelleri , üretken (generative) dil modelleri olarak, tutarlı ve bağlama uygun metinler üretebilme yeteneğine sahiptir. Bu özellik, çevirilerin akıcı ve doğal bir şekilde oluşturulmasına katkı sağlar. GPT modelleri metni ardışık bir şekilde üretir; yani önceki bağlamı göz önünde bulundurarak bir sonraki kelimeyi tahmin eder. Bu otoregresif (autoregressive) yapı, çevirilerin tutarlılığını sağlama ve akıcı çeviriler oluşturma açısından oldukça önemlidir. Yaratıcı metin üretme yeteneğiyle GPT, fikirleri ve kavramları farklı dillerde çeşitli şekilde ifade edebilir. Üretken bir model olduğu için karmaşık cümle yapılarını ve deyimsel ifadeleri işlemede güçlü bir performans sergiler. Doğası gereği, bir cümledeki bir sonraki kelimeyi tahmin etmek için önceden eğitilmiştir. Bu modeller, her seferinde bir kelimeyi tahmin ederek metni otoregresif bir şekilde üretir (Zaki,2024). Transformer mimarisini kullanan ilk otoregresif model GPT’dir. Otoregresif modeller, bir sonraki sonucu üretmek için bir önceki çıktıyı kullanır. Bu modelin, ham metinler üzerinde öğrenim gerçekleştirdiği ön eğitimli bir dil modelleme (language modeling) aşaması vardır. Bu aşama, denetimsiz bir ön eğitim sürecidir; yani model, etiketlenmemiş büyük metin verileri üzerinden dilin yapısını ve istatistiksel örüntülerini öğrenir. İkinci öğrenme aşamasında ise, ağı alt görevlerle (downstream tasks) uyumlu hale getirmek için denetimli ince ayar (supervised fine-tuning) kullanılır. GPT, tek yönlü bir dil modeli olup, yalnızca değerlendirilen belirtecin(token) sol bağlamına ulaşabilir. Bu özellik, modellerin akıl yürütme veya soru yanıtlama görevlerindeki genel performansına zarar verebilir. Çünkü, bu görevlerde, cümleden uygun bir anlam çıkarabilmek için hem sol bağlam hem de sağ bağlam önemlidir (Gillioz,2020).", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 63, "page_end": 63}}
{"content": "52 GPT modelleri, özellikle OpenAI tarafından geliştirilen ChatGPT gibi uygulamalarda yaygın olarak kullanılmaktadır. ChatGPT, derin öğrenme ve büyük veri setlerinin gücünü kullanan çok aşamalı bir süreçle eğitilmektedir. OpenAI tarafından yayımlanan belgelere göre, ChatGPT modeli üç ayrı eğitim aşamasından geçer: 1. Üretken Ön Eğitim (Generative Pre-Training) 2. Denetimli İnce Ayar (Supervised Fine-Tuning) 3. İnsan Geri Bildiriminden Pekiştirmeli Öğrenme (Reinforcement Learning from Human FeedBack) Üretken Ön Eğitim aşamasında model, çeşitli konuları kapsayan geniş bir metin veri kümesi üzerinde eğitilir. Bu durum, modelin çeşitli dilsel desenler ile yapıları benimsemesini sağlayarak, insan iletişimine oldukça benzeyen içerikleri anlamasına ve üretmesine katkıda bulunur. Ardından, model denetimli ince ayar aşamasına geçer. Bu aşamada eğitim, insan temsilcileri (human agent) arasında özenle seçilmiş konuşmalardan oluşan bir veri kümesi kullanılarak yapılır. Bu ince ayar süreci, modelin yanıtlarının insan tercihlerini ve beklentilerini karşılamasını amaçlar. Eğitim sürecinin son aşaması ise insan geri bildirimiyle pekiştirmeli öğrenmedir. Bu aşamada, yapay zekâ modelinin çıktısını değerlendiren insanlar (human evaluators), ChatGPT tarafından üretilen yanıtların kalitesini değerlendirir. Ardından, alınan geri bildirimlere dayalı olarak bir ödül modeli, yanıt kalitesini değerlendirecek şekilde eğitilir. Pekiştirmeli öğrenme teknikleri kullanılarak yapılan bu yaklaşım, ChatGPT’nin çıktılarının insan değerleri ve tercihleri ile uyumlu olmasını sağlar (Mirtaheri et al.,2025). Son dönemde yapılan çalışmalar, büyük bir metin kümesi üzerinde önceden eğitim yapıldıktan sonra belirli bir göreve özgü , ince ayar yaparak çeşitli doğal dil işleme görevleri ve ölçütlerinde önemli kazanımlar elde edildiğini göstermektedir. Genellikle görevden bağımsız mimarilerle kullanılan ince ayar yöntemi, binlerce veya on binlerce örnek içeren göreve özgü veri setlerine ihtiyaç duymaktadır. Buna karşılık, genellikle birkaç örnekle veya basit talimatlarla yeni bir dil görevi yerine getirilebilir. Bu yöntem, doğal dil işleme sistemlerinin zorlandığı bir yöntemdir (Brown et al.,2020). Anlatılmak isteneni netleştirmek adına şöyle açıklayalım; Birkaç atış öğrenme (Few-shot learning), sınırlı veri denetimi ile yapılan makine öğrenimi olarak tanımlanabilir. Bu yöntemde, bir makine öğrenimi modeli, yalnızca birkaç veri örneği ile ilgili sınıfların özelliklerini öğrenir. Birkaç atış öğrenmeye dayalı birçok başarılı modelleme algoritması, ürün görüntüsü tanıma ve sınıflandırma alanlarında uygulanmıştır (Nie et al.,2024).", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 64, "page_end": 64}}
{"content": "53 Sıfır atış öğrenme (Zero-shot learning),bir modelin o sınıflar üzerinden açıkça eğitilmeden, görülmemiş sınıflara genelleme yapabilme yeteneğini ifade eder. Bu yaklaşım, çeşitli kategoriler arasında etkili sınıflandırma yapılmasını sağlar. Ayrıca, yeni kavramlara uyum sağlanması veya kapsamlı eğitim verileri olmadan hızlı dağıtım gerektiren uygulamalarda kullanışlıdır (Yiğit.2025). Bu modeller, başlangıçta yalnızca bir dil modeli olarak tasarlansa da her yeni sürümde model parametreleri, eğitim verisi boyutu ve öğrenme teknikleri açısından önemli gelişmeler göstermiştir. 2018 yılında geliştirilen GPT-1, Transformer mimarisi temelinde, gözetimsiz öğrenme yöntemiyle üretken bir dil modeli eğitmek amacıyla tasarlanmıştır. Önceden eğitilmiş bu model, daha sonra alt görevler için ince ayar kullanılarak uyarlanmıştır. 2019 yılında GPT-2 modeli geliştirilmiş olup, GPT-1’e göre daha fazla parametre ve veriyle eğitilerek çoklu görev öğrenme (multi-task learning) kavramını ortaya koymuştur. Böylece, önceden eğitilmiş üretken dil modeli, ince ayar sürecine girmeden çoğu gözetimli alt görev için genellenebilir hale gelmiştir. Az örnekli (few-shot) senaryolarda modelin performansını artırmak için, GPT-3, bağlam içi öğrenme (in-context learning) ile birlikte meta-öğrenme (meta-learning) teknikleri birleştirilerek modelin genelleme yeteneği artırılmıştır. Ayrıca, GPT-3 ‘ün parametre ölçeği GPT-2’ye göre 100 kat artırılmıştır ve 100 milyar parametre sınırını aşan ilk dil modeli olmuştur. GPT-3.5 serisinin modellerinden biri olarak bilinen InstructGPT ise ChatGPT’nin pilot versiyonu olup, insan geri bildirimiyle öğrenme (Reinforcement Learning with Human Feedback-RLHF) yöntemini kullanarak GPT-3 modelini kademeli olarak eğitmişlerdir. Bu sayede model, kullanıcı niyetine daha iyi uyum sağlayabilir hale gelmiştir (Wu et al.,2023). 2023 yılında GPT-4 modeli geliştirilmiştir ve önceki sürümlere göre önemli iyileştirmeler sunmuştur. GPT-4, çok modlu (multimodal) yetenekler kazanmış; doğruluğu ve performansını ise oldukça artırmıştır. GPT-4, ücretli versiyon olan ChatGPT Plus ile tanıtılmış ve çeşitli yapay zekâ platformlarının altyapısını oluşturmuştur. Bu model ile birlikte kullanıcılar, eklenti ve web tarama özellikleriyle desteklenen daha gelişmiş etkinlikleri gerçekleştirebilmeye başlamıştır. 2023’ün ilerleyen aylarında ise Kod Yorumlayıcı eklentisi sayesinde, veri analizi, problem çözme ve video düzenleme gibi daha teknik görevler için kullanıcı dostu hale getirilmiştir. ChatGPT, görsel ve sesli etkileşim kapasitesini genişleterek DALL-E 3 ile birleştikten sonra yaratıcılığı da destekler hale gelmiştir (Bulut,2024). 2024 ‘te tanıtılan GPT-4o modeli, performans sınırlarını önemli ölçüde ileri taşımıştır. Özellikle genişletilmiş bağlam penceresi, yüksek işlem hızı ve gelişmiş çok modlu mimarisiyle öne çıkmaktadır. GPT-4, güvenilir bir doğal dil işleme modeli olarak kendini kanıtlamış olsa da, GPT-4o önceki modelin sınırlı olduğu noktaları aşmak", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 65, "page_end": 65}}
{"content": "54 amacıyla tasarlanmıştır ve daha iyi bir bütünlük, bağlamsal uygunluk ile çok modlu gelişmiş bir mimari sunmuştur. Bunun yanı sıra, GPT-4o; metin, görsel, ses ve video gibi çok modlu işleme yetenekleri sergilemekte, aynı zamanda sıfır atış öğrenme (zero-shot learning) gerektiren görevlerde de yüksek doğruluk ve verimlilik göstermektedir. GPT- 4o’nun farklı son kullanıcı gereksinimlerini karşılayabilme kapasitesi, onu çok daha yetkin, verimli ve güvenilir bir model haline getirmektedir (Murad et al.,2024). GPT-4.5, OpenAI tarafından 2025 yılında bir araştırma önizlemesi (research preview) olarak tanıtılmıştır. GPT-4 üzerine inşa edilen GPT-4.5, daha büyük, daha güçlü ve gelişmiş bir dil modelidir. Özellikle yazılı anlatımın iyileştirilmesi, programlama, çok adımlı akıl yürütme ve yaratıcı üretim gibi alanlarda öne çıkan bu model daha yüksek “Duygusal Zeka” (Emotinal Quotient-EQ) düzeyiyle kullanıcı niyetini daha iyi anlamaktadır. Model, GPT-4’e göre daha doğal etkileşim sunmakta ve insanlar arası iş birliğini destekleyen yanıtlar üretebilmektedir. Bunun yanı sıra, GPT-4.5’in çok modlu (metin,ses,görsel) destek içermediği, yalnızca gelişmiş metin işleme odaklı bir sürüm olduğu vurgulanmıştır. GPT-4o’ya göre yazarlık, empati, kısa ve etkili ifade gibi görevlerde daha başarılı olduğu gözlenmiştir (OpenAI,2025). 5.3.2 BERT ve Geliştirilmiş Versiyonları BERT, doğal dil işleme alanında çığır açan bir model olup 2018 yılında Google tarafından tanıtılmıştır. Önceden eğitilmiş bir dil modeli olan BERT, çeşitli doğal dil işleme görevlerinde önemli başarılar elde etmiştir. Önceki modeller yalnızca tek yönlü bağlama dayanırken, BERT bir kelimenin anlamını hem solundan hem de sağından aynı anda dikkate alarak çift yönlü bağlam kullanır. Bu yönüyle BERT, metin içindeki anlam ve ilişkileri çok rahat bir şekilde yakalayabilir. BERT, büyük metin veri kümeleri üzerinde önceden eğitildiği için dilin zengin bir temsiline ulaşabilir. Bu ön eğitim aşaması, BERT’in kelime anlamlarını, söz dizimsel yapıları hatta bazı dünya bilgilerini derin bir düzeyde anlamasını sağlar. Ön eğitim aşamasından sonra, BERT belli doğal dil işleme görevleri üzerinde ince ayar yapılabilir. İnce ayar (fine-tuning), modelin önceden kazandığı genel dil anlayışını kullanarak belirli bir göreve özgü ayarlanmasını sağlar (Baskaran,2023). BERT modelinde giriş verisinin bir kısmı maskelenir ve model eğitildikçe maskelenen sözcük veya cümle verisi öğrenilmeye çalışılır. BERT modelinde, giriş verisinde bulunan kelimelerin konumlarına bağlı olarak farklı vektör temsilleri vardır. Bu durum, modelin kelimeye uygulayacağı dikkatin, kelimenin cümlede hangi konumda olduğuna bağlı olarak değişmesine neden olur (Sel ve Hanbay,2021).", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 66, "page_end": 66}}
{"content": "55 BERT modelinin temelini oluşturan maskeli dil modellemesi (masked language modeling), kodlayıcı ön-eğitimin (encoder pre-training), en yaygın yöntemlerinden biridir. Bu yöntem, giriş dizisindeki kelime parçacığı ya da kelime parçacıklarının (token) maskelenerek modelin bu maskelenmiş kelime parçacıklarını tahmin edip eğitilmesiyle gerçekleştirilmektedir. Bu yöntemde, maskelenmeyen kelime parçacıkları kullanılarak tahmin yapılır; bu da modelim hem sol hem de sağ bağlamı dikkate alarak öğrenmesini sağlar. Böylece model, çift yönlü bir bağlamsal anlayış geliştirir (Xiao and Zhu,2025). Örneğin, bir cümledeki bazı kelimeler [MASK] sembolü ile gizlenir. Orijinal Cümle : “Erken kalkan kuş solucanı yakalar.” Maskelenmiş Cümle : “Erken [MASK] kuş [MASK] yakalar.” Modelin görevi, bu eksik kelimeleri doğru bir şekilde tahmin etmektir. Model, sadece gizlenmiş kelimeler üzerinden öğrenme yapar. Sonraki Cümle Tahmini (Next Sentence Prediction) yöntemi, BERT modelinin cümleler arasındaki ilişkileri öğrenmesini sağlar. Bu yöntemde, model bir cümlenin öncesinde ve sonrasında bulunan iki cümleyi alır. Ardından, bu cümlelerin birbirini takip etme durumları ile aralarındaki yapısal ve anlamsal ilişkiyi ortaya koyar. BERT bu yöntemde girdileri ikiye böler. Eğitim aşamasında, ikinci cümlenin birinci cümleyi takip edip etmediği öğretilirken; test aşamasında ise model, rastgele seçilen ikinci bir cümlenin ilk cümleye bağlı olup olmadığını tahmin eder. BERT, WordPiece adı verilen gömme temsillerini kullanır. İlk adımda cümleleri ayırmak için özel [SEP] kelimesini kullanırken, ikinci adımda ise her kelime parçacığının (token) ait olduğu cümleleri tanımlamak için [CLS] kelimesini kullanır. BERT, giriş verilerini üç farklı türde gömme ile temsil eder: kelime parçacığı(token) gömmesi, pozisyon gömmesi ve segment gömmesi. Token gömmesi, her bir kelime parçacığının anlamsal temsili için; pozisyon gömmesi bu parçacıkların cümle içinde ki konum bilgisi için; segment gömmesi ise bir tokenin hangi cümleye ait olduğunu ayırt etmek için kullanılır (Yücesoy Kahraman vd.,2023). [CLS] ve [SEP] özel token türleri olup, BERT tarafından görev genellemesi için kullanılırlar (Mukerje,2025). [CLS]: Her giriş dizisinin başına eklenen özel bir sınıflandırma tokeni olup, bu tokenin son katmandan elde edilen çıktı gömme temsili, cümle düzeyinde sınıflandırma görevleri için kullanılır. [SEP]: Cümle çiftlerini ayırmak için kullanılan bir ayrım tokeni olup, yalnızca bir cümle varsa, böyle bir durumda o cümlenin sonunu işaret eder. Örnek Girdi: [CLS] I love cats.[SEP] They are very intelligent.[SEP]", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 67, "page_end": 67}}
{"content": "56 Transformer (Dönüştürücü) mimarisi, dikkat mekanizmalarıyla birlikte, son yıllarda doğal dil işlemenin yükselmesini sağlamıştır. BERT ve onun türevleri ise, kodlayıcı (encoder) tarafında doğal dil işleme alanına hâkimdir. Bu modeller, ön eğitim aşamasında gerçekleştirdikleri süreç nedeniyle Büyük Dil Modelleri (Large Language Models- LLMs) olarak adlandırılar. Dil modelleme görevini yerine getiren bu modeller, kendilerine “öğretilen” dili “anlamalarını” sağlarlar. Ardından, ince ayar yöntemiyle bilgiyi bir doğal dil işleme görevine özgü uyarlayabilirler. Zamanla BERT modelinden türeyen birçok model ortaya çıkarak bu modellere daha fazla parametre eklenmiştir. Bu durum, Grafik İşlem Birimi (Graphics Processing Unit-GPU) ve Tensör İşlem Birimi (Tensor Processing Unit-TPU) gibi hesaplama kaynaklarının performansını artırmakla kalmamış, modellerin boyutunu ve hesaplama karmaşıklığını da artırmıştır (Escarda et al.,2024). Transformer mimarisi, uzun mesafeli bağımlılıkları yakalamak, metinler arasındaki ilişkiyi belirlemeye yönelik uygun gömme (embedding) tekniklerinin seçilmesi ve metin dizilerinin paralel işlenmesi zorluğu gibi sınırlamaların çözümünde önemli bir gelişme olmuştur. BERT modeli temel yapı olarak Transformer mimarisinin yalın hâlini (Vanilla Transformer) kullanır. Önceden eğitilmiş dil modeli BERT, her ne kadar gelişmiş bir dil anlama seviyesine sahip olsada karşılaştığı bazı sorunlar bulunmaktadır. Bu model; sabit giriş uzunluğu sınırlamaları, kelime parçası (wordpiece) gömme sorunları ve hesaplama karmaşıklıkları gibi problemler yaşamaktadır. BERT ile ilişkili bu temel sorunları hafifletmek için, RoBERTa, DistilBERT, ALBERT, ELECTRA gibi geliştirilmiş modeller sunulmuştur (Acheampong et al.,2020).  RoBERTa (Robustly Optimized BERT Pre-training Approach) BERT için Güçlendirilmiş Optimizasyonlu Ön Eğitim Yaklaşımı anlamına gelen RoBERTa, 2019 yılında Facebook AI araştırmacıları tarafından, BERT’in bazı eksikliklerini gidermek ve çeşitli doğal dil işleme görevleri üzerinde daha etkili hâle getirmek amacıyla geliştirilmiştir. RoBERTa ve BERT arasında, eğitim verileri ve eğitim süreci açısından önemli farklılıklar bulunmaktadır. RoBERTa, kitaplar ve internet makalelerini içeren, BERT’te kullanılandan daha geniş ve çeşitli bir veri kümesi üzerinde eğitilmiştir. Ayrıca RoBERTa, verileri daha verimli kullanan bir eğitim yöntemiyle eğitildiği için, BERT’e göre daha büyük ve daha doğru bir model haline gelmiştir. Bunun yanı sıra RoBERTa, yalnızca daha büyük ve çeşitli eğitim verilerine sahip olmasıyla değil, aynı zamanda veri ön işleme ve eğitim süreci yönüyle de BERT’ ten ayrılmaktadır. BERT’in eğitim sürecinde yer alan sonraki cümleyi tahmin etme aşaması, RoBERTa’nın eğitim sürecinden çıkarılmıştır. Bu durum, modelin maskelenmiş dil modellemesi aşamasını farklı bir şekilde işlemesini sağlamıştır. Bu değişiklikler, RoBERTa’nın eğitim sürecini daha verimli ve etkili bir hâle getirmiştir. (Alagöz ve Uçkan,2024).", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 68, "page_end": 68}}
{"content": "57  DistilBERT : BERT’in Sadeleştirilmiş Bir Versiyonu Bilgi Damıtma (Knowledge Distillation), mevcut daha büyük bir modelin (öğretmen ya da bir model topluluğu), davranışını yeniden üretmeyi öğrenen daha küçük bir modelin (öğrenci) eğitildiği bir sıkıştırma tekniğidir. DistilBERT, daha küçük ve genel amaçlı önceden eğitilmiş bir dil modeli olup, BERT’in geliştirilmiş bir versiyonudur. Bu model, genel amaçlı bir dil modelinin bilgi damıtma (knowledge distillation) yöntemiyle başarılı bir şekilde eğitilmesine dayanır. DistilBERT, daha büyük muadilleri gibi birçok görev üzerinde iyi sonuçlar verecek şekilde ince ayar yapılabilir. Ayrıca, daha hızlı ve daha hafif bir model olduğu için ön eğitim açısından daha düşük maliyetlidir. BERT’e kıyasla %40 daha küçük, %60 daha hızlı olan DistilBERT, BERT’in dil anlama yeteneklerinin %97’sini korumaktadır (Sanh et al.,2020).  ALBERT : Dil Temsillerinin Öz Denetimli Öğrenmesi için Sadeleştirilmiş BERT Doğal dil temsillerinin ön eğitimi sırasında model boyutunun artırılması, genellikle alt görevlerde (downstream task) performansın artmasına yol açmaktadır. Ancak, bir noktadan sonra modelin daha da büyütülmesi, GPU/TPU bellek sınırlamaları ve uzun eğitim süreleri nedeniyle zorlaşmaktadır. Bu sorunları aşmak amacıyla, BERT’in bellek kullanımını azaltmaya ve eğitim hızını artırmaya yönelik parametre azaltma tekniği önerilmiş ve bu tekniğin BERT’e kıyasla çok daha ölçeklenebilir modeller oluşturduğu gözlemlenmiştir. ALBERT, önceden eğitilmiş modellerin ölçeklenmesini engelleyen temel sorunları ortadan kaldıran parametre azaltma tekniğini içermektedir. Ayrıca, ALBERT’in cümleler arası tutarlılığı modellemeye odaklanan öz-denetimli ( self- supervised) bir kayıp fonksiyonu kullanarak cümle girdileri içeren alt görevlerde tutarlı bir şekilde başarı sağladığı gözlemlenmiştir. Bu model, daha az parametreye sahip olmasına rağmen, önemli değerlendirme ölçütlerinde yüksek başarılar elde etmiştir (Lan et al.,2020).  ELECTRA BERT ve RoBERTa gibi modellerin aksine ELECTRA’nın eğitim süreci maskelenmiş dil modellemesine dayanmaz. Cümledeki bazı kelimeleri gizleyip bu kelimeleri tahmin etmek yerine, küçük bir üretici ağ bazı kelimeleri benzer ama farklı kelimelerle değiştirir. Ardından model, bu kelimelerin değiştirilip değiştirilmediğini anlamaya çalışır. Yani hangi kelimelerin orijinal, hangilerinin sonradan değiştirildiğini tahmin eder. Bu yöntemin, büyük veriyle çalışıldığında oldukça etkili olduğu ve RoBERTa kadar iyi sonuçlar verdiği görülmüştür. Aynı bilgisayar gücüyle çalıştırıldığında ise RoBERTa’dan daha başarılı bir performans göstermiştir (Cortiz,2021).", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 69, "page_end": 69}}
{"content": "58 Doğal dil işleme alanında, BERT, RoBERTa, ALBERT ve ELECTRA gibi önceden eğitilmiş dil modelleri, birçok doğal dil işleme görevinde, özellikle çok adımlı akıl yürütme görevlerinde büyük başarılar elde etmiştir. Ancak, önceden eğitilmiş dil modelleri genellikle çok sayıda parametreye sahip oldukları için uzun çıkarım süreleri gerektirir, bu da mobil cihazlar gibi kaynak kısıtlı ortamlarda kullanımlarını zorlaştırır. Son araştırmalar, önceden eğitilmiş dil modellerinde fazlalıklar (gereksiz tekrarlar) bulunduğunu göstermektedir. Bu doğrultuda, modellerin hesaplama yükü ve depolama alanı gereksinimini azaltırken performanslarını korumanın mümkün olduğu gösterilmiştir (Jiao et al.,2020). 5.3.3 GPT ve BERT Modellerinin Karşılaştırılması Dil işleme teknolojilerindeki gelişmelerle birlikte farklı model türleri ortaya çıkmıştır ve bu model türlerinin her biri doğal dil işleme görevlerinde elde ettiği başarıya bağlı olarak çeşitli avantajlar sunmuştur. Derin öğrenme yaklaşımları, özellikle Transformer mimarisi tabanlı GPT ve BERT modelleri , dil işleme görevlerinde oldukça geniş bir kullanıma sahiptir. Bu modeller, eğitim yaklaşımları, mimari yapıları ve uygulama alanları açısından farklılık gösterir. Bu bölümde GPT ve BERT modelleri, bu açılardan ayrıntılı biçimde karşılaştırılacak ve belirli örnekler üzerinden analiz edilecektir. 5.3.3.1 GPT ve BERT Modellerinin Eğitim ve Mimari Yaklaşımları Büyük önceden eğitilmiş dil modelleri ( large pre-trained langugage models), doğal dil işleme alanını yeniden şekillendirmiştir. Bu modeller, yakın zamanda temel modeller olarak adlandırılmıştır ve iki aşamalı bir yaklaşım önerilmiştir. Bu yaklaşıma göre model, büyük ölçekli geniş bir veri üzerinde, asıl görevin yerine geçen, kendi kendine denetimli bir öğrenme görevi (surrogate self-supervised task) ile önceden eğitilir. Bu ön eğitim ile kazanılan bilgi, genellikle küçük etiketli veri ile belirli bir alt göreve ince ayar yapılarak aktarılır. Öne çıkan örnekler arasında BERT ve GPT-3 bulunmaktadır. Kendiliğinden denetimli ön eğitim, etiketli veri gerektirmez. Bu sayede, ortaya çıkan model, birkaç örnek (few-shot) veya sıfır örnekle (zero-shot) öğrenme yoluyla görev yelpazesini hızla genişletebilir (Dor et al.,2022). BERT sınıfı modeller, maskelenmiş dil modellemesi (masked language modeling) yöntemiyle eğitilirken, bir cümledeki eksik kelimelerin tahminini cümledeki diğer kelimelere bakarak , bağlama göre tahmin eder. Öte yandan, GPT-3, yalnızca bir dizideki sonraki kelimeyi tahmin etmeye dayalı olarak eğitilmiştir bu yüzden nedensel dil modellemesi (causal language modeling) olarak bilinir. GPT-3’ün parametre sayısı 175", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 70, "page_end": 70}}
{"content": "59 milyar olup, BERT’in 340 milyon parametre sayısından kat ve kat fazladır. GPT-3’ün bu kadar yüksek bir parametreye sahip olması ön eğitim aşamasında kullanılan devasa metin verisinden daha fazla bilgi edinmesini sağlar. Ayrıca, GPT-3’ün eğitimi sırasında kullanılan veri ve hesaplama kaynaklarının bu denli büyük ölçekli olması, kendiliğinden genelleme davranışlarına yol açmıştır. (Bosley et al.,2023). BERT, yalnızca kodlayıcı (encoder) katmanlardan oluşan bir dil modelidir ve giriş verilerinin çift yönlü temsillerini öğrenir. GPT ise yalnızca kod çözücü (decoder) katmanlardan oluşan bir modeldir ve giriş verilerinin tek yönlü temsillerini öğrenir. BERT ve GPT, dil anlama ve dil üretimi arasında kurulan bir dengeye bağlıdır. Bu yüzden, kullanım senaryoları ve görev türleri açısından farklı avantajları ve sınırlamaları bulunur. BERT, tüm katmanlarda hem sol bağlamı hem de sağ bağlamı birlikte değerlendirerek, etiketlenmemiş metinlerden derin çift yönlü temsiller öğrenmek üzere tasarlanmıştır. Böylece, BERT modeli yalnızca bir tek çıktı katmanı eklenerek farklı görevler için göreve özgü ince ayar yöntemi ile ayarlanabilir. İnce ayar süreci dışında, hem ön eğitim aşamasında hem de sonrasında aynı model mimarisi kullanılarak farklı görevler için de önceden eğitilmiş aynı parametrelerle başlatılmaktadır. BERT’in çift yönlü temsilleri öğrenmesi daha fazla bağlamsal bilgi yakalayabilmesini ve çok anlamlılığı (polysemy) tek yönlü temsillere göre daha iyi ifade edebilmesini sağlar. BERT aynı zamanda maskelenmiş dil modellemesi (masked language modeling) ve sonraki cümle tahmini (next sentence prediction) olmak üzere iki hedef üzerinde ön eğitim aşamasından geçmesi, modelin genelleme yeteneğini ve doğruluğunu artırır. Transformer’ın encoder (kodlayıcı) ve decoder (kod çözücü) kısmı Şekil 5.2’ de gösterilmiştir (Sharkey and Treleaven,2024; Alagöz ve Uçkan,2024). Şekil 5.2 Encoder ve Decoder blokları", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 71, "page_end": 71}}
{"content": "60 BERT ve GPT gibi büyük dil modelleri, bağlama uygun kelime temsili ve metin üretme gibi çeşitli doğal dil işleme görevlerinde oldukça başarılı performans sergilemektedir. Ancak, her iki model de yüksek hesaplama maliyetine sahip olup, iki modelinde eğitimi ve kullanımı için büyük miktarda veri ve güçlü donanıma ihtiyaç vardır. GPT-3 modeli, göreve özgü istemler (prompt) kullanarak ve bu görevleri dil modelleme formatında yapılandırarak, gradyan güncellemesi olmadan birçok doğal dil işleme veri setinde başarılı sonuçlar elde etmiştir. Ancak, GPT-3, 175 milyar parametreye sahip olduğu için gerçek hayat uygulamaları için oldukça büyük bir ayak izine sahiptir. GPT-3’te kullanılan istem stratejisi, daha küçük ayak izine sahip olan BERT ve RoBERTa gibi modellerde de uygulanmıştır. Bu modeller, yalnıza birkaç etiketle ince ayarlandıktan sonra GPT-3’e benzer performans gösterebilmiştir (Arısoy,2024; Chen et al.,2021). 5.3.3.2 GPT ve BERT Modellerinin Görev Performansları ve Uygulama Alanları Genel Dil Anlayış Değerlendirmesi (General Language Understanding Evaluation- GLUE), soru yanıtlama, duygu analizi ve metin çıkarımı gibi çeşitli doğal dil anlama görevlerini içeren bir kıyaslama ölçütüdür (benchmark). BERT, GLUE ölçütüne göre, ortalama 87.1 puanla son derece başarılı sonuçlar elde etmiştir ve önceki en iyi sonuçları büyük farkla geçmiştir. GLUE ölçütünün bir uzantısı olan SuperGLUE, akıl yürütme ve doğal dil çıkarımı gibi daha karmaşık görevleri içeren doğal dil işleme modellerini test etmek için tasarlanmıştır. BERT, SuperGLUE ölçütünde de ortalama 89.6 puana sahip olmasıyla yüksek bir performans sergilemiştir. Öte yandan GPT-3 doğrudan GLUE ve SuperGLUE ölçütünde değerlendirilmemiş olup bu kıyaslama ölçütlerinde bulunan birçok bireysel görevde etkileyici sonuçlar göstermiştir. Stanford Soru Cevaplama Veri Kümesi (SQuAD), bir metin parçasına dayalı olarak soruları yanıtlamayı amaçlayan makine anlama (machine comprehension) kıyaslamasıdır. BERT, SQuAD üzerinde %93.2 F1 skoru ile son teknoloji sonuçlar elde etmiştir. Konuşmaya Dayalı Soru Cevaplama (CoQA) veri kümesi, konuşma ortamında makine anlama için bir kıyaslama yapar. BERT, CoQA’ da %84.2 F1 skoru ile yine başarılı sonuçlar göstermiştir. GPT-3 yine doğrudan, SQuAD ve CoQA da değerlendirilmese de, hem makine anlama görevlerinde hem de konuşmaya dayalı soru-cevap görevlerinde etkileyici performans göstermiştir (Bangi,2023).", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 72, "page_end": 72}}
{"content": "61 BERT ve GPT gibi popüler dil modelleri, duygu analizi ve öneri sistemleri gibi doğal dil işleme uygulamalarında kullanılmaktadır. Duygu analizi ile kullanıcı duyguları analiz edilirken, öneri sistemleri ile kişiselleştirilmiş öneriler sunulur. GPT modelleri doğası gereği metin üretimi görevlerine daha yakın olsa da dil anlama görevlerinde de BERT ile karşılaştırılabilir sonuçlar elde edebilmekte, hatta bazı durumlarda daha iyi sonuçlar gösterebilmektedir. Geleneksel dil anlama görevlerinde çoğunlukla düşük başarı gösteren GPT modelleri, “P-tuning” adı verilen bir yöntemle benzer boyuttaki BERT modelleriyle karşılaştırılabilir performanslar sergilemeye başlamıştır (Salıcı ve Ölçer,2024). BERT, kelimeler bağlam dışında kullanıldığında bile cümlenin amacını belirleyebilir. Metinden bağlamsal bilgileri yakalama yeteneği sayesinde metin sınıflandırma görevi için kullanılması diğer geleneksel yöntemlere göre daha avantajlıdır. BERT, metinde bulunan kelimeleri tek tek dikkate alan kelime çantası (bag of words) veya TF-IDF gibi geleneksel yöntemlerden çok daha iyi performans gösterir. Diğer yandan, 175 milyar parametreye sahip ve devasa bir veri kümesinde üzerinde eğitilmiş GPT-3, insan müdahalesi olmadan metni doğru bir şekilde sınıflandırma yeteneğine sahiptir. GPT-3, metnin konusu ve yazarın üslubu gibi bağlamsal bilgileri kullanarak, daha doğru sınıflandırma yapar (Koçak ve Yiğit,2023). Aşağıda, literatürdeki bazı çalışmalardan örneklerle BERT ve GPT modellerinin doğal dil işleme görevlerindeki kullanım alanları ve performansları karşılaştırmalı olarak incelenmiştir. ÖRNEK 1: Duygu Tanıma Görevinde GPT-3 ve BERT Duygu tanıma görevine yönelik bir çalışmada , GPT-3 ve BERT modeli karşılaştırılmıştır. Bu karşılaştırmaya ilişkin bulgular Şekil 5.3’te görselleştirilmiş, temel sonuçlar ise aşağıda maddeler halinde özetlenmiştir (Boitel et al.,2024). Görselleştirme çalışmadaki temel bulgular ele alınarak ChatGPT desteğiyle hazırlanmıştır. Şekil 5.3 GPT-3 ve BERT ‘in duygu tanıma görevindeki performansları", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 73, "page_end": 73}}
{"content": "62  İşlem Süresi : BERT’in tek bir cümleyi işlemesi yaklaşık 10-20 saniye alırken, GPT oldukça hızlı bir işleme oranı göstererek dakikada 60 cümle işleyebilmektedir.  Tutarlılık : En yetenekli ve en pahalı modellerden biri olan Davinci (GPT-3), BERT’e kıyasla daha yüksek tutarlılık sergilemektedir. GPT’nin duygu tanıma doğruluğundaki standart sapması %3 iken BERT’in standart sapması ise %6’dır. Bu durum, GPT-3’ün farklı girdilerde daha tutarlı ve güvenilir tahminler sunduğunu göstermektedir.  Veri Seti Uyumluluğu : Bu çalışmada kullanılan BAUM veri seti Türkiyede geliştirilmiştir ve mutluluk, nötr gibi duygu kategorilerinde dengesiz bir yapıya sahiptir. GPT-3 dengesiz veri setlerinde duyguları tanımada daha iyi performans göstermektedir.  Etiket Benzerliği : Sürpriz ve korku duyguları, her iki modelin tahmin ettiği etiketler açısından birbirine oldukça benzemektedir.  Olumsuz Duyguların Gruplandırılması : Negatif duygular genellikle öfke ve üzüntü gibi diğer olumsuz duygularla tutarlılık göstermiştir. Bu durum, Davinci’nin duygusal ifadeleri daha incelikli bir şekilde anlayabildiğini göstermektedir.  Karmaşık Duyguların Tanınması: Son olarak, GPT-3 karmaşık duyguları (örneğin hem olumlu hem olumsuz özellikler taşıyabilen şaşkınlık gibi) tanımada daha iyi performans göstermektedir. Bu bulgular doğrultusunda Boitel ve arkadaşları (2024) , her iki modelin de güçlü yönlerinin ve sınırlamalarının bulunduğunu belirtmiştir. GPT-3, pek çok doğal dil işleme görevinde etkileyici performans sergilemesine rağmen belirli alanlarda BERT gibi göreve özgü optimize edilmiş modellerin gerisinde kalabilmektedir. Genel olarak, GPT-3 bağlamları yakalama konusunda yetenekliyken, BERT daha yüksek doğruluk oranları ve dengesiz veri ile başa çıkma konusunda metin tabanlı duygu tanıma görevlerinde öne çıkmaktadır. ÖRNEK 2 : Film Altyazısı Çevirisinde Derin Öğrenme Modellerinin Performansı Yapılan çalışmada, Transformer, GPT, BERT gibi her bir derin öğrenme modelinin altyazı çevirisindeki etkinliği, verimliliği ve bağlamsal duyarlılığı incelenmiştir. Bu çalışma kapsamında, modellerin performanslarının nasıl değiştiği, her bir modelin belirli bir altyazı çeviri türü için uygunluğu ve bu farklılıkların bağlam duyarlılığı üzerindeki etkisi karşılaştırılmalı olarak analiz edilmiştir. Derin öğrenme modellerinin yalnızca dilsel doğruluğu değil, aynı zamanda duygusal ve kültürel bağlamı doğru biçimde yansıtmasının da önemli olduğu vurgulanmıştır (Aytaş,2025).", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 74, "page_end": 74}}
{"content": "63 Aytaş çalışmasında Transformer (Vanilla Transformer), BERT, GPT modellerinin çevirilerdeki bağlam duyarlılığı, hız ve tutarlık sonuçlarını Tablo 5.1’ deki gibi özetlemiştir. Bu çalışmada Transformer, BERT ve GPT modelleri; bağlam duyarlılığı, çeviri hızı ve çeviri tutarlılığı açısından WMT17, OPUS ve film altyazısı gibi kıyaslama ölçütü (benchmark) veri setleri kullanılarak değerlendirilmiştir.  Bağlam Duyarlılığı : BLEU ve METEOR gibi metriklerle ve insan değerlendirilmeleriyle ölçülmüştür. Bu değerlendirmelerle, modellerin belirsiz ifadeleri, deyimleri ve kültürel unsurları ne ölçüde doğru yorumladıkları analiz edilmiştir. GPT modeli, geniş veri setleri üzerinde eğitilmesi ve gelişmiş bağlam anlayışına sahip olması sayesinde, en yüksek bağlam duyarlılığını göstermiştir. Transformer modeli yüksek düzeyde, BERT ise orta düzeyde bağlam duyarlılığı sergilemiştir.  Çeviri Hızı: 1000 kelime başına geçen süre temel alınarak ölçülmüştür. Paralel işlem yeteneği sayesinde Transformer modeli 5 sn ile en hızlı model olmuş, çift yönlü doğası nedeniyle BERT ise 7 saniyede çalışmıştır ve son olarak GPT ise otoregresif yaklaşımı sebebiyle 8 saniye çalışmasıyla en yavaş model olmuştur.  Çeviri Tutarlılığı : TER ( Translation Edit Rate) metriği ve insan değerlendirmeleriyle ölçülmüş, GPT modeli anlam bütünlüğünü koruma yeteneğiyle en yüksek çeviri tutarlılığını sağlamıştır. BERT yüksek düzeyde, Transformer ise orta düzeyde tutarlılık göstermiştir. Sonuç olarak tüm bulgular göz önünde alındığında, GPT’nin bağlamsal ve anlamsal tutarlılık açısından en başarılı model olduğu sonucuna ulaşılırken, Transformer’ın hız açısından öne çıktığını ve BERT’in doğruluk ile verimlilik arasında dengeli bir performans sunduğu görülmüştür. Farklı altyazı çeviri türleri için en uygun model seçiminin bağlam duyarlılığı, hız ve tutarlılık gibi faktörlere göre değiştiği gözlemlenmiştir. Ayrıca, duygusal ve kültürel bağlamın önemli olduğu karmaşık çeviri görevleri için GPT öne çıkarken, hız öncelikli durumlarda ise Transformer’ın tercih edilebileceği, BERT’in bu iki özellik arasında denge sağlayan bir alternatif olduğu ifade edilmiştir. Tablo 5.1 Derin öğrenme modellerinin karşılaştırılması", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 75, "page_end": 75}}
{"content": "64 ÖRNEK 3: Yunan Klinik Konuşmalarından Duygu Analizi Yunan klinik konuşmalarında duygu analizi yapmak için farklı derin öğrenme tabanlı dil modellerinin kıyaslandığı bir çalışma yapılmıştır. Bu çalışmada, tüm duygu kategorilerinde BERT modelinin diğer modelleri geride bıraktığı ve özellikle olumlu ve olumsuz duygular için yüksek puanlar aldığı gözlemlenmiştir. Ayrıca, araştırmacılar tarafından olumsuz ve olumlu duyguların başarılı bir şekilde tespit edilmesinin, klinik ortamlarda hastaların duygusal durumları hakkında fikir sahibi olmak adına önemli olduğu belirtilmiştir. BERT modelinin bir versiyonu olan RoBERTa’nın ise nötr duygu kategorisinde umut verici sonuçlar gösterdiği gözlemlenmiştir; bu da onun nötr duygu tespitinin önemli olduğu uygulamalar için uygun olabileceğini göstermiştir. GPT-2 ise, nötr duyguları tanımada güçlü olmasına rağmen olumsuz ve olumlu duygu tanımada daha düşük doğruluk göstermiştir. GPT-2, BERT ve RoBERTa’ya kıyasla orta düzeyde performans göstermiştir. Bunun nedeni, GPT-2’nin tasarımı gereği verilen bağlam ve önceki kelimelere göre metin üretmeye odaklanması ve bu durumunda duygu analizi için uygun olmaması olarak belirtilmiştir (Chatzimina et al.,2024). ÖRNEK 4: GPT ve BERT’in Birlikte Kullanımı : Çoktan Seçmeli Soru Üretimi Metin üretiminde güçlü olan GPT mimarisi ile anlamsal sınıflandırma ve doğrulamada etkili olan BERT mimarisi, bazı çalışmalarda birlikte kullanılarak tamamlayıcı bir şekilde rol almışlardır. Offerijns ve arkadaşları (2020) tarafından yapılan çalışmada, çoktan seçmeli soruların (Multiple Choice Questions- MCQs) otomatik olarak üretilmesi hedeflenmiş ve bu iki modelin görev uyumu değerlendirilmiştir. Literatürde dikkat dağıtıcı (çeldiren) şıklar konusunun yeterince ele alınmadığı vurgulanmış ve bu eksikliği gidermeye yönelik bir sistem geliştirilmiştir. Çeldirici Üretimi (Distractor Generation), çoktan seçmeli sorularda verilen doğru cevaba göre, bağlamla ilişkili ama yanlış olan üç farklı seçeneğin otomatik olarak oluşturulma süreci olarak açıklanmıştır. Yapılan çalışmada:  GPT-2 modeli verilen bağlam, soru ve doğru cevaba dayanarak bağlamla tutarlı çeldirici şıklar üretmiştir.  Üretilen çoktan seçmeli sorularda doğru cevabın belirlenebilmesi için, BERT modelinin bir versiyonu olan DistilBERT kullanılmıştır.  DistilBERT, bağlam-soru-cevap üçlülerini analiz ederek soruların anlamlı ve cevaplanabilir olup olmadığı filtrelemiştir  DistilBERT, “Bu soru anlamlı mı? Bağlamla uyuşuyor mu? Cevabı bu bilgilerden çıkarılabilir mi?” gibi kriterlere göre filtreleme (QA filtering) işlemini gerçekleştirmiştir.", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 76, "page_end": 76}}
{"content": "65 Yöntemsel olarak :  Çeldirici üretimi (distractor generation) için : RACE veri kümesi,  Soru üretimi için : SQuAD veri kümesi kullanılmıştır. Değerlendirme sonuçları :  GPT-2 tabanlı modelin önceki sıralı modellere kıyasla yüksek performans gösterdiği gözlemlenmiştir.  İnsan değerlendirmeleri sonucunda, BERT filtrelemesinin soru kalitesine katkısının istatistiksel olarak anlamlı bir fark yaratmadığı görülmüştür. Elde edilen sonuçlar, böyle bir sistemin öğretmenlerin sınav hazırlık süreçlerine destek olabileceğini hatta öğrencilere bireysel eğitim sırasında otomatik quizler sunabileceğini göstermişti (Offerijns et al.,2020). ÖRNEK 5: ChatGPT de Anlayabiliyor mu? ChatGPT ve İnce Ayarlı BERT Üzerine Karşılaştırmalı Bir Analiz Zhong ve arkadaşları tarafından yapılan çalışmada (2023), ChatGPT’nin dil anlayış yeteneği, çeşitli doğal dil anlama görevleri üzerinde incelenmiştir. GPT modellerinin sadece üretim için güçlü olduğu yönünde genel bir yanılgı olsa da, bu modeller belirli anlama görevlerinde de güçlü performans göstermektedir. Dolayısıyla, GPT modellerinin anlama görevlerinde başarısız olarak değerlendirilmesi her zaman geçerli değildir. Çalışma kapsamında, ChatGPT’nin 31 Ocak 2023 versiyonu kullanılmış, 4 BERT versiyonuyla karşılaştırma yapılmıştır. (BERT-base, BERT-large, RoBERTa-base, RoBERTa-large). ChatGPT’nin performansını fazla zorlamadan, temel BERT modelleri ile kıyaslamalarının sebebi, eğer buna rağmen ChatGPT birçok görevde benzer ya da daha iyi performans gösterirse, bu sayede modelin anlama yeteneğinin alt sınırına dair bir çıkarım yapılabileceği düşüncesidir. Değerlendirme Sonuçları :  ChatGPT’nin Performansı ChatGPT özellikle çıkarım (inference), görevlerinde oldukça başarılı olmuş, ancak benzerlik (similarity) ve anlamı koruyarak yeniden ifade etme (paraphrase) gibi görevlerde başarısız kalmıştır. Özellikle anlamı çelişen cümleler (negative paraphrase) ve anlamsal olarak çelişmeyen ancak birbirine benzemeyen (neural similarity) olumsuz örneklerde performansı düşmüştür. Bu, ChatGPT’nin dil anlayışı ve metin ilişkisi kurma konusunda hâlâ zayıf olduğunu göstermektedir.", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 77, "page_end": 77}}
{"content": "66  Gelişmiş Prompt Stratejileri Bu çalışmada, ChatGPT’nin dil anlama becerilerini değerlendirmek amacıyla her görev için özel olarak istemler (prompts) tasarlanmıştır. Bunlardan birincisi, araştırmacılar tarafından doğrudan yazılan ve modelin özel bir görevi yerine getirmesi istenen istemlerken, bir diğeri ise ChatGPT’ye, bir görev için uygun istemler üretmesi yönünde talimat vermektir. Böylece model, görev tanımını kendisi üretmiş ve görevle ilgili farklı çözüm yolları önermiştir. Bu durum, büyük dil modellerinin yalnızca cevap üretme yeteneği değil, aynı zamanda bir doğal dil işleme görevinin nasıl çözüleceğine dair kendi talimatlarını oluşturma yeteneğinin de değerlendirilmesini sağlamıştır. Örneğin, duygu analizi görevinde ChatGPT’den “Duygu analizi görevini gerçekleştirebilmek için beş kısa istem öner” şeklinde bir istek verildiğinde model kendi görev istemlerini oluşturabilmektedir (Bkz. Şekil 5.4). Prompt (istem) mühendisliği ile ChatGPT’nin performansında iyileşme sağlanmış ve bazı görevlerde RoBERTa-large gibi güçlü modelleri bile geride bıraktığı belirtilmiştir. Bu durum, ChatGPT’nin esnekliğini ve iyileştirilebilir olduğunu göstermektedir. Sadece modelin yapısı değil, modelin nasıl kullanıldığı ve yönlendirildiği de doğal dil anlama görevlerinde önemli bir etkendir.  BERT ile Karşılaştırma ChatGPT, ince ayarlı BERT tarzı modellere kıyasla, bazı görevlerde karşılaştırılabilir bir performans göstermiş, ancak henüz bazı doğal dil anlama görevlerinde en iyi BERT modellerinin gerisinde kalmıştır. Bu durum, ChatGPT’nin bir potansiyele sahip olduğunu, ancak bazı görevler için BERT ve türevleri kadar iyi sonuçlar gösteremediğini ortaya koymuştur. Şekil 5.4 Duygu analizi için ChatGPT tarafından oluşturulan promptlar", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 78, "page_end": 78}}
{"content": "67 Bu çalışmada, ChatGPT’nin performansı sadece sınırlı sayıda görev türü üzerinde değerlendirilmiş ve daha fazla örnekle test yapıldığında daha güvenilir sonuçlar elde edilebileceği belirtilmiştir. Ayrıca, çalışmada yalnızca GLUE kıyaslama ölçütünün kullanılması, modelin tüm doğal dil anlama görev çeşitliliği açısından kapsamlı şekilde test edilemediği anlamına gelmektedir. Bu yüzden, gelecekte daha geniş görevleri kapsayan testlerle daha kapsamlı ve güvenilir analizler yapılabileceği ifade edilmiştir (Zhong et al.,2023). ÖRNEK 6: Farklı Sektörlerdeki Haberler Üzerinde GPT ve FinBERT’in Duygu Analizi Performansının Karşılaştırılmalı İncelenmesi Belirli şirketler, sektörler, ekonomik gelişmeler hakkında yayımlanan haberler ile sosyal medyada yer alan olumlu veya olumsuz içerikler, yatırımcı davranışlarını ve piyasa hareketliliğini etkilemektedir. Bu durum, duygu analizine olan ihtiyacı da beraberinde getirmektedir. Bu çalışma kapsamında, son teknoloji bir dil modeli olan GPT ile finans alanına özel olarak geliştirilmiş duygu analizi modeli FinBERT kullanılarak finansal haber verileri üzerinde analizler gerçekleştirilmiştir. İki modelin performansları karşılaştırılarak GPT’nin potansiyeli ortaya konmuştur. Ayrıca, finansal piyasalardaki duygu değişimleri ile haberler arasındaki ilişki incelenerek, yatırım kararları için uygulanabilir veriden anlamlı gözlemler çıkarılması amaçlanmıştır. Bu çalışmada, hem modellerin performansı hem de yorumlanabilirliği vurgulanmıştır. GPT-4o’nun performansını artırmak için sistematik bir istem (prompt) tasarımı ve optimizasyon yaklaşımı benimsenmiştir. Bu süreç, etiketli verilerden anlamlı çıkarımlar yapabilmek için adım adım geliştirilen istem iyileştirmelerini kapsamaktadır. Bu yaklaşım, istem tasarımının model doğruluğunu artırmadaki kritik rolünü vurgulamış ve sonuç olarak GPT-4o’nun FinBERT’e kıyasla daha yüksek bir performans sergilemesini sağlamıştır. Sonuçlara göre, istem tasarımı (prompt engineering) yoluyla optimize edilen GPT-4o’nun performansı, sektöre bağlı olarak FinBERT’ten %10’a kadar daha yüksek olmuştur (Kang and Choi,2025). Çalışma Sonuçları :  Finansal metin analizi ve duygu sınıflandırma görevindeki başarısıyla tanınan FinBERT, finansal veriler üzerinde önceden eğitilmiş olduğu için özel görevlerde oldukça başarılıdır. Ancak, haber veri kümeleri genellikle finansın ötesine geçen çeşitli konuları da içerdiği için FinBERT’in uygulama alanı sınırlıdır.  GPT-4 ise milyarlarca parametreyle geniş bir veri kümesinde eğitildiği için finans da dahil olmak üzere çeşitli görevlerde güçlü bir genel amaçlı performans sergilemektedir.", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 79, "page_end": 79}}
{"content": "68  Bu esneklik, GPT-4’ün haber veri kümeleri gibi çeşitli içerik içeren görevlerde FinBERT’ten daha iyi performans göstereceğini ortaya koymaktadır. Ayrıca, GPT-4o, istem tasarımı ile optimize edilmiş istemleri kullanmıştır.  Genel olarak iki modelde benzer eğilimler sergilemiş, fakat belirli terimler, kısaltmalar veya yargılarla ilgili belirsiz ifadelerde farklılıklar görülmüştür. Bu belirsiz ifadeler doğaları gereği öznel oldukları için bu ifadelerin doğru yorumlanmasında zorluklar yaşanmıştır.  Eğitildiği veri kümesi sayesinde birden çok bağlamlı yorumlar üretebilme yeteneği sayesinde GPT-4, FinBERT’ten daha üstün performans göstermiştir. Dolayısıyla, GPT-4 karmaşık metin içeriklerini ele almakta daha etkili olmuştur.  İstem mühendisliği, özellikle belirsiz cümlelerin yorumlanmasında GPT’nin performansını maksimize etmede kritik bir rol oynamaktadır. Bağlamın iyileştirilmesi ya da istemlerin daha detaylı hale getirilmesi sayesinde GPT, daha doğru ya da çok yönlü yorumlar yapabilmektedir.  GPT ile istem tasarımı arasındaki bu ilişki, belirsiz metinlerin esnek ve yaratıcı bir şekilde işlenmesini mümkün kılar. Etkili sonuçların elde edilebilmesi için BERT ya da GPT modeli için analiz hedefi neyse ona göre istem tasarımı optimize edilmelidir. Kang ve Choi tarafından yapılan bu çalışmada (2025), farklı sektörler arasında yapılan performans analizleri sayesinde GPT-4o’nun FinBERT’e göre üstün performans gösterdiği vurgulanmıştır. Ayrıca, çalışmada GPT-4o modeli duygu analizi için büyük dil modeli (LLM) olarak kullanılırken, son gelişmeler doğrultusunda Gemini ve LLama gibi çeşitli modeller de tanıtılmıştır. Gelecek araştırmalarda bu modeller kullanarak duygu analizi yapılabileceği ve bu modellerin sonuçlarının GPT-4 ile kıyaslanabileceği belirtilmiştir. Öte yandan, yapılan duygu analizi görevi için yalnızca olumlu (positive), olumsuz (negative) ve nötr (neutral) şeklinde 3 adet duygu etiketi kullanılmıştır. İlerideki çalışmalarda bu etiket yelpazesinin genişletilebileceği vurgulanmıştır. Son olarak, büyük dil modellerine özgü bir sınırlılık olan modellerin kararlarına dair açıklayıcı bilgi sunulamamasının bu çalışma içinde geçerli olduğu ifade edilmiş ve ileride modellerin yorumlanabilirliğinin artırılmasının önemine dikkat çekilmiştir. 5.3.3.3 GPT ve BERT Modellerinin Karşılaştırmalı Değerlendirilmesi ve Sonuç Yapılan literatür incelemesi ve örnek çalışmalar, GPT ve BERT modellerinin doğal dil işleme alanında farklı güçlü yönlere sahip olduğunu göstermiştir. Her iki model de Transformer mimarisine dayanır ve eğitim yaklaşımları, mimari yapıları ve uygulama alanları açısından önemli farklılıklar göstermektedir.", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 80, "page_end": 80}}
{"content": "69 Genel Bulgular ve Yorumlar 1. Mimari ve Eğitim Yaklaşımları  BERT, çift yönlü (bidirectional) bağlam anlayışı sayesinde dil anlama görevlerinde (örneğin, metin sınıflandırma, soru-cevap) yüksek performans sergilemektedir. Maskeli dil modelleme (masked language modelling) ve sonraki cümle tahmini ( next sentence prediction) gibi ön eğitim stratejileri, modelin bağlamsal ilişkileri daha iyi kavramasını sağlamaktadır.  GPT, otogresif (tek yönlü) bir yapıya sahiptir ve metin üretimi görevlerinde (örneğin, çeviri, özetleme) daha başarılıdır. Büyük ölçekli parametre yapısı (175 milyar parametre) ve geniş veri kümeleri üzerinde eğitilmesi, GPT’nin çeşitli görevlerde genelleme yapabilme yeteneğini artırır. 2. Performans ve Uygulama Alanları  Duygu Analizi: BERT, özellikle dengesiz veri kümelerinde ve klinik metinler gibi uzmanlık gerektiren alanlar için daha tutarlı sonuçlar vermektedir (Örnek 1 ve Örnek 3). GPT ise karmaşık duyguları tanımada daha başarılıdır, ancak istem tasarımı (prompt engineering) ile optimize edilmesi gerekebilir (Örnek 6).  Metin Üretimi ve Çeviri : GPT, bağlamsal tutarlılık ve akıcılık gerektiren görevlerde (Örnek 2) öne çıkarken, BERT daha çok anlamsal doğruluk ve verimlilik odaklı senaryolarda tercih edilir.  Çoklu Görev Yeteneği: GPT, birkaç atış (few-shot) ve sıfır atış (zero-shot) öğrenme ile çeşitli görevlere uyarlanabilirken, BERT genellikle göreve özgü ince ayar (fine- tuning) gerektirir. 3. Hesaplama Kaynakları  BERT, daha küçük boyutu sebebiyle kaynak kullanımı açısından daha verimlidir.  GPT-3 gibi büyük modeller, yüksek hesaplama maliyetleri ve enerji tüketimi nedeniyle gerçek zamanlı uygulamalarda sınırlı kalabilir. 4. GPT ve BERT’in Birlikte Kullanımı  Bazı çalışmalar (Örnek 4), GPT’nin metin üretme yeteneği ile BERT’in anlama ve doğrulama kapasitesinin birleştirilmesinin başarılı sonuçlar verdiğini göstermiştir. Örneğin, çoktan seçmeli soru üretiminde, GPT modelinin verilen bağlama ve doğru cevaba dayanarak çeldirici şıkları üretmesi ve BERT’in bu şıkları anlamlılık ve bağlam uyumuna göre filtrelemesi gibi görevlerde bu iki modelin birlikte kullanımı mümkündür.", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 81, "page_end": 81}}
{"content": "70 GPT ve BERT modelleri, doğal dil işleme alanında birbirine rakip olmaktan ziyade, farklı ihtiyaçlara yanıt veren tamamlayıcı modeller olarak değerlendirilmelidir. Bu modeller karşılaştırılırken görev odaklı bir yaklaşım benimsenmelidir. Modellerin geliştiriliş amaçları ve yapısal özellikleri bilinse bile, tek bir görevle eşleştirmek doğru değildir. Dil modelleri, farklı göreve özgü tekniklerle başka görevler için kullanışlı hale getirilebilir. Model seçimi yapılırken görev türü, veri boyutu, veri çeşitliliği, kaynak kısıtları ve istem tasarımı (prompt engineering) gibi faktörler dikkate alınmalıdır. BERT, dil anlama görevlerinde öne çıkarken, GPT metin üretimi konusunda daha etkilidir. Ancak, bir modelin belirli bir görevde öne çıkması, diğer görevler için optimize edilemeyeceği anlamına gelmez. Kaynak kısıtları olan durumlarda ise, BERT veya DistilBERT gibi hafifletilmiş versiyonlar tercih edilebilir. Bu bölümde yapılan karşılaştırmaların bir kısmı Tablo 5.2’de özetlenmiştir. Tablo 5.2 GPT ve BERT karşılaştırılması", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 82, "page_end": 82}}
{"content": "71 6.DİL MODELLERİNDE GÜNCEL YÖNELİMLER VE ETİK BOYUTLAR Doğal dil işleme alanında kullanılan büyük dil modelleri, gelişen yapay zekâ teknolojilerindeki gelişmelerle önemli ilerlemelere öncülük etmektedir. Bu bölümde dil modellerindeki güncel yönelimler ve bu yönelimlerin beraberinde getirdiği etik tartışmalar ele alınacaktır. İlk olarak, yalnızca metinle sınırlı kalmayıp görsel, işitsel ve diğer veri türlerini de işleyebilen çok modlu (multimodal) yaklaşımlar incelenecek ve bu yaklaşımların modellerin yeteneğini nasıl genişlettiği açıklanacaktır. Devamında, farklı yapay zekâ bileşenlerinin ortaklaşa çalıştığı bir yapı sunan çok ajanlı sistemler (multi- agent systems) ele alınacak; bu sistemlerin dil modellerine kazandırdığı yetenekler değerlendirilecektir. Son olarak ise, büyük dil modellerinin performansını artırmaya yönelik kullanılan yöntemlere değinilecek ve bu sistemlerin etkileri kapsamında ortaya çıkan etik sorunlar ; özellikle tarafsızlık, önyargı, şeffaflık ve güvenilirlik konuları ele alınacaktır. 6.1 Dil Modellerinde Güncel Yönelimler Büyük dil modelleri (Large Language Models-LLM) , yalnızca metin üretmekle sınırlı kalmayıp, karar verme ve akıl yürütme gibi insan benzeri yetenekler sergileyerek yapay zekâ sistemlerinde çığır açıcı bir gelişme olmuştur. Bu gelişmeler doğrultusunda, LLM tabanlı yapay zekâ ajanlarının çok modlu alanlara genişletilmesi yönünde önemli bir araştırma eğilimi ortaya çıkmıştır. Bu yaklaşım, yapay zekâ sistemlerinin yalnızca yazılı değil ; aynı zamanda görsel, işitsel ve diğer veri türlerini de anlayarak daha karmaşık ve çok boyutlu görevleri yerine getirebilmesini sağlamıştır. (Xie et al.,2024). 6.1.1 Çok Modlu Büyük Dil Modelleri (Multimodal Large Language Models) Metin ve görsel modların birleştirilmesi, üretken zekâ açısından hayati bir rol oynamaktadır. Bu nedenle, büyük dil modellerinin elde ettiği başarılardan ilham alınarak, Çok Modlu Büyük Dil Modellerinin (Multimodal Large Language Models- MLLM) geliştirilmesine yönelik araştırmalar hız kazanmıştır. Bu modeller, görsel ve metinsel modları sorunsuz şekilde birleştirebilmekte, aynı zamanda diyalog tabanlı bir arayüz ve talimat takip yetenekleri sunmaktadır. Dikkat mekanizmasının ve Transformer mimarisinin tanıtılması, farklı modları büyük ölçekte işleyebilen modellerin geliştirilmesini sağlamıştır. Özellikle bağlam içi öğrenme (in-context learning) yetenekleriyle dikkat çeken büyük dil modellerinin yaygınlaşması, araştırmacıların bu modellerin kapsamını genişletmek istemesine sebep olmuştur. Bu durum, birden fazla modu hem girdi hem de çıktı olarak kapsayan modellerin tasarlanmasını teşvik etmiştir.", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 83, "page_end": 83}}
{"content": "72 Bu genişleme sonucu, GPT-4V ve Gemini gibi son teknoloji modeller geliştirilmiş ve bu modeller üstün performans sergilemiştir (Caffagni et al.,2024). Çok modlu modeller, diyalog sistemleri, metin özetleme ve makine çevirisi gibi çok çeşitli uygulamalarda kullanılma potansiyeline sahip olduğu için önemli bir araştırma alanıdır. Bu nedenle, son yıllarda bu modellere yönelik önemli bir ilgi ve ilerleme kaydedilmiştir. GPT-4, çok modlu modele bir örnektir ve birçok gerçek dünya senaryosunda insanlardan daha az yetenekli olmasına rağmen, çeşitli profesyonel ve akademik değerlendirmelerde insan seviyesi performans sergilemektedir (OpenAI,2024). Ortaya çıkan modeller, büyük dil modellerinde bulunan akıl yürütme ve karar verme yeteneklerini korumakla kalmaz, aynı zamanda çeşitli çok modlu görevleri yerine getirme yeteneği kazandırır. Çok modlu büyük dil modelleri, farklı modlardaki görevleri güçlendirmek için büyük dil modellerini bilişsel güç merkezi olarak kullanırlar. Büyük dil modelleri; güçlü dil üretme yetenekleri, bağlam içi öğrenme (in-context learning), sıfır-atış öğrenme (zero-shot learning) veya birkaç-atış (few-shot learning) gibi istenilen özellikleri beraberinde getirir. Farklı modlardaki temel modellerin her biri ayrı ayrı önceden eğitilmiştir. Dolayısıyla, çok modlu büyük dil modellerinin karşılaştığı temel zorluklardan biri, bu modellerin büyük dil modelleriyle etkili bir biçimde birleştirilmesi ve çıkarım yapabilmesini sağlamaktır (Zhang et al.,2024). Çok modlu büyük dil modeli (multimodal large language model) mimarisi aşağıdaki gibi ele alınır: 1. Modalite Kodlayıcısı (Modality Encoder) Görüntü/ses kodlayıcıları, optik/akustik sinyalleri alan ve bunları ön işleme tabi tutan insan gözleri ve kulakları gibi işlev görür. 2. Önceden Eğitilmiş Büyük Dil Modeli (Pre-trained LLM) Büyük dil modelleri, işlenmiş sinyalleri anlayan ve yorumlayan insan beyni gibi davranır. 3. Modalite Arayüzü (Modality Interface) Farklı modlar (örneğin görüntü ve metin modu) arasındaki hizalamayı sağlamak için görev yapar. Bazı çok modlu büyük dil modelleri, yalnızca metin yerine diğer modları da çıktı olarak üretebilmek için bir üretici (generator) modülü de içerebilir. Model mimarisi Şekil 6.1’de sunulmuştur (Yin et al.,2024).", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 84, "page_end": 84}}
{"content": "73  Özellik Birleştirme Mekanizması (Feature Fusion Mechanism) Çok modlu (multimodal) bir modelin kalbinde, farklı modalitelerden (örneğin metin,görsel,ses) gelen özellikleri entegre edebilme (birleştirebilme) yeteneği vardır. Bu entegrasyon çeşitli aşamalarda gerçekleşebilir.  Erken Birleştirme (Early Fusion) : Girdi verilerini ilk aşamada birleştirir ve bu sayede farklı modalitelerin ham biçimdeki ilişkilerinden faydalanılır.  Ara Birleştirme (Intermediate Fusion) : Özellik çıkarımı aşamasında birleştirme yapılır. Bu, her bir modalitenin kendine özgü bilgileri ayrı ayrı çıkarıldıktan sonra ortak bir temsilde birleştirilmesi sürecidir.  Geç Birleştirme (Late Fusion) : Her bir modaliteye ait işlem hatlarının (pathway) son çıktıları karar aşamasında birleştirilir. Genellikle çoklu veri türlerinden alınan sonuçların bir arada değerlendirilmesi gereken görevlerde kullanılır.  Ortak Birleştirme (Joint Fusion) : Erken, ara ve geç birleştirmeleri bir araya getiren hibrit bir yaklaşımdır. Böylece veriler tüm aşamalarda en verimli şekilde kullanılmış olur. Bu birleştirme süreçlerinde genellikle önceden eğitilmiş büyük dil modelleri kullanılır. Başlangıçta sadece metin verisi için tasarlanan bu modeller, gelişmiş özellik projeksiyon (feature projection) ve serileştirme (serialization) teknikleri sayesinde çok modelli girdileri işleyebilecek ve sentezleyebilecek şekilde uyarlanır. Şekil 6.1 Çok modlu büyük dil modeli mimarisi", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 85, "page_end": 85}}
{"content": "74  Çok Modlu Çıktı Çözücüsü (Multimodal Output Decoder) Son olarak, çok modlu çıktı çözücüsü, birleştirilmiş bilgiyi belirli görevlere uygun kullanılabilir bir çıktıya dönüştürür. Örneğin:  Görsel Betimleme (Image Captioning) görevlerinde, çözücü görsel girdilere dayalı açıklayıcı metinler üretebilir.  Video Anlama (Video Understanding) görevlerinde ise hem görsel hem de işitsel verileri birleştirerek açıklamalar veya özetler oluşturabilir. Her çözücü, çıktının doğru ve kaliteli olması için dikkatle geliştirilir. Bu sayede, farklı modalitelerden elde edilen bilgiler birleştirilerek anlamlı ve bütüncül sonuçlar elde edilir. Çok modlu büyük dil modellerinin bu mimarisi , metin, görüntü ve ses gibi farklı veri türlerini analiz ederek karmaşık görevlerin üstesinden gelmelerini sağlar (Wang et al.,2024). Tek modlu (unimodal) modeller yalnızca tek bir girdi türü ile sınırlıyken, çok modlu modeller birden fazla modaliteyi eşzamanlı olarak işleyerek gerçek dünya etkileşimlerini daha kapsamlı bir şekilde yansıtabilir. Çok modlu büyük dil modellerinin farklı modaliteleri işleme yeteneği, insan bilişine daha yakın bir yapı sunar. Bu sayede bağlama duyarlı ve sezgisel tepkiler üretebilen, kullanımı daha doğal yapay zekâ sistemleri geliştirilebilir. Örneğin, çok modlu bir büyük dil modeli tabanlı sanal asistan, kullanıcının ses tonuna, yüz ifadesine ve kelime seçimlerine göre ruh halini anlayabilir ve ona göre yanıt verebilir.  Çok Modlu Dil Modellerinin Temel Özellikleri Çapraz Modelli Öğrenme (Cross-Modal Learning) Çok modlu büyük dil modelleri; metin, görsel, işitsel ve bazı durumlarda duyusal verileri içeren geniş veri kümeleri üzerinde eğitilir. Bu yetenek, farklı modaliteler arasında bağlantı kurabilmelerini sağlar. Çok modlu büyük dil modelleri bu sayede, çok çeşitli veri türlerini anlama ve içerik üretimi gerektiren görevleri yerine getirebilir.  Metinden Görüntü Üretimi (Text-to-Image Generation) : Çok modlu büyük dil modelleri, metinsel açıklamalardan görseller üretebilir. Bu durum, grafik tasarımı ve reklamcılık gibi yaratıcılığa ihtiyaç duyulan sektörleri dönüştürmektedir. Örneğin, “gün batımında bir şehir manzarası” tanımına uygun bir görsel üretebilirler.  Görsel Soru Cevaplama (Visual Question Answering) : Bu modeller, görselleri analiz edebilir ve doğal dildeki sorulara doğru yanıtlar verebilir. Örneğin, bir fotoğraf hakkında “Bu görseldeki köpek cinsi nedir?” sorusunu cevaplayabilir.", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 86, "page_end": 86}}
{"content": "75  Çok Modelli İçerik Üretimi (Multimodal Content Creation) : Çok modlu büyük dil modelleri, metin, görsel ve sesi birleştiren içerikler (örneğin, resimli hikayeler veya multimedya sunumları) oluşturmayı kolaylaştırır. Kısa bir komutla hem tutarlı bir hikaye hem de uyumlu görseller üretilebilir. Birleşik temsil (Unified Representation) Çok modlu büyük dil modellerinin farklı modaliteler arasında sorunsuz bir şekilde işlem yapabilmesini sağlamak için, çok modlu verilerin ortak bir biçimde temsil edilmesidir. Bir fotoğrafın betimlenmesi veya metinden görüntü üretimi gibi modaliteler arasında sorunsuz geçiş sağlar. Ayrıca, metin tabanlı sorgularla görsel eşleştirme ya da ses ile görsel içeriklerin eşleştirilmesi gibi çapraz modelli bilgi erişimine imkân tanır (Liang et al.,2024). 6.1.1.1 Öne Çıkan Çok Modlu Büyük Dil Modelleri Çok modlu modellerin getirdiği yenilikleri ve sunduğu fırsatları daha iyi kavrayabilmek adına, bu alandaki bazı güncel modellerin incelenmesi faydalı olacaktır. Bu bölümde, teorik altyapı ile anlatılan bilgilerin somut örneklerle desteklenmesi için söz konusu güncel modellere değinilecektir.  GPT-4V(ision) Büyük çok modlu modeller, büyük dil modellerinin başarısının ardından, yapay zekâ alanında bir sonraki araştırma ve ilgi konusu olmuştur. Büyük dil modellerine yönelik ilk çabalar, görsel kodlayıcıların dil gömme (language embedding) yapılarıyla hizalanacak şekilde ince ayar yapılarla bilgisayarla görmenin büyük dil modelleri ile entegre olmasını içermektedir. Bu durum, görsel bilginin metine dönüştürülmesini kapsamaktadır. Bu modellerin çoğu genellikle ölçek bakımından sınırlıdır. Yakın zamanda, GPT-4V, eşi görülmemiş ölçekteki veri ve hesaplama kaynaklarıyla eğitilmesiyle çok modlu büyük dil modellerinde öncü haline gelmiştir. Bu çok modlu modeller, dil ve görsel görevlerde değerlendirme amaçlı kullanılabilmenin yanı sıra çeşitli kabiliyetlere sahiptir (Wu et al.,2024). Mart 2023’te GPT-4’ün piyasaya sürülmesi, çok modlu büyük dil modelleri açısından önemli bir gelişme olmuştur. GPT-4V ( Generative Pre-training Transformer- 4th series with Vision), gelişmiş görsel değerlendirme yetenekleriyle tanıtılmıştır. GPT- 4V modeli, görsel analiz yeteneğine sahiptir. Bu yüzden, dilsel ve görsel bilgiyi birleştirerek karmaşık durumların üstesinden gelebilir (Güneş ve Ülkir,2025).", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 87, "page_end": 87}}
{"content": "76 GPT-4V’nin giriş modlarını 3 başlık altında inceleyebiliriz : 1. Sadece Metin Girişi (Text-only Input) GPT-4V, güçlü dil yetenekleri sayesinde sadece metin girişleriyle etkili bir dil modeli olarak kullanılabilir. Yalnızca metin ile giriş ve çıkış işlemleri yaparak, çeşitli dil ve kodlama görevlerini yerine getirebilir 2. Sadece Tek Görüntü-Metin Çifti (Single Image-Text Pair) Çok modlu büyük dil modeli olan GPT-4V, görüntüleri ve metinleri giriş olarak alıp metinsel çıktılar üretebilir. Mevcut genel amaçlı görsel-dil modelleriyle uyumlu olarak çalışır. Tek bir görüntü-metni çifti veya yalnızca tek bir görüntü olarak çeşitli görsel ve görsel-dil görevlerini yerine getirebilir. Bu görevlere görüntü tanıma, nesne konumlandırma, görüntü başlığı ve görsel soru yanıtlama örnek olarak verilebilir. Görüntü-metin çiftindeki metin, başlık için “görüntüyü tanımla” gibi bir talimat olarak ya da görsel soru yanıtlama görevindeki soru gibi bir sorgu girişi olarak kullanılabilir. 3. İç İçe Geçmiş Görüntü-Metin Girişleri ( Interleaved Image-text Inputs) GPT-4V’nin, iç içe geçmiş görüntü-metin girişlerini işleme yeteneği çok modlu bu büyük dil modelinin genel kapsamını artırmıştır. İç içe geçmiş görüntü-metin girişleri, farklı türlerde olabilir. Bunlar, kısa bir soru veya talimatla birden fazla görüntü içeren görsel girişler, iki görüntüyle birlikte uzun bir web sayfası gibi metin odaklı girişler ya da hem görselleri hem de metinleri dengeli bir şekilde içeren karışık girişler olabilir. Bu çeşitlilik, çok sayıda farklı uygulama alanında uyarlanmasını sağlar. Örneğin, birden fazla fiş görüntüsündeki toplam ödenen vergiyi hesaplayabilir. Menüdeki içeceklerin fiyatını, sayısını bulmak ve toplam maliyeti hesaplamak gibi işlemleri gerçekleştirebilir. GPT-4V’nin olağanüstü zekası, önceki çalışmalarla karşılaştırıldığında önemli ölçüde geliştirilmiş bir performans ve genellenebilirlik özelliğiyle öne çıkmaktadır (Yang et al.,2023).  GEMINI Google DeepMind tarafından geliştirilen Gemini AI, çok modlu üretken yapay zekâ (GenAI) teknolojisinin gücünden faydalanmaktadır. Bu yaklaşım, yapay zekânın yalnızca metin verileriyle değil, aynı zamanda görseller, sesler ve daha fazla çeşitli diğer veri türleriyle eğitilmesini içerir. Bu durum, yapay zekâ teknolojisinin çapraz-modal anlayış (cross-modal understanding) ve bütünleştirme gerektiren görevlerde yüksek performans göstermesini sağlar (Perera and Lankathilake,2023).", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 88, "page_end": 88}}
{"content": "77 Büyük dil modelleri ve çok modlu büyük dil modellerinin ortaya çıkması, bağlam uzunluğunu geliştirmiş ve iyileştirmiştir. Çok modlu yetenekler sayesinde, çeşitli veriler arasında son derece karmaşık akıl yürütmelerin mümkün olabileceği öngörülmektedir. Sadece metinle sınırlı kalmayıp, son zamanlardaki çalışmalar bu temel çok modlu modellerin yeteneklerini, çeşitli alanlarda genişletmiştir. Google tarafından geliştirilen Gemini modellerinin yayınlanması, gelişmiş çok modlu yetenekleri ve uzun bağlam anlama konusundaki ilerlemeleri ile çok modlu akıl yürütme konusunda önemli bir gelişmenin temsilcisi olmuştur. Böylece, bu yeni çok modlu modellerin temel performanslarını değerlendirme çalışmaları başlamıştır. (Google Research and Google DeepMind,2024). Gemini modelleri doğal olarak çok modlu bir yapıya sahiptir. Bu modeller, farklı modaliteler (farklı veri türleri) arasında sorunsuz bir şekilde yeteneklerini birleştirebilmektedir. Örneğin, bir tablo, grafik veya şekilden hem içerik hem de mekânsal yerleşim bilgilerini çıkarabilmekte ve bunu güçlü bir dil modelinin akıl yürütme yetenekleriyle birleştirebilmektedir ( örneğin, matematik ve kodlama alanlarında elde edilen yüksek performans). 1. Video Anlama (Video Understanding) Video girdisini anlayabilmek, faydalı bir genel amaçlı ajan geliştirmede önemli bir adımdır. Gemini modellerinin video anlama kapasitesi, eğitim verisinde hariç tutulan çeşitli kıyaslama ölçütleri (benchmark) aracılığıyla değerlendirilmiştir. Gemini Ultra birkaç örnekle yapılan video açıklama görevlerinde ve sıfır örnekle yapılan video soru- cevap görevlerinde son teknoloji seviyesinde performans göstermiştir. 2. Görüntü Üretimi (Image Generation) Gemini modelleri, doğrudan görsel çıktı üretebilmektedir. Bu durum modelin görselleri üretirken araya doğal dil tanımlaması yerleştirmeksizin çalışmasını sağlar. Böylece, dilsel açıklamaların betimleme kapasitesini sınırlama etkisi ortadan kalkar ve model çok daha etkili şekilde görsel içerik üretebilir. Model az sayıda örnekle bile çalışırken, metin ve görüntülerin iç içe geçtiği girdilere dayalı olarak başarılı görsel önerileri üretebilir. 3. Ses Anlama (Audio Understanding) Gemini Nano-1 ve Gemini Pro modelleri, çeşitli açık veri setleri üzerinden değerlendirilmiştir. Değerlendirilen kıyaslama ölçütleri arasında otomatik konuşma tanıma görevleri ve konuşmadan çeviri görevi yer almaktadır. Gemini Pro modeli, konuşma tanıma ve konuşmadan çeviri görevlerinde en iyi performansı göstermiştir. Gemini Nano-1, çoğu konuşma tanıma ve çeviri görevinde başarılı olmuştur ancak bazı veri setlerinde daha düşük performans sergilemiştir. (Gemini Team and Google,2025).", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 89, "page_end": 89}}
{"content": "78  FLAMINGO Çok modlu makine öğrenimi alanında, yalnızca sınırlı sayıda etiketli örnekle yeni görevlere hızlı bir şekilde uyum sağlayabilen modeller geliştirme ihtiyaç duyulmaktadır. Bu ihtiyaç, Flamingo modelinin geliştirilmesine zemin hazırlamıştır. Flamingo modelleri, büyük ölçekli çok modlu verilerin dikkatle seçilmiş bir karışımı üzerinde eğitilmiştir. Bu eğitim sonrasında model, herhangi bir göreve özgü ince ayar gerekmeksizin yalnızca birkaç örnekle farklı görsel görevlerde doğrudan kullanılabilmektedir. Flamingo modeli, görsel soru-cevaplama, sahne ya da olayı tanımlamaya dayalı altyazı oluşturma ve çoktan seçmeli görsel soru-cevaplama gibi görevlerde, yalnızca örnekli istemlerle (prompt) son teknoloji düzeyinde sonuçlar elde etmektedir. Ayrıca, birçok ölçüt üzerinde, binlerce kat daha fazla göreve özgü ince ayar gerektiren modellerden daha iyi performans göstermektedir. Mimari tasarımı sayesinde yüksek çözünürlüklü görsel veya video verilerini işleyebilen bu modeller, metin ile birlikte görselleri veya videoları içeren bir dizi girdiyi alıp çıktı olarak metin üretme yeteneğine sahiptir. Bu yönüyle Flamingo, görsel ve dilsel bilgiyi birleştiren güçlü bir model olarak öne çıkmaktadır (Alayrac et al.,2022).  KOSMOS-1 KOSMOS-1, genel modları algılayabilen, talimatları izleyebilen, bağlamdan öğrenebilen ve çıktı üretebilen çok modlu bir dil modelidir. Bu model, önceki bağlamı göz önünde bulundurarak metinleri otoregresif bir şekilde üretmeyi öğrenir. KOSMOS- 1’in temel yapısı, Transformer tabanlı bir nedensel dil modelidir. Metin dışında, diğer modlar da modele gömülerek dil modeline verilir. Transformer kod çözücüsü (decoder), çok modlu girdi için genel amaçlı bir arayüz olarak hizmet eder. KOSMOS-1, tek modlu veriler, çapraz modlu veriler ve sıralı çok modlu veriler dahil olmak üzere veri kümeleri üzerinde eğitilmiştir (Huang et al.,2023).  CLAUDE 3 Anthropic tarafından geliştirilen Claude 3 modeli, Mart 2024’te duyurulmuştur. Claude 3 Opus, en yetenekli model olarak tanıtılırken, Claude 3 Sonnet, beceri ve hızı bir araya getiren model, Claude 3 Haiku ise en hızlı ve en uygun maliyetli model olarak tanıtılmıştır. Bu modellerin tamamı, görsel verileri işleyip analiz edebilme yeteneğine sahip olup “çok modlu model ailesi” olarak tanımlanır. Claude 3 ailesi, kıyaslama ölçütlerinde güçlü bir performans sergilemekte ve akıl yürütme, matematik ve kodlama alanlarında yeni bir standart belirlemektedir. Claude 3 ailesindeki önemli bir iyileştirme, metin çıktılı çok modlu yetenekleridir. Bu özellik, kullanıcıların tablolar, grafikler, fotoğraflar gibi görselleri metin istemleriyle birlikte yükleyerek daha zengin bir bağlam sağlamalarına ve kullanım senaryolarını genişletmelerini sağlar. Claude model ailesi", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 90, "page_end": 90}}
{"content": "79 ayrıca, araç kullanımı (ya da fonksiyon çağrısı) olarak bilinen özelliğiyle de öne çıkar. Bu özellik, Claude’un zekâsının özel uygulamalara ve iş akışlarına sorunsuz bir şekilde entegre edilmesini sağlar (Anthropic,2024). Son zamanlarda, GPT-4o- ve Gemini 1.5 Pro gibi çok modlu büyük dil modelleri yeteneklerini görsel ve işitsel modları da kapsayacak şekilde genişleterek etkileyici performanslar sergilemiştir. Çok modlu büyük dil modelleri görsel-dil modelleriyle başlayarak kademeli olarak evrilmiştir. Örneğin, GPT-4V(ision), dil modellerine görsel algı kazandırarak onları geniş bir yelpazedeki görsel-dil görevlerini çözebilecek hale getirmiştir. Bu modeller, görüntülerdeki nesneleri sayma, tablo verileri üzerinde sayısal hesaplamalar yapma ve verilen şekillerle geometrik problemleri çözme gibi dikkat çekici yetenekler sergilemektedir. Bu gelişmeler, çok modlu akıl yürütmenin sınırlarını zorlamakta, özellikle otomatik konuşma tanıma, otomatik konuşmadan çeviri, görsel- işitsel açıklama üretimi ve genel görsel-işitsel işleme gibi alanlarda ilerlemektedir (Gong et al.,2024). Tüm bu gelişmeler göz önüne alındığında, akıllara şu sorunun gelmesi kaçınılmazdır : Madem bu alan Turing’in “Makineler düşünebilir mi?” sorusuyla başladıysa, geldiğimiz noktada bu modeller Turing testini geçebilecek seviyeye ulaşmış mıdır? Jones ve Bergen tarafından gerçekleştirilen ve 2025 yılında yayımlanan “Large Language Models Pass the Turing Test” (Büyük Dil Modelleri Turing Testini Geçiyor) başlıklı çalışmada, GPT-4.5 modelinin klasik üçlü Turing testinde %73 oranında insan olarak tanındığı belirtilmiştir. Bu sonuçlar, bir yapay sistemin Turing testini başarıyla geçtiğine dair ilk ampirik kanıt olarak sunulmuştur. Ancak çalışmada da vurgulandığı üzere, Turing testi zekânın doğrudan bir ölçüsü değil, insan benzerliğinin bir göstergesidir. Bu nedenle bu başarıyı, yapay zekânın “düşünebildiği” olarak değil, insanla ayırt edilemeyecek düzeyde iletişim kurabildiği şeklinde değerlendirmek daha yerinde olacaktır. 6.1.2 Çok Ajanlı Sistemler (Multi-Agent Systems) Ajan (agent) terimi, çevresini doğal dil, görsel, işitsel veya kod gibi çok modlu girdiler aracılığıyla algılayan ve belirli bir hedefe ulaşmak için eyleme geçen yapay zekâ temelli bir varlık olarak tanımlanır. Ajanik / etken (agentic), ise bu tür davranışları tanımlamak için kullanılan bir kavramdır. Bu tanım, büyük dil modellerinden çok modlu temel modellere; geleneksel makine öğrenimi yaklaşımlarından, bilgi tabanları ve araçları entegre eden çok ajanlı sistemlere kadar geniş bir yelpazeyi kapsar (Jiang et al.,2025).", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 91, "page_end": 91}}
{"content": "80 Çok ajanlı sistemlerin (Multi-Agent System-MAS) temelini anlamak için önce tek ajanlı sistemlerden bahsetmek faydalı olacaktır. Tek ajanlı sistem (single-agent system) çevresini bağımsız olarak algılayabilen ve kararlar verebilen tek bir LLM tabanlı akıllı ajandan oluşur. Bu tür sistemlerin tasarımı, belirli görevleri yerine getirmeye odaklanır. Bu görevler, basit otomasyondan karmaşık karar verme süreçlerine kadar geniş bir yelpazeyi kapsar. Sistem, ajanın bireysel özellikleri, algılama yetenekleri ve eylem gerçekleştirme kapasitesine dayanır. Tek ajan, benzersiz özelliklere ve yeteneklere sahiptir. Bu özellikler ajanın hedeflerini, bilgisini, becerisini ve diğer ajanlarla etkileşim yollarını içerebilir. Ayrıca, ajanın dış dünyayı anlaması ve yorumlaması, sensörlerden ya da veri kaynaklarından aldığı bilgiyi işlemesini içerir. Bu algılamanın sonucunda, ajanın çevresel değişimlere yanıt verebilmesi ve kendi hedeflerine ulaşabilmesi için kararlar alıp eylemler yapabilme yeteneği vardır. Tek ajanlı sistemler, kaynakları tek bir ajana yönlendirdikleri için belirli görevlere hızlı yanıt verebilir ve yüksek verimlilikle çalışabilirler. Bu tür sistemler, çok ajanlı sistemlere göre daha basit tasarıma ve bakıma sahiptir. Karmaşık iletişim ve işbirliği gereksinimi olmadığı için sistemin hata ayıklama süreci daha kolay hale gelir (Li et al.,2024). Dağıtık Problem Çözme (Distributed Problem Solving), ortak bir problemi çözmek amacıyla birden fazla ajan (agent) arasında görev paylaşımının yapıldığı sistemleri ifade ederken, çok ajanlı sistemler (MAS), her bir ajanın kendi hedeflerine veya bilgi alanına sahip olabildiği, etkileşimli ajanlardan oluşan daha genel yapılar olarak tanımlanır (Durfee and Rosenchein,1994). Dağıtık yapay zekânın bir alt dalı olan çok ajanlı sistemler, dağıtık problemleri çözmedeki esnekliği ve zekâsı sayesinde hızlı bir gelişim göstermektedir. MAS, tek bir ajan veya sistemin çözmesinin zor olduğu problemleri ele alabilir. Bu sistemlerin zekâsı; organize, işlevsel ve süreç odaklı davranışlar ile arama algoritmaları ve pekiştirmeli öğrenme yaklaşımlarında kendini gösterir. Görevleri yerine getirmek için, çok ajanlı sistemler farklı ajanlara farklı roller atar ve karmaşık görevler ajan iş birliği sayesinde tamamlanır (Dong,2024). Çok ajanlı sistem, bağımsız kararlar alabilen, kendi özel bilgi durumlarını sürdürebilen ve doğrudan iletişim kanalları aracılığı ile ya da ortak çevrede değişiklik yaparak birbirleriyle etkileşime girebilen iki ya da daha fazla otonom yapay zekâ ajanının oluşturduğu bir ağ olarak tanımlanır. Bu sistemlerde ajanlar, kendi hedeflerini ya da kendilerine atanmış görevleri yerine getirirken, çevresel değişimlere ve diğer ajanların eylemlerine karşı davranışlarını uyarlayabilirler (Witt et al.,2025). Çok ajanlı sistemler, sundukları faydalar nedeniyle birçok uygulama alanında yaygın olarak benimsenmiştir. Büyük sistemlerde MAS teknolojisinin sunduğu avantajlar şunlardır :", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 92, "page_end": 92}}
{"content": "81 1. Paralel hesaplama ve eş zamansız çalışma (asynchronous operation) sayesinde işlem hızında ve verimliliğinde artış sağlanır. 2. Ajanlardan bir veya daha fazlası arızalandığında sistemde zarif bir bozulma (graceful degradation) meydana gelir. Bu da sistemin güvenilirliğini ve sağlamlığını (robutness) artırır. 3. Ölçeklenebilirlik (scalability) ve esneklik (flexibility) sayesinde ajanlar gerektiğinde sisteme eklenebilir. 4. Azaltılmış maliyete sahiptir çünkü bireysel ajanların maliyeti merkezi bir mimariye göre çok daha düşüktür. 5. Ajanlar modüler bir yapıya sahiptir ve başka sistemlerde kolayca değiştirilebilir, bu sayede yeniden kullanılabilir.. (Balaji and Srinivasan,2010) Çok ajanlı sistemler (MAS), çeşitli fayda ve avantajlar sunduğu gibi, bazı önemli zorluklarla da karşı karşıyadır. MAS teknolojisi, çeşitli uzmanlıklara sahip ajanlar ve daha karmaşık etkileşimler ile katmanlı bağlam bilgileri içerdiğinden, iş akışının ve tüm sistemin tasarlanmasında zorluklar ortaya çıkmaktadır. Bu sistemlerdeki ajanlar, tek ajanlı sistemlerle aynı yeteneklere sahip olsa da, çok ajanlı sistemlerin iş akışından kaynaklanan zorluklarla karşılaşmaktadır. Ayrıca, çok ajanlı sistemlerde bellek yönetimi, karmaşık bağlam verilerini ve tarih bilgilerini yönetmek zorunda olduğu için bellek için gelişmiş bir tasarım gerektirir (Han et al.,2025). Çok ajanlı sistemler, gerçek dünyada oldukça geniş bir kullanıma sahiptir ve çok sayıda sektörü kapsamaktadır. Ulaştırma alanında, otonom araçlar arasında iletişimi kolaylaştırarak trafik akış verimliliğini artırmada rol oynamaktadır. Bu iletişim, araçların hareketlerini zaman içinde koordine etmelerini sağlar, bu da tıkanıklığı azaltır ve güvenlik seviyelerini artırır. Böyle bir koordinasyon, araçların yol koşulları ve trafik trendleri hakkında bilgi alışverişinde bulunmasını sağlayan algoritmalar sayesinde gerçekleşir. Benzer şekilde sanayide MAS, pazar dinamiklerini analiz etmek için kullanılmaktadır. Otonom ticaret ajanları, diğer ajanların pazardaki kararlarına tepki verirken ticaret yapmak için rekabet etmektedir. Bu yöntemler, pazar trendlerini tahmin etmeye ve hızlı kararlar almaya çalışan algoritmalar kullanır. MAS, robotik alanında da kullanılmaktadır. Bir grup robot, montaj hatlarını veya depoları yönetmek gibi görevleri yerine getirmek için bir araya gelir. Bu robotların işbirlikçi yapısı, görevleri bölüştürerek ve çevrelerindeki değişikliklere hızla uyum sağlayarak etkili bir şekilde birlikte çalışmalarını sağlar (Kosaraju,2024).", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 93, "page_end": 93}}
{"content": "82 6.1.2.1 Çok Ajanlı Sistemlerin Büyük Dil Modelleri ile Entegrasyonu Üretken yapay zekânın çok ajanlı sistemlere entegre edilmesi, simülasyonlarda ajan davranışlarının bağımsızlığını, uyarlanabilirliğini ve gerçekçiliğini artırmayı amaçlamaktır. Bu yaklaşım, ajan etkileşimlerini etkileyebilmek için büyük dil modellerinde yer alan geniş bilgi birikiminden faydalanmaktadır. Büyük dil modellerinin ajan tabanlı simülasyonlarla birleşimi, doğal dil işleme ile karmaşık sistem modellemesini mümkün kılmıştır. İnsan benzeri metin üretme ve karmaşık dil kalıplarını anlama yetenekleri sayesinde büyük dil modelleri, yapay zekâ alanında, özellikle dil temelli görevlerin otomasyonu ve yorumlanması açısından birçok alanı dönüştürmüştür (Romero et al.,2025). Çok ajanlı büyük dil modelleri, günümüzde popüler bir araştırma konusu haline gelmiştir. Çalışmaların çoğu toplumdaki bireylerin etkileşimlerini simüle eden modeller (sosyal simülasyonlar) ve bu modelleri oluşturup çalıştırmak için kullanılan yazılım altyapılarına yoğunlaşmıştır. Bu başarılar temel alınarak oyun ortamlarını, dünya savaşlarını, ekonomi piyasalarını, öneri sistemlerini ve pandemi simülasyonlarını inceleyen çalışmalar yapılmıştır. Öte yandan, problem çözme alanında da ilerlemeler sağlanmaktadır. Çok ajanlı tartışmalar yoluyla kısa metinlerden akıl yürütülmesine, farklı akıl yürütme görevleri için tartışma yöntemlerine, mekanik problemler, makale inceleme, bilgi grafiklerinin oluşturulması ve kodlama gibi alanlara odaklanılmaktadır (Zhang et al.,2024). Ajan, bir uygulamanın kontrol akışını belirlemek için büyük dil modellerini kullanan bir sistemdir. Daha önce de belirtildiği gibi, tek ajanlı sistemler tek bir görevi yerine getirirken çok ajanlı sistemler karmaşık görevleri yerine getirirler. Bağlam, tek bir ajanın takip edemeyeceği kadar karışık bir hale geldiğinde çözüm yöntemi olarak uygulama küçük birkaç bağımsız ajana bölünür. Ardından, çok ajanlı sistem yapısına göre yapılandırılır. Denetleyici/ Gözetmen Ajan (Agent Supervisor) iş akışı içerisinde merkezi bir rol oynar; farklı görev/işçi ajanı (worker agents) arasındaki iletişimi ve görev dağılımını koordine eder. Denetleyici ajan, bir ajandan çıktıları alır ve bu mesajları yorumladıktan sonra iş akışını buna göre yönlendirir. Denetleyici, görevlerin sırasını yönetmekten, iş akışında atılacak bir sonraki adımlara karar vermekten ve sistemin genel verimliliğini sağlamaktan sorumludur. Çok ajanlı sistemin genel mimarisi Şekil 6.2’de sunulmuştur.", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 94, "page_end": 94}}
{"content": "83 Bir çok ajanlı sistem, birden fazla otonom ajanın etkileşime girerek iş birliği yapması sonucunda karmaşık problemlerin çözüldüğü sistemi ifade eder. Her bir ajan, bağımsız karar verme yeteneklerine sahiptir. Bu ajanlar bilgi alışverişinde bulunarak ortak bir hedefe ulaşmak için birlikte çalışır. Son zamanlarda büyük dil modelleri alanındaki gelişmelerle birlikte, büyük dil modelleri tabanlı çok ajanlı sistem (LLM- MAS) yeni bir yaklaşım olarak ortaya çıkmıştır. Geleneksel çok ajanlı sistemlerin aksine, büyük dil tabanlı çok ajanlı sistemler, doğal dil işleme yetenekleri sayesinde daha esnek ve uyarlanabilir bir iş birliği fırsatı sunar. Özellikle, her bir ajan alanına özgü görevleri yerine getirirken, doğal dilde etkileşimler yoluyla karmaşık görevlerin koordine edilmesini sağlar (Jeong,2025).  Büyük Dil Modelleri Tabanlı Çok Ajanlı Sistemlerin Temel Bileşenleri 1. Üretici Ajanlar (Generative Agents) Üretici ajanlar, LLM-MAS’ın bileşenleri olup, rol tanımlarına sahip, çevreyi algılayabilen, kararlar verebilen ve çevreyi değiştirmek için karmaşık eylemleri yerine getirebilen yapılardır. Bu ajanlar, bir oyundaki bir oyuncu ya da sosyal medyada bir kullanıcı olabilir. LLM-MAS’ın gelişimini yönlendirerek sonuçlarını etkileyen bu ajanlar, geleneksel ajanlara göre farklılık gösterir. Örneğin, geçmiş bilgilere dayalı olarak kişiselleştirilmiş blog gönderileri üretmek gibi karmaşık davranışları başarıyla sergileyebilir. Bu nedenle, LLM’leri çekirdek olarak kullandıkları söylenebilir. Üretici ajanların aşağıdaki özelliklere sahip olması gerekir:  Profil Oluşturma (Profiling) : Davranışlarını birbirleriyle koordine bir şekilde ayarlamak için rolleri doğal dilde tanımlayarak veya her üretici ajan için görevlerine göre özel istemler (promptlar) oluşturarak gerçekleştirilir.  Bellek (Memory) : Geçmişteki bilgileri saklayarak daha sonraki ajan eylemleri için ilgili anıları geri çağırmak amacıyla kullanılır. Bu durum sayesinde, LLM’lerin bağlam penceresi sorunu çözülürken uzun mesafeli bağımlılıkları yakalaması sağlanır. Genellikle uzun vadeli, kısa vadeli ve duyusal bellek olmak üzere üç katmandan oluşur.  Planlama (Planning): Gelecekteki daha uzun bir zaman dilimindeki hedefler doğrultusunda genel davranışların formüle edilmesidir. Şekil 6.2 Çok ajanlı sistemin genel mimarisi", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 95, "page_end": 95}}
{"content": "84  Eylem (Action) : Üretici ajan ile çevre arasındaki etkileşimi yürütür. Üretici ajanlardan, birden fazla aday davranıştan birini seçmeleri ya da belirli bir kısıtlama olmadan bir davranış üretmeleri beklenebilir. Üretici ajanlar sistem içinde iş birliği sağlamak amacıyla birbirleriyle iletişim kurabilir. Ayrıca, elde ettikleri bilgileri diğer zeki ajanlarla paylaşmak ve belli bir düzeyde birden fazla ajanı bütün sistemle birleştirerek bağımsız ajanlarında üstünde performans elde etmeyi hedefler. Üretici ajanlar iletişim kurarken fikir birliğine ulaşmaya çalışır. Bazı ajanlar arasında davranış ve strateji benzerliğini artırmayı hedefler. 2. Çevre (Environment) Çevresel faktörler, kuralları, araçları ve müdahale arayüzlerini içerir.  Araçlar (Tools) : Ajanın eylem talimatını belirli sonuçlara dönüştürür. Üretici ajanlar çevreye bir eylem talimatı gönderir. Çevre bu talimatı, eylemin gerçekleştirildiğine dair bir kayıt haline getirir. Farklı sahnelerde farklı eylem alanları bulunur. Örneğin, sosyal medyada eylem alanı “beğen”, “takip et”, “yorum yap” gibi eylemleri içerir.  Kurallar (Rules) : Üretici ajanlar arasındaki iletişim şeklini ve çevre ile olan etkileşimi tanımlar. Kurallar sayesinde tüm sistemin davranış yapısı doğrudan belirlenir. Sahneye bağlı olarak sisteme özel bazı kurallar vardır (Örneğin, oyun kuralları ve sosyal davranış normları).  Müdahale (Intervention): Müdahale dışsal herhangi bir kaynaktan gelebilir. Bu dışsal kaynak, bir insan bir denetleyici model ya da bir üretici ajan olabilir. Müdahalenin amacı, sistemden aktif şekilde bilgi almak ya da kontrolsüz davranışlarını oluşmasını önlemek için sistemi pasif şekilde kesmek olabilir. (Chen et al.,2025) 6.1.2.2 Büyük Dil Modelleri Tabanlı Ajanların Uygulama Alanları Büyük Dil Modelleri tabanlı ajanlar ( LLM-based agents) incelendiğinde, Tek Ajanlı (Single-Agent) ve Çok Ajanlı (Multi-Agent) sistemler olarak iki temel sınıfa ayrıldıkları görülmektedir. Bu farklı sistem türleri ; uygulama alanları, bellek ve yeniden değerlendirme mekanizmaları, veri gereksinimleri, farklı veri türleri ve araç setleri gibi birçok açıdan farklılıklar göstermektedir. Bu bölümde LLM tabanlı ajanların uygulama alanları incelenecektir (Cheng et al.,2024).", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 96, "page_end": 96}}
{"content": "85 1. Dünya Simülasyonu Büyük dil modelleri tabanlı çok ajanlı sistemlerin (LLM-MAS) yaygın kullanıldığı uygulama alanlarından olan dünya simülasyonları oldukça dikkat çeken ve hızla büyüyen bir araştırma konusudur. Dünya simülasyonu sosyal bilimler, oyunlaştırma, psikoloji, ekonomi, politika üretimi gibi çok çeşitli alanlara yayılmaktadır. Bu uygulama alanı için LLM-MAS kullanılmasının nedeni, bu sistemlerin başarılı rol yapma yetenekleridir. Bu yetenekler, simüle edilen dünyalarda farklı rollerin ve bakış açılarının gerçekçi biçimde canlandırılması açısından kritik bir öneme sahiptir. Dünya simülasyonu projelerinde kullanılan çevreler, genellikle simüle edilmek istenen senaryoya uygun olacak şekilde tasarlanır ve ajanlar, bu bağlama uygun çeşitli profillerle yapılandırılır. Ajan iş birliğine odaklanan problem çözme sistemlerinden farklı olarak, dünya simülasyonu sistemleri, ajan yönetimi ve iletişimine ilişkin çeşitli yöntemleri kapsar; bu da gerçek dünyada var olan çeşitliliği yansıtır (Guo et al.,2024). 2. Soru-Cevap / Doğal Dil Üretimi Büyük dil modellerinin, çok ajanlı sistemlere entegre edilmesi, soru-cevap ve doğal dil üretimi yeteneklerinde önemli ilerlemeler sağlamıştır. Günümüzün önde gelen teknoloji şirketleri tarafından geliştirilmiş ve her biri ajan iş birliğini pratik uygulamalarda kolaylaştırmak için mekanizmalar kullanan sistem mimarileri veya yapıları bulunmaktadır. OpenAI’ın Swarm Yapısı : Bu yapı, rutinler (routines) ve görev devirleri (handoffs) kavramları aracılığıyla birden çok ajanın birlikte çalışmasını organize eden bir yaklaşım sunmaktadır. Bu yapıda, bir ajan ; belirli talimatlar ve araçlar içeren bir varlık olarak tanımlanır ve aktif bir konuşmayı başka bir ajana devredebilir. Bu sürece “görev devri” (handoff) adı verilir. Bu mekanizma, belirli görevlerde uzmanlaşmış ajanlar arasında sorunsuz geçiş sağlayarak sistemin genel verimliliğini ve uyarlanabilirliğini artırır. Swarm’ın tasarımı, hafif koordinasyon ve yürütmeye odaklanarak onu ölçeklenebilir, gerçek dünya uygulamaları için uygun hale getirir. Şekil 6.3’te müşteri hizmetleri bağlamında çalışan çok ajanlı bir Swarm sisteminin örneği yer almaktadır. Bu örnekte bir kullanıcı sipariş vermek ister, ancak fikrini değiştirerek iade talebinde bulunur. Swarm yapısı, bu süreci farklı rollerde uzmanlaşmış ajanlar arasında görev devriyle yöneterek kullanıcıya hızlı ve esnek bir deneyim sunar (Tran et al.2025).", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 97, "page_end": 97}}
{"content": "86 3. Kod Üretimi Kod üretimi, yazılım mühendisliği araştırmalarında uzun süredir önemli bir odak noktası olmuştur. Bu alan, kodlama görevlerini otomatikleştirerek üretkenliği artırmayı ve insan hatalarını azaltmayı hedeflemiştir. Çok ajanlı sistemlerde kod üretimi genellikle ajanların görev uzmanlaşması ve geri bildirim döngüleri ile iş birliği optimize etmesi üzerine kuruludur. Literatürde yaygın olarak tanımlanan roller arasında orkestratör (orchestrator), programcı(programmer), inceleyici (reviewer), testçi (tester) ve bilgi toplayıcı (information retriever) ajanlar bulunur. Uygulama aşamasında süreç genellikle programcının kodun ilk versiyonunu yazmasıyla başlar. İlk kod üretildikten sonra inceleyici ve testçi gibi roller devreye girer. Kodun kalitesi, işlevselliği ve gereksinimlerine göre geri bildirim sağlanır. Bu geri bildirimler, programcının kodu iyileştirmesini ve hata ayıklayıcının (debugger) tespit edilen sorunları çözmesini sağlar. Böylece kod istenilen standartlara uygun hale gelerek beklendiği gibi çalışır. Örneğin, INTERVENOR sisteminde Kod Öğrencisi (Code Learner) ilk kodu üretir ve derleyerek doğruluğunu kontrol eder. Hata varsa Kod Öğreticisi (Code Teacher) hataları analiz eder ve onarılması için talimat verir. Kendini Onaran (Self-repair) ve TGen sistemleri ise önceden belirlenmiş test senaryolarından alınan geri bildirimlerle kodu iyileştirir. Testler yoksa, testçi farklı senaryolar ve uç durumlar için testler üretir; bu testler gizli sorunların ortaya çıkmasını sağlar ve sonraki iyileştirme süreçlerine rehberlik eder (He et al.2024). Şekil 6.3 OpenAI’nın Swarm sistemi ve müşteri hizmetlerinde kullanımı", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 98, "page_end": 98}}
{"content": "87 4. Eğitim Büyük dil modelleri tabanlı ajanlar, bellek, araç kullanımı ve planlama gibi temel bileşenleri birleştirerek çeşitli görevlerdeki performansı artırır. Bu ajanlar, temel bilgi için uzun vadeli belleği ve gerçek zamanlı uyum için ise kısa vadeli belleği kullanarak bağlama duyarlı etkileşimler sağlar. Özetleme ve geri getirme gibi aktif bellek yönetimi yöntemleri bilginin korunmasına yardımcı olur. Büyük dil modelleri tabanlı ajanların eğitimdeki kullanım senaryoları arasında, öğretmenleri desteklemek amacıyla geliştirilen öğretim yardımı ajanları (agents for teaching assistance) öne çıkmaktadır. Bu ajanlar, büyük dil modellerinden yararlanarak eğitim sürecinin çeşitli aşamalarında kişiselleştirilmiş, ölçeklenebilir ve verimli destek sunar. Temel amaçları, öğretim kalitesini artırmak, öğrenci öğrenme deneyimlerini zenginleştirmek ve öğretmenlerin iş yükünü azaltmaktır. Bu ajanlar, bağlamsal bilgileri hatırlayabilme (hafıza yönetimi), dış veri tabanlarına ve API’lere erişim sağlayan araçlarla birçok gelişmiş yeteneği yerine getirebilir. Örneğn, öğrenci profillerini hassas bir şekilde modellemeye yönelik planlama yapabilir. Bu sayede, sınıf simülasyonu, geri bildirim yorum üretimi ve öğrenme kaynağı önerisi gibi temel alanlarda etkili destek sağlarlar (Chu et al.,2025). 5. Finans TradingAgents, çok sayıda uzmanlaşmış ajanın tartışmalar ve etkileşimler yoluyla iş birliği yaptığı, gerçekçi bir ticaret firması ortamını simüle eden büyük dil modelleriyle çalışan hisse senedi ticaret sistemi olarak tanıtılmıştır. Gelişmiş büyük dil modelleri yeteneklerini kullanarak çeşitli veri kaynaklarını işleyip analiz eden bu sistem, bilinçli ticaret kararlarının alınmasını sağlamaktadır. Çok ajanlı etkileşimler, karar vermeden önce ayrıntılı bir düşünme ve tartışma sürecinden geçerek yüksek performans göstermişlerdir. Farklı roller ve risk profillerine sahip ajanlar, yansıtıcı bir ajan ve özel risk yönetim ekibi ile entegre edilmiştir. Böylece, geleneksel modellerden çok daha iyi ticaret sonuçları elde edilmiştir. Ayrıca ajanların işbirlikçi yapısı, piyasa koşullarına uyum sağlama yeteneğini de güçlendirmiştir (Xiao et al.,2025). Finansal İstikrar Kurulu (Financial Stability Board-FSB), finansal hizmetlerde yapay zekâ ve makine öğrenimin artan önemini kabul etmiştir. Finans kurumları yapay zekâ ajanlarını şu amaçlarla kullanmaktadır :  Risk Değerlendirmesi : Yapay zekâ ajanları, finansal riskleri tahmin etmek için büyük veri setlerini analiz eder.  Algoritmik Ticaret : Çok ajanlı modeller, ticaret stratejilerini geliştirir.  Dolandırıcılık Tespiti : Yapay zekâ ajanları, gerçek zamanlı olarak sahte işlemleri tespit eder (Joshi,2025).", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 99, "page_end": 99}}
{"content": "88 Sonuç olarak, büyük dil modelleri tabanlı ajanlar, yalnızca doğal dil işleme alanında değil, aynı zamanda etkileşimli ve bağlama duyarlı karar destek sistemleri açısından da yeni bir bakış açısı sunmaktadır. Tek ve çok ajanlı sistemleri, dünya simülasyonundan eğitime, kod üretiminden finansal kararlara kadar pek çok farklı alanda başarılı performans göstermektedir. Bu sistemler; bellek yönetimi, rol uzmanlaşması, görev devri gibi yalnızca teknik başarıyı değil, aynı zamanda esneklik ve uyarlanabilirlik gibi nitelikleri de beraberinde getirmektedir. Önümüzdeki dönemde, bu ajanların gerçek zamanlı verilere erişerek daha karmaşık görevlerde kullanılması beklenmektedir. Böylece, büyük dil modelleri tabanlı ajanlar hem araştırma hem de uygulamalarda dönüştürücü bir rol oynayarak insan-ajan iş birliğinin daha da derinleşmesini sağlayacaktır. 6.2 Büyük Dil Modellerinde Etik Sorunlar Teknolojinin gelişmesiyle birlikte mevcut etik sınırlar genişlemiş ve yeni etik yaklaşımlara ihtiyaç duyulmuştur. Özellikle yapay zekânın her geçen gün gelişmesi ve kullanımının yaygınlaşması, hem bireysel haklar hem de toplumsal fayda açısından kritik bir önem taşımaktadır. Adalet, gizlilik, sorumluluk ve algoritmik tarafsızlık gibi kavramlar, yapay zekâ alanında en çok tartışılan etik konular arasında yer almaktadır (Turan,2024). Büyük dil modellerinin doğası, genellikle kara kutu (black-box) olarak bilinir. Bu modeller, milyarlarca parametre içeren ve doğrusal olmayan dönüşümlere sahip son derece karmaşık yapılardır. Bu karmaşıklık, belirli girdilerin belirli çıktılara nasıl dönüştüğünü takip etmeyi zorlaştırmakta ve şeffaflık eksikliğine yol açmaktadır. Modelin kararlarının arkasındaki gerekçelerin anlaşılması, adaletin, hesap verilebilirliğin ve güvenilirliğin sağlanması açısından çok önemlidir (Komera and Manche,2023). Büyük dil modelleri, geçmiş verilerle eğitildikleri için kullanılan veri setlerindeki hatalar ve önyargılar sistemin kararlarını olumsuz etkileyebilir. Bu durum, işe alım, kredi değerlendirme ve hukuki kararlar gibi alanlarda etik sorunlara neden olabilir. Örneğin, Amazon’un işe alım süreçlerinde kullandığı yapay zekâ tabanlı sistem, geçmiş verilerde erkek adayların daha fazla tercih edilmesinden dolayı, kadın adaylara karşı önyargılı kararlar üretmiştir. Benzer şekilde, ABD’ de sağlık alanında kullanılan bir modelin, siyahi hastalara yönelik ayrımcı kararlar verdiği ve onlara daha düşük öncelik tanıdığı gözlemlenmiştir (Can,2025;Maral,2024). Açıklanabilir Yapay Zekâ (Explainable AI- XAI) teknikleri, yapay zekâ modelinin verdiği sonuçların insanlar tarafından yorumlanmasını ve anlaşılmasını sağlayan teknikler ve yöntemler bütünüdür. Kullanıcıların, yapay zekâ uygulamalarını anlamaları, güvenmeleri ve yönetmeleri açısından açıklanabilirlik önemli bir kriter olarak", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 100, "page_end": 100}}
{"content": "89 görülmektedir. Açıklanabilirlik bir modelin işlevlerini ayrıntılı bir şekilde açıklamak için gerçekleştirilen eylemleri ifade eder. Açıklanabilir yapay zekâ ihtiyacı hedef ve uygulama alanına göre farklılık göstermektedir. Basit sınıflandırma görevlerinde (örneğin, bir görselin kedi mi yoksa köpek mi olduğu) açıklanabilirlik ikinci planda kalırken, sağlık,hukuk veya finans gibi alanlarda kullanılan modellerde açıklanabilirlik ihtiyacı oldukça yüksektir. Her ne kadar bu sistemler doğaları gereği bir kara kutu (black-box) niteliği taşısa da, bir çok yapay zekâ çalışması sistemin şeffaflığını ve açıklanabilirliğini ikinci plana atarak öncelikli olarak modelin performansını iyileştirmeye odaklanmaktadır (Polat,2024; Deliloğlu ve Pehlivanlı 2021). Büyük dil modellerinin eğitilmesi için internetten toplanan büyük miktarda veriye ihtiyaç vardır. Bu veriler; adresler, telefon numaraları, sohbet kayıtları gibi bazı kullanıcı gizliliği bilgilerini içerebilir. Modeller, ön eğitim verilerinden veya kullanıcı etkileşimlerinden hassas bilgileri ezberleyip üretebilir, bu durum kişisel bilgi sızıntısına yol açabilir. Ayrıca, model eğitim verilerindeki makaleler, kodlar gibi telif hakkı içeren içerikleri yeniden üretebilir. Bu tür verilerin yetkisiz şekilde kullanılması, veri üreticilerinin telif haklarını ihlal eder ve model geliştiricilerinin karşılaştığı hukuki riskleri artırır (Yi et al.2023). Büyük dil modelleriyle ilgili temel zorluklardan biri de verilen cevapların doğruluğudur. Güçlü ve etkileyici yeteneklerine rağmen, büyük dil modelleri zaman zaman halüsinasyon (hallucination) adı verilen yanıltıcı veya hatalı bilgiler üretebilir. Modeller, geniş veri kümesi üzerinde eğitildiği ve bazen güncellenmemiş yada yanlış bilgilerden etkilenmesi nedeniyle halüsinasyon üretebilmektedir. Bu durum, tıp ve hukuk gibi kritik alanlarda ciddi sonuçlara yol açabilir (Çetin ve Onan,2025). Büyük dil modelleri politik ve diplomatik süreçlerle birleştirildiğinde birtakım riskleri içermektedir. Bu modellerin, yanıltıcı bilgi kampanyalarında kullanılması veya demokratik ortamı bozabilme potansiyeli konusunda ciddi endişeler mevcuttur. Örneğin, kamuoyunu manipüle etmek ve otomatik mesajlarla iletişim sistemlerini yoğun bir şekilde meşgul etmek için kullanılabilirler. Bu teknolojiler öncesinde, karar alıcılar ile kamuoyu arasındaki iletişimin doğruluğu ve samimiyeti doğrudan demokrasiyle ilişkiliydi. Ancak günümüzde, büyük dil modellerinin inandırıcı metin üretme kapasitesi bu ilişkiyi zedeleme riski taşımaktadır. Bunun bir sonucu olarak siyasi figürlerin yapay zekâ tarafından üretilen içeriklerle ilişkilendirilmesi, gerçek kamuoyu ilişkilerinin kurulmasını güçleştirebilir ve halkla ilişkiler açısından ciddi tehditler doğurabilir (Akıllı ve Şimşek,2024). Büyük dil modellerinin etik etkileri üzerine önemli araştırmalar yapılmakta ve etik sorunlara çözümler aranmaktadır. Akademisyenler, potansiyel riskleri tanımlamayı, incelemeyi ve azaltmayı hedefleyerek, daha sorumlu yapay zekâ sistemlerinin", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 101, "page_end": 101}}
{"content": "90 geliştirilmesi için çaba göstermektedir. Bu çaba, büyük dil modellerinin faydalarını artırmayı ve zararlarını en aza indirmeyi amaçlamaktadır. Böylece, tasarlanan modeller, kamu yararını gözeterek etik ve etkili bir şekilde hizmet verir. Bu hedeflerin gerçekleştirilmesi, büyük ölçekte yüksek kaliteli veri setlerine bağlıdır. Ancak veri toplama süreci ; gizlilik, telif hakkı ve önyargı gibi etik sorunları da beraberinde getirebilir. Bu etik sorunlar uzun süredir var olan ve çözülmesi güç olan sorunlar olup, büyük dil modelleri geliştikçe yeni etik sorunlar da ortaya çıkmaktadır (Deng et al.,2024). Bu etik sorunlar, yapay zekâ sistemlerinde insan denetimine duyulan ihtiyacı da beraberinde getirmiştir. Bu sistemler, uygun bir denetim olmadan kullanıldığında zararlı önyargıları sürdürebilir, etik dışı kararlar alabilir ve bu teknolojilere duyulan güveni zedeleyebilir. İnsan müdahalesi olmadan karar verebilen bağımsız yapay zekâ sistemlerinin geliştirilmesi göz ardı edilemeyecek karmaşık etik sorunları da beraberinde getirmektedir. Bu sistemler genellikle çok az ya da hiç insan müdahalesi olmaksızın algoritma ve veri temelli görevleri yerine getirmek için tasarlanmıştır. Bu durum, verimliliği ve ölçeklenebilirliği artırsa bile sorumluluk, etik ve hesap verebilirlik gibi sorunları da gündeme getirir. Örneğin, sağlık alanında veri örüntülerine dayalı tedavi öneren yapay zekâ destekli bir sistem, doktorun sahip olduğu bireysel hasta ihtiyaçlarını ve etik hususları gözeten bir anlayıştan yoksundur. Benzer şekilde, otonom araçlarda yapay zekâ sistemleri tehlikeli durumlarda nasıl hareket edeceğine karar vermek zorundadır. Örneğin, kaçınılmaz bir kazada zararı en aza indirmek için nasıl bir yol izleyeceğini belirlemek zorundadır. Bu tür kararlar, genellikle bir çarpışmada kimin öncelikli olarak korunacağına karar vermek gibi ahlaki ikilemleri de beraberinde getirir (Johnson,2024). Avrupa Birliği, 2010’ların ortalarından itibaren dijitalleşme konusunda çeşitli girişimlerde bulunmaktadır. Bu bağlamda, Avrupa Birliği Yapay Zekâ Yasası (günümüzde Yapay Zekâ Tüzüğü) ile bu teknolojilere yönelik ilk kapsamlı yasal düzenlemeyi ortaya koymuştur. Yapay Zekâ Tüzüğü, 1 Ağustos 2024 tarihinde yürürlüğe girmiş olup 2 Ağustos 2026 tarihinden itibaren uygulanmaya başlayacağı belirtilmiştir. Avrupa Birliği Yapay Zekâ Tüzüğü, yapay zekâ sistemlerine ilişkin hukuki alanda gerçekleştirilen ilk girişimdir. Bu tüzük, Avrupa Birliği’nin dışındaki ülkeleri de etkilemektedir. Örneğin, Türkiye’de bu alanda çalışan girişimciler ürettikleri yazılımları Avrupa Birliği’ne ihraç etmek istediklerinde Yapay Zekâ Tüzüğü’ne uymak zorundadır (Turan Başara,2025; Bozkurt Yüksel,2022). Genel Veri Koruma Tüzüğü (General Data Protection Regulation-GDPR), 25 Mayıs 2018 tarihinde yürürlüğe girmiştir. Bu düzenleme, Avrupa Birliği ülkelerindeki vatandaşların veri gizliliğini ve mahremiyetini korumak amacıyla oluşturulmuştur. GDPR, kişisel verilerin işlenmesinde şeffaflık ve hukuka uygunluk ilkelerini temel", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 102, "page_end": 102}}
{"content": "91 almakla beraber verinin gizliliği, bütünlüğü ve kişisel verilerin ilişkili olduğu kişinin haklarının korunması gibi konularda kapsamlı yükümlülükler getirmektedir (Savaş vd.,2020). Verinin korunması için mahremiyet ve güvenlik kritik bir noktadır. Günümüzde, hayatların dijital alana büyük oranda yayılması ile bu ortamlarda paylaşılan ve işlenen verilerin korunması konusu oldukça önemli bir ihtiyaç haline gelmiştir. 2021-2025 dönemi T.C. Ulusal Yapay Zekâ Stratejisi, mahremiyetin yapay zekâ alanı için korunmasına dair temel ilkeler arasında yer almaktadır. Türkiye’deki yapay zekâ uygulamalarının temelini ise Kişisel Verilen Korunması Kanunu (KVKK), oluşturmaktadır.. KVKK, kişisel verilerin işlenmesi ve korunması için ayrıntılı yönetmelikler getirerek verilerin güvenli bir şekilde kullanılmasını sağlamaktadır (Özparlak ve Çetin, 2023 ; Topuzoğlu ve Çevik Tekin,2024). Sonuç olarak, büyük dil modelleri her geçen gün etkileyici bir şekilde gelişmeye devam etse de, bu gelişmelerin beraberinde getirdiği etik sorunlar görmezden gelinemeyecek kadar ciddidir. Yapay zekâ teknolojilerinin insan hayatına bu kadar dahil olduğu bu dönemde, şeffaflık, hesap verebilirlik ve insan hakları gibi temel ilkeler göz önünde bulundurulmalıdır. Teknolojik gelişmeler insan haklarına ve toplumsal değerlere zarar vermemelidir. Modellerin performansı kadar, insan yaşamı üzerindeki etkileri de dikkate alınmalıdır. Aksi halde, teknolojik gelişmeler, insani değerleri gölgede bırakır. Bu nedenle, etik sorunları gözetmeyen bir yapay zekâ yaklaşımı, faydadan çok zarar getirebilir. Özellikle açıklanabilir yapay zekâ yaklaşımları, modellerin karar alma süreçlerinin anlaşılması için çözüm sunmayı amaçlar. Bu alanda karşılaşılan sorunlar sadece teknik önlemlerle çözülemez; aynı zamanda yasal düzenlemeler, toplumsal farkındalık ve insan denetimi gibi unsurlar da sürece dahil edilmelidir.", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 103, "page_end": 103}}
{"content": "92 7.SONUÇ VE DEĞERLENDİRME Bu tez kapsamında incelenen araştırmalar göz önüne alındığında, basit makine öğrenimi yöntemleriyle veri kümelerindeki örüntüleri tanıma ve keşfetme ile başlayan yapay zekâ serüveninin günümüz dünyasında geldiği nokta son derece dikkat çekicidir. Başlangıçta sadece klasik makine öğrenmesi algoritmalarıyla sınırlı ve kurallara dayalı tahminlerde bulunan kural tabanlı modellere kıyasla, zamanla kendiliğinden öğrenebilen ve hatta karar verebilen modellerin geliştirilmesi devrim niteliğinde bir gelişme olmuştur. Özellikle, neredeyse her şeyin dijitalleştiği günümüz dünyasında bu teknolojilere duyulan ilgi ve ihtiyaç da her geçen gün artış göstermektedir. Tarih boyunca tekerleğin icadından beri ortaya çıkan her teknolojik gelişme, insan hayatını büyük ölçüde kolaylaştırsa da bireyleri zamanla bu sistemlere bağımlı hale getirmiştir. Günümüzde yapay zekâ teknolojileri de benzer bir davranış göstermektedir. Örneğin, günümüzde sosyal medya platformlarındaki kullanıcılar haber içeriklerini okumak yerine, içerikleri yorumlaması için platformda bulunan yapay zekâ modellerine başvurmaktadır. X (eski adıyla Twitter) platformundaki Grok adlı dil modeli, X kullanıcıları tarafından çoğunlukla haberlerin doğruluğunu sorgulamak için kullanılmaktadır. Bu durum yapay zekâ teknolojisinin insanların bilgiye erişme alışkanlıklarını nasıl değiştirdiğine bir örnektir. Benzer şekilde, eğitim ve sunum hazırlama gibi alanlarda da yapay zekâ araçlarının sunduğu hız ve kolaylık, kullanıcıları bu sistemlere bağımlı hale getirmektedir. Oysa bu kolaylık, zaman tasarrufu ve pratiklik sağlarken, bir yandan düşünme, analiz etme ve üretme gibi becerilerde gerileme riskini ortaya çıkarmaktadır. Bu noktada, önemli olan teknolojik gelişmelere karşı çıkmak değil; bu teknolojileri ne sıklıkta ve nasıl kullandığımızın farkında olmaktır. Yapay zekâ ve ona bağlı teknolojilerin gelişim hızı göz önüne alındığında, önümüzdeki birkaç yıl içinde bu alanlara iyi bir şekilde yatırım yapmayan ülkelerin hem teknolojik hem de ekonomik olarak geri kalacakları düşünülmektedir. Özellikle güncel gelişmelere bakıldığında bir yandan yeni bir araştırma konusu olan büyük dil modelleri tabanlı çok ajanlı sistemler göze çarparken, bir yandan da bu sistemler sonucu ortaya çıkan etik sorunlar, şeffaflık, hesap verebilirlik ve açıklanabilirlik ilkeleri oldukça dikkat çekmektedir. Türkçe akademik literatürde ise çoğunlukla büyük dil modellerinin yapısı, kullanımları ve etik sorunları ele alınmıştır. Ancak, büyük dil modelleri tabanlı çok ajanlı sistemlerle ilgili sınırlı sayıda çalışma bulunmaktadır. Bu alanda, özellikle son dönemde uluslararası literatürde yayımlanan çalışmalar düşünüldüğünde, Türkçe literatürde de ilerleyen yıllarda daha fazla sayıda araştırma yapılması beklenmektedir.", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 104, "page_end": 104}}
{"content": "93 KAYNAKLAR DİZİNİ Abro, A.A., Talpur, M.S.H. and Jumani, A.K., 2022, Natural Language Processing Challenges and Issues: A Literature Review, Gazi University Journal of Science, vol. 36, no. 4, pp. 1522-1536pp, DOI: 10.35378/gujs.1032517 Acheampong, F. A., Nunoo-Mensah, H. and Chen, W., 2020, Comparative Analyses of BERT, Roberta, Distilbert, and Xlnet for Text-Based Emotion Recognition (unpublished). Akıllı, E. ve Şimşek, M., 2024, Dijital diplomaside büyük dil modelleri: Fırsatlar ve riskler, İnsan & Toplum, 14(4), 1-23, DOI: 10.12658/M0744 Akın, E. ve Şahin, M. E., 2024, Derin Öğrenme ve Yapay Sinir Ağı Modelleri Üzerine Bir İnceleme. EMO Bilimsel Dergi, 14(1). Alagöz, O., and Uçkan, T., 2024, Text Clustering with Pre-Trained Models: BERT, RoBERTa, ALBERT and MPNet, Malatya Turgut Ozal University Journal of Engineering and Natural Sciences, 5(2), 37–46pp, DOI:10.46572/naturengs.1577517 Alaskar, H. and Saba, T., 2021, Machine Learning and Deep Learning: A Comparative Review, Proceedings of Integrated Intelligence Enable Networks and Computing, IIENC 2020, 143–150pp, DOI: 10.1007/978-981-33-6307-6_15 Alayrac, J.-B., Barr, I., Hasson, Y., Donahue, J., Lenc, K., Luc, P., Mensch, A., Miech, A., Millican, K., Reynolds, M., Ring, R., Rutherford, E., Cabi, S., Han, T., Gong, Z., Borgeaud, S., Binkowski, M., Samangooei, S., Brock, A., Barreira, R., Monteiro, M., Nematzadeh, A., Vinyals, O., Simonyan, K., Menick, J., Sharifzadeh, S. and Zisserman, A., 2022. Flamingo: a Visual Language Model for Few-Shot Learning, arXiv:2204.14198, https://arxiv.org/pdf/2204.14198v2 Anthropic., 2024, The Claude 3 Model Family: Opus, Sonnet, Haiku, https://assets.anthropic.com/m/61e7d27f8c8f5919/original/Claude-3-Model-Card.pdf (Accessed: 14.05.2025). Arısoy, A., 2024, Natural Language Processing Algorithms And Performance Comparison, Yalvaç Akademi Dergisi, 9(2), 106–121, DOI: 10.57120/yalvac.1536202 Arslan, A. T., 2023, Büyük Dil Modellerinin Türkçe Veri Setleri ile Eğitilmesi ve İnce Ayarlanması, arXiv:2306.03978, https://arxiv.org/abs/2306.03978", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 105, "page_end": 105}}
{"content": "94 KAYNAKLAR DİZİNİ (devam) Arslan, K., 2020, Eğitimde Yapay Zekâ ve Uygulamaları, Batı Anadolu Eğitim Bilimleri Dergisi, 11(1), 71-88. Arslankaya, S. ve Toprak, Ş., 2021, Makine öğrenmesi ve derin öğrenme algoritmalarını kullanarak hisse senedi fiyat tahmini. Uluslarası Mühendislik Araştırma ve Geliştirme Dergisi (UMAGD), 13(1), 178-192, DOI: 10.29137/umagd.771671 Arzu, M. ve Aydoğan, M., 2023, Türkçe Duygu Sınıflandırma İçin Transformers Tabanlı Mimarilerin Karşılaştırılmalı Analizi, Journal of Computer Science, 1-6, DOI: 10.53070/bbd.1350405 Aslan, S., 2023. Doğal Dil İşleme Teknikleri Kullanarak E-Ticaret Kullanıcı İncelemelerinde Özellik Tabanlı Duygu Analizi, Fırat Üniversitesi Mühendislik Bilimleri Dergisi, cilt 35, sayı 2, 875-882, DOI: 10.35234/fumbd.1335583 Asongo, A.I., Barma, M. ve Muazu, H.G., 2021, Machine Learning Techniques, methods and Algorithms: Conceptual and Practical Insights, International Journal of Engineering Research and Applications, 55-64pp, DOI: 10.9790/9622-1108025564 Ataseven, B., 2013, Yapay Sinir Ağları ile Öngörü Modellemesi, Öneri, Cilt 10, Sayı 39, 101–115. Ateş, E.C., 2021, Doğal Dil İşleme (Natural Language Processing), Siber Ansiklopedi: Siber Ortama Çok Disiplinli Bir Yaklaşım, 169-174, Pegem Akademi Yayıncılık. Atzeni, L. ,2021, Long-Term Temporal Attention in Efficient Human Action Recognition Architectures Master’s thesis, Politecnico di Torino, Master of Science in Data Science and Engineering, Department of Control and Computer Engineering. Aydın, Ş.E., 2017, Yapay Zekâların Dünü Bugünü Yarını, Çukurova Üniversitesi Sosyal Bilimler Enstitüsü, İşletme ve Teknoloji Yönetimi (UE), Yüksek Lisans Dönem Projesi, Adana (yayınlanmamış). Aytaş, G., 2025, Enhancing Translation with Visual and Auditory Modalities, Uluslararası Dil, Edebiyat ve Kültür Araştırmaları Dergisi (UDEKAD), 8(1), 425–438pp, DOI:10.37999/udekad.1611713 Balaji, P.G. and Srinivasan, D., 2010, An Introduction to Multi-Agent Systems, Innovations in Multi-Agent Systems and Applications – 1. SCI 310, 1–27pp.", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 106, "page_end": 106}}
{"content": "95 KAYNAKLAR DİZİNİ (devam) Bangi, A. F., 2023, Exploring the Role of Transformers in NLP: From BERT to GPT-3, International Research Journal of Engineering and Technology (IRJET), 10(06). Baskaran, S. H., 2023, A Comparison of Transformer and Autoregressive LLM Designs, International Journal of Research Publication and Reviews, 4(11), 19–26pp. Bayır, F., 2006,Yapay Sinir Ağları ve Tahmin Modellemesi Üzerine Bir Uygulama, Yüksek Lisans Tezi, İstanbul Üniversitesi, Sosyal Bilimler Enstitüsü. Bekker, M., 2024, Large language models and academic writing: Five tiers of engagement. South African Journal of Science, 120(1/2), Article #17147, DOI:10.17159/sajs.2024/17147 BigScience Workshop, 2023, BLOOM: A 176B-Parameter Open-Access Multilingual Language Model, arXiv:2211.05100v4, https://arxiv.org/abs/2211.05100v4 Boitel, E., Mohasseb, A., and Haig, E., 2024, A comparative analysis of GPT-3 and BERT Models for Text-based Emotion Recognition: Performance, Efficiency, and Robustness. Computational Intelligence Systems, 567–579pp, DOI:10.1007/978-3-031-47508-5_44 Bosley, M., Jacobs-Harukawa, M., Licht, H., & Hoyle, A., 2023, Do we still need BERT in the age of GPT? Comparing the benefits of domain-adaptation and in-context-learning approaches to using LLMs for Political Science Research (unpublished). Bozkurt Yüksel, A. E., 2022, Avrupa Komisyonu’nun Yapay Zekâ Tüzük Teklifi’ne Genel Bir Bakış, TAAD - Türkiye Adalet Akademisi Dergisi, 13(51), DOI:10.54049/taad.1093105 Brown, T. B., Kaplan, J., Mann, B., Dhariwal, P., Ryder, N., Neelakantan, A., Subbiah, M., Shyam, P., Askell, A., Child, R., Agarwal, S., Ramesh, A., Hesse, C., Chess, B., Chen, M., Herbert-Voss, A., Ziegler, D. M., Sigler, E., Clark, J., Krueger, G., Wu, J., Litwin, M., Sastry, G., Henighan, T., Winter, C., Gray, S., Berner, C., McCandlish, S., Radford, A., Sutskever, I. and Amodei, D., 2020, Language Models are Few-Shot Learners, arXiv:2005.14165, https://arxiv.org/abs/2005.14165 Bulut, S., 2024, Üretken Yapay Zeka Teknolojisi: GPT-4o, Uluslararası İleri Doğa Bilimleri ve Mühendislik Araştırmaları Dergisi, 8(4), 380-387.", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 107, "page_end": 107}}
{"content": "96 KAYNAKLAR DİZİNİ (devam) Caffagni, D., Cocchi, F., Barsellotti, L., Moratelli, N., Sarto, S., Baraldi, L., Cornia, M. and Cucchiara, R., 2024, The Revolution of Multimodal Large Language Models: A Survey, Findings of the Association for Computational Linguistics: ACL 2024, 13590– 13618 pp, 11–16 August 2024. Can, E., 2025, Bilimsel Yayınların Yapay Zekâ Programları Tarafından Değerlendirilmesine İlişkin Literatür İncelemesi, Kapanaltı Dergisi, 7, 101–114, DOI:10.62080/kmfed.1604323 Chahal, A. And Gulia, P., 2019, Machine Learning and Deep Learning, International Journal of Innovative Technology and Exploring Engineering (IJITEE), 8(12), DOI: 10.35940/ijitee.L3550.1081219 Chatzimina, M. E., Papadaki, H. A., Pontikoglou, C. and Tsiknakis, M., 2024, A Comparative Sentiment Analysis of Greek Clinical Conversations Using BERT, RoBERTa, GPT-2, and XLNet. Bioengineering, 11(6), 521, DOI: 10.3390/bioengineering11060521 Chauhan, N. K. and Singh, K., 2018, A Review on Conventional Machine Learning vs Deep Learning, International Conference on Computing, Power and Communication Technologies (GUCON), Galgotias University, Greater Noida, India. Chen, S., Liu, Y., Han, W., Zhang, W. and Liu, T., 2025, A Survey on LLM-based Multi- Agent System: Recent Advances and New Frontiers in Application, arXiv:2412.17481, https://arxiv.org/pdf/2412.17481v2 Chen, Y., Zhang, Y., Zhang, C., Lee, G., Cheng, R and Li, H., 2021, Revisiting Self- Training for Few-Shot Learning of Language Model, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP 2021), 9125–9135pp, Association for Computational Linguistics. Cheng, Y., Zhang, C., Zhang, Z., Meng, X., Hong, S., Li, W., Wang, Z., Wang, Z., Yin, F., Zhao, J. and He, X., 2024, Exploring Large Language Model Based Intelligent Agents: Definitions, Methods, and Prospects, arXiv:2401.03428. https://arxiv.org/pdf/2401.03428v1", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 108, "page_end": 108}}
{"content": "97 KAYNAKLAR DİZİNİ (devam) Chu, Z., Wang, S., Xie, J., Zhu, T., Yan, Y., Ye, J., Zhong, A., Hu, X., Liang, J., Yu, P. S. and Wen, Q., 2025, LLM Agents for Education: Advances and Applications, arXiv:2503.11733, https://arxiv.org/abs/2503.11733 Cortiz, D., 2021, Exploring Transformers in Emotion Recognition: A Comparison of BERT, DistillBERT, RoBERTa, XLNet and ELECTRA, arXiv:2104.02041, https://arxiv.org/pdf/2104.02041 Çapalı, V., 2022, Yapay Sinir Ağları ve Makine Öğrenme Yöntemlerinin Nükleer Fizik Uygulamaları, El-Cezeri Fen ve Mühendislik Dergisi, 9(4), 1240–1248. DOI:10.31202/ecjse.1132803 Çelik, Ş.., 2022, Yapay Sinir Ağlarının Farklı Aktivasyon Fonksiyonlarında Uygulaması: Türkiye’de Ördek Popülasyonu Öngörüsü, Osmaniye Korkut Ata Üniversitesi Fen Bilimleri Enstitüsü Dergisi, 5(2), 800–811. Çelik, Y., 2024, Bellek Tabanlı LSTM ve GRU Makine Öğrenmesi Algoritmaları Kullanarak BIST100 Endeks Tahmini, Fırat Üniversitesi Mühendislik Bilimleri Dergisi, 36(2), 553- 561, DOI: 10.35234/fumbd.1406688 Çetin, M. ve Beyhan, S., 2019, Yapay Zeka ve Tat Algısı: Makineler tat alabilir mi?, Koku ve Tat Algısı, İntertıp Yayınevi, 421-430. Çetin, M., Uğur, A ve Bayzan, Ş., 2006, İleri Beslemeli Yapay Sinir Ağlarında Backpropagation (Geriye Yayılım) Algoritmasının Sezgisel Yaklaşımı , 8. Akademik Bilişim Konferansı ,Pamukkale Üniversitesi, Denizli. Çetin, P. ve Onan, A., 2025, Büyük Dil Modellerinin Tıp Eğitimde Soru Yanıtlama Sistemlerinde Kullanımı: Potansiyel, Zorluklar ve Gelecek Yönelimleri, Journal of Innovations in Science, Mathematics and Computing Education, 1(1), 82-108. Dai, Z., Yang, Z., Yang, Y., Carbonell, J., LE, Q. V. and Salakhutdinov, R., 2019, Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2978–2988pp, Florence, Italy.", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 109, "page_end": 109}}
{"content": "98 KAYNAKLAR DİZİNİ (devam) Dayan, A., Yılmaz, A., 2022, Doğal dil işleme ve derin öğrenme algoritmaları ile makine dili modellemesi, Dicle University Journal of Engineering (DUJE), 13(3), 467-475, DOI:10.24012/dumf.1131565 Deliloğlu, R. A. S. ve Pehlivanlı, A. Ç., 2021, Hibrit Açıklanabilir Yapay Zeka Tasarımı ve LIME Uygulaması, Avrupa Bilim ve Teknoloji Dergisi, (27), 228–236, DOI:10.31590/ejosat.959030 Demir, M., 2024, Yapay Zekâ Kopyası (AI Cheating) ve Büyük Dil Modellerinin Çevrimiçi Sınavlarda Kullanımı, Iğdır Üniversitesi Sosyal Bilimler Dergisi, 37, 339–352, DOI:10.54600/igdirsosbilder.1498843 Demiröz, H. ve Tıkız-Ertürk, G., (2025). Akademik Yazmada İletişimsel Yapay Zekânın Kullanımına Dair Bir İnceleme, Eskiyeni, 56, 469-496, DOI: 10.37697/eskiyeni.1565854 Deng, C., Duan, Y., Jin, X., Chang, H., Tian, Y., Liu, H., Wang, Y., Gao, K., Zou, H.P., Jin, Y., Xiao, Y., Wu, S., Xie, Z., Lyu, W., He, S., Cheng, L., Wang, H. and Zhuang, J., 2024, Deconstructing the Ethics of Large Language Models from Long-standing Issues to New-emerging Dilemmas: A Survey, arXiv:2406.05392, https://arxiv.org/abs/2406.05392 Dick, S., 2019, Artificial Intelligence, Harvard Data Science Review, Vol 1, No 1, Summer 2019, The MIT Press, DOI: 10.1162/99608f92.92fe150c Dong, K., 2024, Large Language Model Applied in Multi-agent System—A Survey, Proceedings of the 2nd International Conference on Machine Learning and Automation, DOI: 10.54254/2755-2721/109/20241330 Duman, N., Yüksek, A. G., Caner, M. ve Buyruk, E., 2024, Yapay Sinir Ağları Yaklaşımı ile Toprak Kaynaklı Isı Pompasının Performans Analizi. Çukurova Üniversitesi Mühendislik Fakültesi Dergisi, 39(1), 57–72. Durfee, E.H. and Rosenschein, J.S., 1994, Distributed Problem Solving and Multi-Agent Systems: Comparisons and Examples.", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 110, "page_end": 110}}
{"content": "99 KAYNAKLAR DİZİNİ (devam) EITCA, 2024, “How does the LSTM architecture address the challenge of capturing long- distance dependencies in language?” , EITCA Academy, https://eitca.org/artificial- intelligence/eitc-ai-tff-tensorflow-fundamentals/natural-language-processing-with- tensorflow/long-short-term-memory-for-nlp/examination-review-long-short-term- memory-for-nlp/how-does-the-lstm-architecture-address-the-challenge-of-capturing- long-distance-dependencies-in-language/(Accessed: 03.05.2025). Ein-Dor, L., Shnayderman, I., Spector, A., Dankin, L., Aharonov, R. and Slonim, N., 2022, Fortunately, Discourse Markers Can Enhance Language Models for Sentiment Analysis, arXiv:2201.02026v2, https://arxiv.org/pdf/2201.02026v2.pdf Erbir, T. M., and Sevli, O., 2023, Arama Motorlarında Yapay Zekâ Teknolojilerinin Kullanımı, 10. Uluslararası Mardin Artuklu Bilimsel Araştırmalar Kongresi, Mardin, Türkiye. Escarda, M., Eiras-Franco, C., Cancela, B., Guijarro-Berdiñas, B. and Alonso-Betanzos, A., 2025, Performance and Sustainability of BERT Derivatives in Dyadic Data, Expert Systems With Applications, DOI: 10.1016/j.eswa.2024.125647 Fahim, A. K., Azad, S. B., Rashid, T., Albaidaei, E. A. A. and Ighzali, H., 2024, Harnessing Large Language Models For Transformative Applications İn Natural Language Processing, European Journal of Theoretical and Applied Sciences, 2(6), 428-438pp, DOI: 10.59324/ejtas.2024.2(6).37 Galanis, N.-I., Vafiadis, P., Mirzaev, K.-G. and Papakostas, G.A., 2021, Machine Learning Meets Natural Language Processing - The Story So Far, arXiv:2104.10213, https://arxiv.org/abs/2104.10213 Gazaz, D. C. ve Ayvaz, S., 2024, Türkiye’de ChatGPT algısı: Bert modeli ile duygu analizi. Balıkesir Üniversitesi Sosyal Bilimler Enstitüsü Dergisi (BAUNSOBED), 27(52). DOI:10.31795/baunsobed.1470846 Geçici, E., 2024, Yapay Zekâ Destekli ChatGPT’nin Muhasebe Eğitimi Alanına Uygunluğu: Fırsatlar ve Zorluklar, İşletme Akademisi Dergisi, 5(2), 96-117. DOI:10.26677/TR1010.2024.1420 Gemini Team, Google, 2025, Gemini: A Family of Highly Capable Multimodal Models, arXiv:2312.11805, https://arxiv.org/pdf/2312.11805v5", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 111, "page_end": 111}}
{"content": "100 KAYNAKLAR DİZİNİ (devam) Gillioz, A., Casas, J., Mugellini, E. and Abou Khaled, O., 2020, Overview of the Transformer-based Models for NLP Tasks, Proceedings of the Federated Conference on Computer Science and Information Systems (FedCSIS), 21, 179–183pp, DOI:10.15439/2020F20 Gong, K., Feng, K., Li, B., Wang, Y., Cheng, M., Yang, S., Han, J., Wang, B., Bai, Y., Yang, Z. and Yue, X., 2024, AV-Odyssey Bench: Can Your Multimodal LLMs Really Understand Audio-Visual Information?, arXiv:2412.02611, https://arxiv.org/pdf/2412.02611v1 Gonzalez-Gomez, L. J., Hernandez-Munoz, S. M., Borja, A., Azofeifa, J. D., Noguez, J., and Caratozzolo, P., 2023, Analyzing Natural Language Processing Techniques to Extract Meaningful Information on Skills Acquisition From Textual Content, IEEE Access, DOI: 10.1109/ACCESS.2024.3465409 Gonzalez-Gomez, L. J., Hernandez-Munoz, S. M., Borja, A., Azofeifa, J. D., Noguez, J. and Caratozzolo, P., 2024, Analyzing Natural Language Processing Techniques To Extract Meaningful İnformation On Skills Acquisition From Textual Content, IEEE Access, DOI: 10.1109/ACCESS.2024.0429000 Google Research and Google DeepMind, 2024, Advancing Multimodal Medical Capabilities of Gemini, arXiv:2405.03162v1, https://arxiv.org/pdf/2405.03162v1.pdf Grossman, G., 2020, “We’re entering the AI twilight zone between narrow and general AI”, VentureBeat, https://venturebeat.com/ai/were-entering-the-ai-twilight-zone-between- narrow-and-general-ai/ (Acessed : 01.03.2025). Guo, T., Chen, X., Wang, Y., Chang, R., Pei, S., Chawla, N. V., Wiest, O. and Zhang, 2024, Large Language Model based Multi-Agents: A Survey of Progress and Challenges, arXiv:2402.01680, https://arxiv.org/abs/2402.01680 Güler, A. ve Akgül, İ., 2022, A Review on the Science of Natural Language Processing, 3rd International Azerbaijan Congress on Life, Engineering, and Applied Sciences.", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 112, "page_end": 112}}
{"content": "101 KAYNAKLAR DİZİNİ (devam) Güneş, Y.C. and Ülkir, M., 2024, Comparative Performance Evaluation of Multimodal Large Language Models, Radiologist, and Anatomist İn Visual Neuroanatomy Questions, Journal of Uludağ University Medical Faculty, 50(3), 551–556pp, DOI:10.32708/uutfd.1568479 Han, S., Zhang, Q., Yao, Y., Jin, W. and Xu, Z., 2025, LLM Multi-Agent Systems: Challenges and Open Problems, arXiv:2402.03578, https://arxiv.org/pdf/2402.03578v2 Hassan, H., 2023, Brief History of Natural Language Processor (NLP), ResearchGate, https://www.researchgate.net/publication/373710671_Brief_History_of_Natural_Lang uage_Processor_NLP ( Accessed: 04.03.2025). Hatipoğlu, A. ve Bilgin, T.T., 2024, Doğal Dil Metinlerinden Programlama Dili Kodu Oluşturma Çalışmaları: Bir Derleme Çalışması, İstanbul Ticaret Üniversitesi Fen Bilimleri Dergisi , cilt 23, sayı 45, 209-244, DOI:10.55071/ticaretfbd.1354040 Hatipoğlu, A., Güneri, Y. ve Yılmaz, E., 2024, A comparative predictive maintenance application based on machine and deep learning, Journal of the Faculty of Engineering and Architecture of Gazi University, 39(2), 1037–1048 Hè, H. and Kabic, M., 2023. A Unified View of Long-Sequence Models towards Modeling Million-Scale Dependencies, arXiv:2302.06218v3, https://arxiv.org/abs/2302.06218 He, J., Treude, C., Lo, D., 2024, LLM-Based Multi-Agent Systems for Software Engineering: Literature Review, Vision and the Road Ahead, arXiv:2404.04834, https://arxiv.org/abs/2404.04834 Hey, T., Butler, K., Jackson, S. and Thiyagalingam, J., 2020, Machine learning and big scientific data, Phil.Trans.R.Soc.A , DOI : 10.1098/rsta.2019.0054 Huang, S., Dong, L., Wang, W., Hao, Y., Singhal, S., Ma, S., Lv, T., Cui, L., Mohammed, O.K., Patra, B., Liu, Q., Aggarwal, K., Chi, Z., Bjorck, J., Chaudhary, V., Som, S., Song, X. and Wei, F., 2023, Language Is Not All You Need: Aligning Perception with Language Models, arXiv:2302.14045, https://arxiv.org/pdf/2302.14045v2", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 113, "page_end": 113}}
{"content": "102 KAYNAKLAR DİZİNİ (devam) Hussain, N. Y., 2024, Deep Learning Architectures Enabling Sophisticated Feature Extraction and Representation for Complex Data Analysis, International Journal of Innovative Science and Research Technology, 9(10), 2290-2300pp, DOI: 10.38124/ijisrt/IJISRT24OCT1521 Işık, M., 2024, Büyük Dil Modellerinin Mühendislikteki Rolü, Bölüm 7, Mühendislik Bilimlerinde Güncel Araştırmalar ve Disiplinlerarası Uygulamalar, İKSAD Publishing House, ss.179–196, DOI: 10.5281/zenodo.14252011 Janiesch, C., Zschech, P. and Heinrich, K., 2021, Machine learning and deep learning, Electronic Markets , DOI: 10.1007/s12525-021-00475-2. Jeong, C., 2025, Beyond Text: Implementing Multimodal Large Language Model-Powered Multi-Agent Systems Using a No-Code Platform, Journal of Intelligence and Information Systems, 31(1), 191–213pp , DOI: 10.13088/jiis.2025.31.1.191 Jiang, B., Xie, Y., Wang, X., Yuan, Y., Hao, Z., Bai, X., Su, W.J., Taylor, C.J. and Mallick, T., 2025, Towards Rationality in Language and Multimodal Agents: A Survey, arXiv:2406.00252, https://arxiv.org/pdf/2406.00252 Jiao, X., Yin, Y., Shang, L., Jiang, X., Chen, X., Li, L., Wang, F. and Liu, Q., 2020, TinyBERT: Distilling BERT for Natural Language Understanding. arXiv:1909.10351v5 , https://arxiv.org/abs/1909.10351 Jimenez-Romero, C., Blum, C. And Yegenoglu, A., 2025, Multi-Agent Systems Powered by Large Language Models: Applications in Swarm Intelligence, arXiv:2503.03800, https://arxiv.org/abs/2503.03800 Johnson, M. and Hoover, R., 2024, The Role of Human Oversight in Responsible AI Systems. Johri, P., Khatri, S. K., Al-Taani, A. T., Sabharwal, M., Suvanov, S. and Kumar, A. ,2021, Natural Language Processing: History, Evolution, Application, and Future Work, Proceedings of 3rd International Conference on Computing Informatics and Networks, 365–375pp, DOI: 10.1007/978-981-15-9712-1_31 Jones, C. R. ve Bergen, B. K. ,2025, Large Language Models Pass the Turing Test, arXiv:2503.23674, https://arxiv.org/abs/2503.23674", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 114, "page_end": 114}}
{"content": "103 KAYNAKLAR DİZİNİ (devam) Joshi, S., 2025, Advancing Innovation in Financial Stability: A Comprehensive Review of AI Agent Frameworks, Challenges and Applications, World Journal of Advanced Engineering Technology and Sciences, 14(2), 117–126pp, DOI:10.30574/wjaets.2025.14.2.0071 Jozefowicz, R., Vinyals, O., Schuster, M., Shazeer, N., and Wu, Y., 2016, Exploring the Limits of Language Modeling, arXiv:1602.02410, https://arxiv.org/abs/1602.02410 Juluru, K., Shih, H.H., Murthy, K.N.K. and Elnajjar, P., 2021. Bag-of-Words Technique in Natural Language Processing: A Primer for Radiologists. RadioGraphics, vol 41, 1420–1426pp, DOI: 10.1148/rg.2021210025 Kang, J.-W. and Choi, S.-Y., 2025, Comparative Investigation of GPT and FinBERT’s Sentiment Analysis Performance in News Across Different Sectors, Electronics, 14(6), 1090, DOI: 10.3390/electronics14061090 Kapoor, R. and Dimple, 2022, Machine Learning Techniques, AGPH Books (Academic Guru Publishing House), Bhopal, India. Karaca, A. ve Aydın, Ö., 2024, Transformatör Mimarisi Tabanlı Derin Öğrenme Yöntemi ile Türkçe Haber Metinlerine Başlık Üretme, Gazi Üniversitesi Mühendislik ve Mimarlık Fakültesi Dergisi, 39(1), 485–495. Karaca, M.F. ve Bayır, Ş., 2024, Türkçe doğal dil işleme: ses bilgisi ve morfolojik analiz, Journal of Innovative Engineering and Natural Sciences, cilt 4, sayı 2, 448-465, DOI:http://doi.org/10.61112/jiens.1472513 Karaoğlan, K. M., 2022, Özellik Tabanlı Görüş Madenciliğinde Yapay Zeka Teknikleri Kullanarak Görüş Hedefi Çıkarımı ve Kategori Tespiti, Doktora tezi, Karabük Üniversitesi, Karabük. Katırcı, R. ve Çelik, H., 2024, Transformer Mimarisi, Bilgisayar Bilimleri ve Mühendisliği, 73-94, DOI:10.5281/zenodo.13971609 Katırcı, R. ve Çelik, H., 2024, Transformer Mimarisinde Dropout Oranlarının Performans Üzerindeki Etkisi, 1. International Transylvania Scientific Researches and Innovation Congress, Romania.", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 115, "page_end": 115}}
{"content": "104 KAYNAKLAR DİZİNİ (devam) Kayaalp, K. ve Süzen, A. A., 2018, Derin Öğrenme ve Türkiye’deki Uygulamaları, IKSAD Yayınevi. Kaynak Balta, B., 2020, Yapay Zekâ Ürünlerinin Hukuki Niteliği ve Fikrî Eser Kavramı, Ankara Hacı Bayram Veli Üniversitesi Hukuk Fakültesi Dergisi, Cilt XXIV, Sayı 3, DOI: 10.34246/ahbvuhfd.775498 Keleşoğlu, Ö. ,2006, Yapay Sinir Ağları ile Betonarme Kiriş Kesitlerin Analizi, İMO Teknik Dergi, (Yazı 260), 3935–3942. Khadake, V., 2024. Understanding Natural Language Processing (NLP) Techniques, International Journal of Computer Engineering and Technology (IJCET), vol. 15, no 4, 527-536pp. DOI: 10.5281/zenodo.13311223 Khasanov, D., Tojiyev, M., and Primqulov, O., 2021, Gradient Descent In Machine Learning, International Conference on Information Science and Communications Technologies (ICISCT), DOI:10.1109/ICISCT52966.2021.9670169 Khurana, D., Koli, A., Khatter, K. and Singh, S., 2022, Natural language processing: state of the art, current trends and challenges, Multimedia Tools and Applications, vol 82, 3713–3744pp, DOI: 10.1007/s11042-022-13428-4 Kızılkaya, Y. M. ve Oğuzlar, A., 2018, Bazı Denetimli Öğrenme Algoritmalarının R Programlama Dili ile Kıyaslaması, Karadeniz Uluslararası Bilimsel Dergi (37),90-98, DOI: 10.17498/kdeniz.405746 Klein, T. and Nabi, M., 2019, Learning to Answer by Learning to Ask: Getting the Best of GPT-2 and BERT Worlds, SAP Machine Learning Research, arXiv:1911.02365, https://arxiv.org/abs/1911.02365 Koç, S. ve Sevli, O., 2024, Mühendislikte Bilgisayar Yazılımı Uygulamaları, Doğal Dil İşleme: Tarihi, Evrimi, Uygulamaları ve Gelecek Perspektifi, Bölüm V, BİDGE Yayınları, 1. Baskı. Koçak, Ç. ve Yiğit, T., 2023, Determining Cyberbullying Status of Turkish Tweets with Gpt- 3 Classification Model, Gazi Journal of Engineering Sciences, 9(4), 278–285, DOI:10.30855/gmbd.0705", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 116, "page_end": 116}}
{"content": "105 KAYNAKLAR DİZİNİ (devam) Komera, O. and Manche, R, 2023, Black-Box Behavior in Large Language Models: Challenges and Implications, Proceedings of the 2023 International Conference of Renewable Energy, Green Computing and Sustainable Development CVR College of Engineering, Hyderabad, India. Kosaraju, D., 2023, AI and Multi-Agent Systems: Collaboration and Competition in Autonomous Environments, International Journal of Research and Review, 10(12), DOI: 10.52403/ijrr.20231288 Köse, B., Radıf, H., Uyar, B., Baysal, İ. ve Demirci, N., 2023, Öğretmen Görüşlerine Göre Eğitimde Yapay Zekanın Önemi. Journal of Social, Humanities and Administrative Sciences, 9(71), 4203-4209, DOI: 10.29228/JOSHAS.74125 Küçük, D. ve Arıcı, N., 2018, Doğal Dil İşlemde Derin Öğrenme Uygulamaları Üzerine Bir Literatür Çalışması, Uluslararası Yönetim Bilişim Sistemleri ve Bilgisayar Bilimleri Dergisi, 2(2), 76-86. Küçük, D. ve Can, F., 2024, Hukuki Metinlerin Otomatik İşlenmesinde Yapay Zekâ Teknolojilerinin Kullanımı , Bilişim Hukuku Dergisi, 6(1), 1–23. Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P.,and Soricut, R., 2020, ALBERT: A Lite BERT for Self-Supervised Learning of Language Representations, 8th International Conference on Learning Representations (ICLR 2020), https://arxiv.org/abs/1909.11942 LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P., 1998, Gradient-Based Learning Applied to Document Recognition, Proceedings of the IEEE, 86(11), 2278–2324pp, DOI:10.1109/5.726791 Li, X., Wang, S., Zeng, S., Wu, Y. and Yang, Y. , 2024, A survey on LLM-Based Multi- Agent Systems: Workflow, Infrastructure, and Challenges, Vicinagearth, 1(1), DOI:10.1007/s44336-024-00009-2 Liang, C.X., Yua, Y., Wang, T., Tian, P., An-Hou, W., Bi, Z., Yin, C.H., Ming, L. and Liu, M., 2024, A Comprehensive Survey and Guide to Multimodal Large Language Models in Vision-Language Tasks, arXiv:2411.06284v2, https://arxiv.org/pdf/2411.06284", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 117, "page_end": 117}}
{"content": "106 KAYNAKLAR DİZİNİ (devam) Liu, Y., He, H., Han, T., Zhang, X., Liu, M., Tian, J., Zhang, Y., Wang, J., Gao, X., Zhong, T., Pan, Y., Xue, S., Wu, Z., Li, Z., Zhang, X., Zhang, S., Hu, X., Zhang, T., Qiang, N., Li, T. and Ge, B., 2024, Understanding LLMs: A Comprehensive Overview from Training to Inference, arXiv:2401.02038v2, https://arxiv.org/abs/2401.02038 Ma, L., and Zhang, Y., 2015, Using Word2Vec to Process Big Text Data, In Proceedings of IEEE International Conference on Big Data, Georgia State University, Computer Science Department, DOI: 10.1109/BigData.2015.7364114 Manning, C.D., Raghavan, P. and Schütze, H., 2009, An Introduction to Information Retrieval, Cambridge University Press (http://www.informationretrieval.org/). Maral, T. ,2024, Sosyal Bilimlerin Kesişim Noktası: Yapay Zekâ ve Etik, AUSBD – “Yapay Zekâ ve Sosyal Bilimler Öğretim” Özel Sayısı, 17-33. Marreddy, M., Agarwal, R., Oota, S. R. and Mamidi, R., 2019, Evaluating the Combination of Word Embeddings with Mixture of Experts and Cascading gcForest in Identifying Sentiment Polarity, WISDOM’19, Proceedings of the 8th Workshop on Issues of Sentiment Discovery and Opinion Mining, Anchorage, Alaska. Meeradevi, Sowmya, B. J., and Swetha, B. N., 2024, Evaluating the machine learning models based on natural language processing tasks, IAES International Journal of Artificial Intelligence (IJ-AI), vol 13, no 12, 1954-1968pp, DOI : 10.11591/ijai.v13.i2.pp1954-1968 Mirtaheri, S. L., Pugliese, A., Movahed, N., and Shahbazian, R., 2025, A Comparative Analysis on Using GPT and BERT for Automated Vulnerability Scoring, Intelligent Systems with Applications, 26, 200515, DOI: 10.1016/j.iswa.2025.200515 Mohammadi, M., Mundra, R., Socher, R., Wang, L. and Kamath, A ,2019, CS224n: Natural Language Processing with Deep Learning – Lecture Notes: Part V: Language Models, RNN, GRU and LSTM. Unpublished course notes, Stanford University. Morales, E. F. and Escalante, H. J., 2022, A Brief Introduction to Supervised, Unsupervised, and Reinforcement Learning, Biosignal Processing and Classification Using Computational Learning and Intelligence ,111–129pp, DOI:10.1016/B978-0-12- 820125-1.00017-8", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 118, "page_end": 118}}
{"content": "107 KAYNAKLAR DİZİNİ (devam) Mortezapour Shiria, F., Perumal, T., Mustapha, N. and Mohamed, R., A Comprehensive Overview and Comparative Analysis on Deep Learning Models, Faculty of Computer Science and Information Technology, University Putra Malaysia (UPM), Serdang, Malaysia. Mukerjee, J., 2025, Understanding BERT: From theory to code, AI Researcher and Educator, Academia,https://www.academia.edu/128815738/Understanding_BERT_From_Theory _to_Code (Accessed: 08.05.2025). Murad, I. A., Khaleel, M. I. and Shakor, M. Y., 2024 , Unveiling GPT-4o: Enhanced Multimodal Capabilities and Comparative Insights with ChatGPT-4, International Journal of Electronics and Communications System, 4(2), 127-136pp, DOI: 10.24042/ijecs.v4i2.25079 Naeem, M., Rizvi, S. T. H., and Coronato, A., 2020, A Gentle Introduction to Reinforcement Learning and Its Application in Different Fields, IEEE Access, 8, DOI:10.1109/ACCESS.2020.3038605 Naveed, H., Khan, A. U., Qiu, S., Saqib, M., Anwar, S., Usman, M., Akhtar, N., Barnes, N. and Mian, A., 2024, A Comprehensive Overview of Large Language Models, arXiv:2307.06435, https://arxiv.org/abs/2307.06435 Nie, J., Yuan, Y., Li, Y., Wang, H., Li, J., Wang, Y., Song, K. and Ercisli, S., 2024. Few- shot Learning in Intelligent Agriculture: A Review of Methods and Applications. Journal of Agricultural Sciences (Tarım Bilimleri Dergisi), 30(2), 216-228pp, DOI:10.15832/ankutbd.1339516 Offerijns, J., Verberne, S., and Verhoef, T., 2020, Better Distractions: Transformer-based Distractor Generation and Multiple Choice Question Filtering, arXiv:2010.09598, https://arxiv.org/pdf/2010.09598.pdf Okuyucu Ergün, G., 2023, Machina Sapiens, Ankara Üniversitesi Hukuk Fakültesi Dergisi, Cilt 72, Sayı 2, 717–758. OpenAI Help Center, “GPT-4.5 in ChatGPT”, https://help.openai.com/en/articles/10658365- gpt-4-5-in-chatgpt (Acessed :01.03.2025 ). OpenAI, 2023, GPT-4 Technical Report, arXiv:2303.08774, https://arxiv.org/abs/2303.08774", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 119, "page_end": 119}}
{"content": "108 KAYNAKLAR DİZİNİ (devam) OpenAI, 2024, “Introducing GPT-4.5”, https://openai.com/index/introducing-gpt-4-5/ (Erişim Tarihi : 0905.2025). Özdağ, M. E., 2019, Derin Öğrenme Teknikleri Kullanılarak Anayol Trafik Analizi, Yüksek Lisans Tezi, Karabük Üniversitesi, Karabük. Özdemir, Ö. T. ve Güven, Y., 2025, ChatGPT’nin Diş Hekimliğinde Kullanım Alanları ve Sınırlamaları, Selçuk Dental Journal, 12(1), 184-190. Özparlak, B. O. ve Çetin, M., 2023, ChatGPT ve Üretici Yapay Zekâ Modellerinde Mahremiyet ve Güvenliğin Hukuki Boyutu, Marmara Üniversitesi Hukuk Fakültesi Hukuk Araştırmaları Dergisi, 29(2, Özel Sayı), 1003–1040, DOI:10.33433/maruhad.347497 Öztürk, K. ve Şahin, M.E., 2018, Yapay Sinir Ağları ve Yapay Zekâ’ya Genel Bir Bakış. Takvim-i Vekayi, 6(2), 25–36. Pal, S., 2023, The Future of Large Language Models: A Futuristic Dissection on AI and Human Interaction, International Journal for Multidisciplinary Research (IJFMR). Patil, D., Rane, N. L., Desai, P., and Rane, J., 2023, Machine learning and deep learning: Methods, techniques, applications, challenges, and future research opportunities, Deep Science Publishing (Chapter 2), DOI : 10.70593/978-81-981367-4-9_2 Perera, P. and Lankathilake, M., 2023, Preparing to Revolutionize Education with The Multi-Model GenAI Tool Google Gemini? A Journey towards Effective Policy Making. Journal of Advances in Education and Philosophy, 7(8), 246-253pp, DOI:10.36348/jaep.2023.v07i08.001 Polat, H. ve Körpe, M., 2018, TBMM Genel Kurul Tutanaklarından Yakın Anlamlı Kavramların Çıkarılması, Araştırma Makalesi, Bilişim Teknolojileri Dergisi, Cilt 11, Sayı 3, DOI: 10.17671/gazibtd.402468 Polat, M., 2024, Yapay Zekanın Denetimde Kullanılması Ve Etik Sorunlar, Sayıştay Dergisi, 35(134), Derleme.", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 120, "page_end": 120}}
{"content": "109 KAYNAKLAR DİZİNİ (devam) Qader, W. A., Ameen, M. M., and Ahmed, B. I., 2019, An overview of Bag of Words: Importance, Implementation, Applications, and Challenges, In Proceedings of the Fifth International Engineering Conference on Developments in Civil & Computer Engineering Applications 2019 (IEC2019). Qamar, R., and Zardari, B. A., 2023, Artificial Neural Networks: An Overview, Mesopotamian Journal of Computer Science, 130-139pp. Ravanelli, M., Brakel, P., Omologo, M. and Bengio, Y., 2017, Improving Speech Recognition by Revising Gated Recurrent Units, arXiv:1710.00641. https://arxiv.org/abs/1710.00641 Ravanelli, M., Brakel, P., Omologo, M. and Bengio, Y., 2018, Light gated recurrent units for speech recognition. IEEE Transactions on Emerging Topics in Computational Intelligence, 2(2), 92–102, DOI: 10.1109/TETCI.2017.2762739 Sakarya, Ş. ve Yılmaz, Ü., 2019, Derin Öğrenme Mimarisi Kullanarak BIST30 İndeksinin Tahmini, European Journal of Educational and Social Sciences Dergisi, 4(2), 106–121. Salıcı, M. ve Ölçer, E., 2024, Transformer Tabanlı Modellerin NLP'deki Etkisi: BERT ve GPT Üzerine Derinlemesine İnceleme , 8th International Artificial Intelligence and Data Processing Symposium (IDAP'24), Malatya, Türkiye, 21-22 Eylül 2024. IEEE. DOI:10.1109/IDAP64064.2024.10710796 Sanh, V., Debut, L., Chaumond, J. and Wolf, T., 2020, DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter, arXiv:1910.01108. https://arxiv.org/abs/1910.01108 Sarı, F., 2021, Cahit Arf’ın “Makine Düşünebilir mi ve Nasıl Düşünebilir?” Adlı Makalesi Üzerine Bir Çalışma, TRT Akademi, Cilt 6, Sayı 13, Eylül 2021, 812-833. Savaş, R. N., Zaim, A. H. ve Aydın, M. A, 2020, KVKK ve GDPR kapsamında firmaların mevcut durum analizi üzerine bir inceleme, İstanbul Ticaret Üniversitesi Fen Bilimleri Dergisi, 19(38), 208-223. Schroeder de Witt, C., 2025, Open Challenges in Multi-Agent Security: Towards Secure Systems of Interacting AI Agents, arXiv:2505.02077, https://arxiv.org/pdf/2505.02077", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 121, "page_end": 121}}
{"content": "110 KAYNAKLAR DİZİNİ (devam) Sel, İ. ve Hanbay, D., 2021, Ön Eğitimli Dil Modelleri Kullanarak Türkçe Tweetlerden Cinsiyet Tespiti, Fırat Üniversitesi Mühendislik Bilimleri Dergisi, 33(2), 675–684, DOI:10.35234/fumbd.929133 Serrano, S., Brumbaugh, Z. ve Smith, N. A., 2023, Language models: A guide for the perplexed, arXiv:2311.17301, https://arxiv.org/abs/2311.17301 Sharkey, E., Treleaven, P., n.d., Comparison of BERT vs GPT: Although GPT receives all the publicity, is BERT the new XGBOOST?, arXiv:2405.12990, https://arxiv.org/pdf/2405.12990 Shewalkar, A. N., 2018, Comparison Of Rnn, Lstm And Gru On Speech Recognition Data Master’s thesis, North Dakota State University of Agriculture and Applied Science. Şeker, A., Diri, B. ve Balık, H.H., 2017, Derin Öğrenme Yöntemleri ve Uygulamaları Hakkında Bir İnceleme, Gazi Mühendislik Bilimleri Dergisi, 3(3), 47–64. Şen, E., 2021, GPT3: DALL-E ve JL2P Ekseninde Veri Görselleştirme ve Hareketlendirme Üzerine Bir İnceleme, USBAD Uluslararası Sosyal Bilimler Akademi Dergisi, 3(5), 253-280, DOI: 10.47994/usbad.871726 Tan, Z. ve Karaköse, M., 2022, Dinamik Ortamlarda Derin Takviyeli Öğrenme Tabanlı Otonom Yol Planlama Yaklaşımları İçin Karşılaştırmalı Analiz, Adıyaman Üniversitesi Mühendislik Bilimleri Dergisi, 16, 248-262 DOI: 10.54365/adyumbd.1025545 Taşkıran, S. F. , 2021, Doğal dil işleme ile akademik metinlerin kümelenmesi ,Yüksek lisans Tezi, Konya Teknik Üniversitesi, Lisansüstü Eğitim Enstitüsü, Konya. Tay, Y., Dehghani, M., Bahri, D. and Metzler, D., 2022, Efficient Transformers: A Survey, Google Research, arXiv:2009.06732v3, https://arxiv.org/abs/2009.06732 Tekin, B. ve Gurbanov, R., 2023, Alphafold: Derin Öğrenme ve Sinir Ağları Yoluyla Protein Katlamasında Devrim Yaratmak, İstanbul Ticaret Üniversitesi Fen Bilimleri Dergisi, 22(44), 445-466 , DOI: 10.55071/ticaretfbd.1323165 Temur, S., 2024, Yapay Zekâ Kategorizasyonu ve Tarihsel Gelişim Süreci, EJONS 17th International Congress \"Artificial Intelligence and Society: Theory to Practice\", Necmettin Erbakan Üniversitesi, Konya, 21-22 Ağustos 2024.", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 122, "page_end": 122}}
{"content": "111 KAYNAKLAR DİZİNİ (devam) Topuzoğlu, T. ve Çevik Tekin, İ., 2024, Türkiye Finans Sektöründe Yapay Zekâ Etiği ve Veri Etiği, Fırat Üniversitesi Uluslararası İktisadi ve İdari Bilimler Dergisi, 8(2), DOI: 10.61524/fuuiibfdergi.1526411 Tran, K.-T., Dao, D., Nguyen, M.-D., Pham, Q.-V., O’Sullivan, B. and Nguyen, H. D., 2025, Multi-Agent Collaboration Mechanisms: A Survey of LLMs. arXiv:2501.06322, https://arxiv.org/abs/2501.06322 Tuncel, İ., Albayrak, A. ve Akın, M., 2022, Öz Dikkat Mekanizması Tabanlı Görü Dönüştürücü Kullanılarak Sıtma Parazit Tespiti , Dicle University Journal of Engineering(DUJE), 13(2), 271–277, DOI: 10.24012/dumf.1120289 Turan Başara, G.,2024, Avrupa Dijital Düzenleme Sisteminde Yapay Zekâ Yasası, Ankara Üniversitesi Hukuk Fakültesi Dergisi, 73(4), 2983–3016. Turan, A. K. ve Polat, H., 2024, Yarı denetimli makine öğrenmesi yöntemini kullanarak müzik türlerinin tespiti, Gazi Üniversitesi Fen Bilimleri Dergisi, Part C: Tasarım ve Teknoloji, 12(1), 92–107, DOI: 10.29109/gujsc.1352477 Turan, T., 2024, Dijital vicdan: Yapay Zeka Çağında Etik, YAZ Yayınları. Uçan, A. ve Akçapınar Sezer, E., 2019, Kelime Özyerleşikleri Kullanarak His Analojisi Üzerine Yeni Bir Yaklaşım, A New Approach on Emotion Analogy by Using Word Embeddings,27, Signal Processing and Communications Applications Conference (SIU), Hacettepe Üniversitesi, Ankara, DOI : 10.1109/SIU.2019.8806475 Ünal, A., ve Kılınç, İ., 2024, Üretken Yapay Zekâların İş Dünyası Üzerine Etkilerine İlişkin Erken Dönem Bir Değerlendirme, Elektronik Sosyal Bilimler Dergisi, 23(90), 776-797. Ünveren, D., 2024, Dil Eğitiminde Yapay Zeka ve Teknoloji: Bibliyometrik Bir Analiz, Dil Eğitimi ve Araştırmaları Dergisi, 10(2), 218-242, DOI: 10.31464/jlere.1463861 Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł. and Polosukhin, I., 2017, Attention İs All You Need, 31st International Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 123, "page_end": 123}}
{"content": "112 KAYNAKLAR DİZİNİ (devam) Wang, J., Jiang, H., Liu, Y., Ma, C., Zhang, X., Pan, Y., Liu, M., Gu, P., Xia, S., Li, W., Zhang, Y., Wu, Z., Liu, Z., Zhong, T., Ge, B., Zhang, T., Qiang, N., Hu, X., Jiang, X., Zhang, X., Zhang, W., Shen, D., Liu, T. and Zhang, S., 2024, A Comprehensive Review of Multimodal Large Language Models: Performance and Challenges Across Different Tasks, arXiv:2408.01319v1, https://arxiv.org/pdf/2408.01319 Wang, S., Zhang, S., Zhang, J., Hu, R., Li, X., Zhang, T., Li, J., Wu, F., Wang, G., and Hovy, E., 2025, Reinforcement Learning Enhanced LLMs: A Survey ,arXiv:2412.10400v3, https://arxiv.org/abs/2412.10400 Wang, X., Lyngaas, I., Tsaris, A., Chen, P., Dash, S., Shekar, M. C., Luo, T., Yoon, H.- J., Wahib, M and Gounley, J., 2023, Ultra-Long Sequence Distributed Transformer, arXiv:2311.02382, https://arxiv.org/abs/2311.02382 Wikipedia, “ELIZA”, https://en.wikipedia.org/wiki/ELIZA (Erişim Tarihi: 27.02.2025). Wikipedia, “Google DeepMind”, https://tr.wikipedia.org/wiki/Google_DeepMind (Erişim Tarihi: 01.03.2025). Wu T., He S., Liu J., Sun S., Liu K., Han Q.-L. and Tang Y., 2023, A Brief Overview of ChatGPT: The History, Status Quo and Potential Future Development. IEEE/CAA Journal of Automatica Sinica, 10(5). Wu, T., Guibas, L., Yang, G., Li, Z., Lin, D., Zhang, K., Liu, Z. and Wetzstein, G., 2024, GPT-4V(ision) is a Human-Aligned Evaluator for Text-to-3D Generation , arXiv:2401.04092v2, https://arxiv.org/pdf/2401.04092 Xiao, T. and Zhu, J., 2025. Foundations of Large Language Models,arXiv:2501.09223v1, https://arxiv.org/abs/2501.09223 Xiao, Y., Sun, E., Luo, D. and Wang, W., 2025, TradingAgents: Multi-Agents LLM Financial Trading Framework, arXiv:2412.20138, https://arxiv.org/abs/2412.20138 Xie, J., Chen, Z., Zhang, R., Wan, X. and Li, G. ,2024, Large Multimodal Agents: A Survey, arXiv:2402.15116, https://arxiv.org/pdf/2402.15116.pdf Yang, Z., Li, L., Lin, K., Wang, J., Lin, C.-C., Liu, Z. and Wang, L., 2023, The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision), arXiv:2309.17421v2, https://arxiv.org/pdf/2309.17421v2.pdf", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 124, "page_end": 124}}
{"content": "113 KAYNAKLAR DİZİNİ (devam) Yazar, S., Demiralay, T., ve Demirhan, T., 2024, Yazılım Geliştirme Öğreniminde Beceri Derinliği ve Dil Yeterliliğinin Yapay Zekâ ile Entegrasyonu , Üniversite Araştırmaları Dergisi, 7(4), 382–399. Yıldırım, S., 2024, Fine-tuning Transformer-based Encoder for Turkish Language Understanding Tasks, Istanbul Bilgi University, Faculty of Engineering and Natural Sciences, arXiv:2401.17396, https://arxiv.org/abs/2401.17396 Yılmaz, N. ve Tarhan, A., 2019, Açık Kaynak Yazılımlarda Bakım Yapılabilirliği ve Güvenilirliği Ölçmek İçin İki Boyutlu Değerlendirme Metodu, Gazi Üniversitesi Mühendislik ve Mimarlık Fakültesi Dergisi, 34(4), 1807-1829. DOI:10.17341/gazimmfd.571563 Yi, X., Yao, J., Wang, X. and Xie, X., 2023, Unpacking The Ethical Value Alignment in Big Models, arXiv:2310.17551, https://arxiv.org/abs/2310.17551 Yiğit, G., 2025. A Comparative Study of Deep Learning Approaches for Human Action Recognition, Turkish Journal of Engineering, 9(2), 281-289, DOI: 10.31127/tuje.1579795 Yin, S., Fu, C., Zhao, S., Li, K., Sun, X., Xu, T. and Chen, E. , 2024 , A Survey on Multimodal Large Language Models. arXiv:2306.13549, https://arxiv.org/pdf/2306.13549.pdf Yücesoy Kahraman, S., Durmuşoğlu, A. ve Dereli, T., 2024, Ön Eğitimli Bert Modeli ile Patent Sınıflandırılması , Gazi Üniversitesi Fen Bilimleri Dergisi, 39(4), 2485–2496. DOI: 10.17341/gazimmfd.1292543 Yürütücü, Ö. Y. ve Demir, Ş., 2023, Ön Eğitimli Dil Modelleriyle Duygu Analizi, İstanbul Sabahattin Zaim Üniversitesi Fen Bilimleri Enstitüsü Dergisi, 5(1), 46-53, DOI:10.47769/izufbed.1312032 Zaki, M. Z., 2024, Revolutionising Translation Technology: A Comparative Study of Variant Transformer Models - BERT, GPT and T5, Computer Science & Engineering: An International Journal (CSEIJ), 14(3) , DOI: 10.5121/cseij.2024.14302", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 125, "page_end": 125}}
{"content": "114 KAYNAKLAR DİZİNİ (devam) Zhang, D., Yu, Y., Dong, J., Li, C., Su, D., Chu, C. and Yu, D., 2024, MM-LLMs: Recent Advances in Multimodal Large Language Models, arXiv:2401.13601v5, https://arxiv.org/pdf/2401.13601v5.pdf Zhang, J., 2019, Gradient Descent Based Optimization Algorithms for Deep Learning Models Training , arXiv:1903.03614v1, https://arxiv.org/abs/1903.03614 Zhang, M., 2022, Unsupervised Learning Algorithms in Big Data: An Overview, Proceedings of ICHESS 2022, Advances in Social Science, Education and Humanities Research (ASSEHR), Vol. 720, 910–931pp, DOI: 10.2991/978-2-494069-89-3_10 Zhang, Y., Sun, R., Chen, Y., Pfister, T., Zhang, R. and Arık, S. Ö., 2024, Chain of Agents: Large Language Models Collaborating on Long-Context Tasks, 38th Conference on Neural Information Processing Systems (NeurIPS 2024), Penn State University, Google Cloud AI Research. Zhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z., Du, Y., Yang, C., Chen, Y., Chen, Z., Jiang, J., Ren, R., Li, Y., Tang, X., Liu, Z., Liu, P., Nie, J.-Y., and Wen, J.-R., 2025, A Survey of Large Language Models, arXiv:2303.18223, https://arxiv.org/abs/2303.18223 Zhong, Q., Ding, L., Liu, J., Du, B. and Tao, D., 2023, Can ChatGPT Understand Too? A Comparative Study on ChatGPT and Fine-tuned BERT, arXiv:2302.10198. https://arxiv.org/pdf/2302.10198.pdf Zorluel, M.,2019, Yapay zekâ ve telif hakkı, TBB Dergisi, (142).", "meta": {"title": "Yapay Zekâ Dil Modelleri – Lisans Tezi", "source": "yapayzekadilmodelleri.pdf", "page_start": 126, "page_end": 126}}
