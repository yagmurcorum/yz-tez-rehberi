# app.py
# ==================================================================================================
# AkÄ±llÄ± Tez Rehberi â€” RAG Chatbot (LangChain + Chroma + Gemini + Gradio)

# AmaÃ§
# - Bu uygulama, belirli bir kaynak korpustan (bu projede: tek bir lisans tezi) "RAG" (Retrieval-
#   Augmented Generation) yÃ¶ntemiyle soru-cevap Ã¼retir.
# - Kaynak korpus, Kaggle'da hazÄ±rlanmÄ±ÅŸ JSONL/Parquet Ã§Ä±ktÄ±larÄ±dÄ±r. Uygulama aÃ§Ä±lÄ±ÅŸÄ±nda bu
#   dosyalarÄ± otomatik olarak yÃ¼kler ("auto-ingest") ve Chroma vektÃ¶r veritabanÄ±na kaydeder.
# - KullanÄ±cÄ±, Gradio arayÃ¼zÃ¼nden soru yazar. Sistem, soruya en Ã§ok benzeyen metin parÃ§alarÄ±nÄ±
#   bulur ("retrieve"), bu baÄŸlamÄ± bir istem (prompt) ile Gemini 2.0 modeline verir ve cevabÄ± Ã¼retir.
# - CevabÄ±n sonunda "Kaynaklar" baÅŸlÄ±ÄŸÄ± altÄ±nda hangi dosya/sayfa(lar)dan yararlanÄ±ldÄ±ÄŸÄ± listelenir.

# TasarÄ±m Ä°lkeleri
# - Veri, repo iÃ§indeki data/ klasÃ¶rÃ¼nden otomatik yÃ¼klenir.
# - Her adÄ±m (ingest â†’ retrieve â†’ prompt â†’ generate â†’ answer) aÃ§Ä±k ve izlenebilir ÅŸekilde
#   kodlanmÄ±ÅŸtÄ±r

# Ortam DeÄŸiÅŸkenleri (HF Spaces â†’ Settings â†’ Variables and secrets):
# - SECRET: GOOGLE_API_KEY        â†’ Gemini API anahtarÄ± (zorunlu)
# - VAR   : EMBEDDINGS_MODEL      â†’ trmteb/turkish-embedding-model
# - VAR   : GENERATION_MODEL      â†’ gemini-2.0-flash (eriÅŸim yoksa gemini-1.5-flash)
# - VAR   : CHROMA_PERSIST_DIR    â†’ .chroma (Chroma'nÄ±n kalÄ±cÄ± dizini; Space yeniden aÃ§Ä±ldÄ±ÄŸÄ±nda korunur)
#
# Gerekli Paketler (requirements.txt)
# ==================================================================================================

import os
import re
import json
from typing import List, Tuple

# 1) LLM: Gemini (Google Generative AI)
#    - Sadece yanÄ±t Ã¼retiminde kullanÄ±lÄ±r. Retrieval/embedding aÅŸamalarÄ± Space'in container'Ä±nda Ã§alÄ±ÅŸÄ±r.
import google.generativeai as genai

# 2) VektÃ¶r DB ve Embedding
#    - LangChain-Chroma: Chroma'nÄ±n resmi LangChain paketidir (deprecation sorunlarÄ± yaÅŸamaz).
#    - LangChain-HuggingFace: SentenceTransformers tabanlÄ± gÃ¶mlemeler iÃ§in gÃ¼ncel paket.
import chromadb
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings

# 3) Dosya okuma (yalnÄ±zca JSONL/Parquet)
import pandas as pd

# 4) Web arayÃ¼zÃ¼
import gradio as gr
from gradio.themes.base import Base
from gradio.themes.utils import colors, sizes


# --------------------------------------------------------------------------------------------------
# 0) Ortam deÄŸiÅŸkenleri ve LLM yapÄ±landÄ±rmasÄ±
#    - HF Spaces'te Secrets/Variables ile gelir; yerelde .env Ã¼zerinden verilebilir.
# --------------------------------------------------------------------------------------------------
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY", "")
EMBEDDINGS_MODEL = os.getenv("EMBEDDINGS_MODEL", "trmteb/turkish-embedding-model")
GENERATION_MODEL = os.getenv("GENERATION_MODEL", "gemini-2.0-flash")
CHROMA_PERSIST_DIR = os.getenv("CHROMA_PERSIST_DIR", ".chroma")

# STRICT RAG: aÃ§Ä±kâ€‘alan fallback kapalÄ± (yalnÄ±zca tezden yanÄ±t ver).
ALLOW_OPEN_DOMAIN_FALLBACK = os.getenv("ALLOW_OPEN_DOMAIN_FALLBACK", "false").lower() == "true"

# Sayfa filtreleme: PDF sayfa 13-104 arasÄ± tez iÃ§eriÄŸi (1-12: Ã¶n sayfalar, 105+: kaynakÃ§a/ekler)
PDF_PAGE_START = int(os.getenv("PDF_PAGE_START", "13"))
PDF_PAGE_END = int(os.getenv("PDF_PAGE_END", "104"))
PDF_TO_THESIS_OFFSET = int(os.getenv("PDF_TO_THESIS_OFFSET", "0"))

# Gemini istemcisi; API anahtarÄ± zorunludur.
genai.configure(api_key=GOOGLE_API_KEY)

# Ãœretim (generate) parametreleri:
# - temperature: RAG'de dÃ¼ÅŸÃ¼k tutulur (0.0â€“0.3 aralÄ±ÄŸÄ±), kaynaÄŸa sadakat artar.
# - top_p/top_k: Ã–rnekleme Ã§eÅŸitliliÄŸi; varsayÄ±lanlar korunur, gerekirse ayarlanÄ±r.
# - max_output_tokens: CevabÄ±n Ã¼st uzunluÄŸu; kesilme yaÅŸanÄ±rsa artÄ±rÄ±labilir (Ã¶r. 768/1024).
GENERATION_CFG = dict(
    temperature=0.25,
    top_p=0.95,
    top_k=40,
    max_output_tokens=1024
)

# YanÄ±t uzunluÄŸu Ã¶n ayarlarÄ± (STRICT RAG)
RESPONSE_LENGTH_TO_TOKENS = {
    "KÄ±sa": 200,
    "Orta": 800,
    "Uzun": 1500
}
DEFAULT_RESPONSE_LENGTH = os.getenv("DEFAULT_RESPONSE_LENGTH", "Orta")
CURRENT_TOP_K = int(os.getenv("CURRENT_TOP_K", "5"))


# --------------------------------------------------------------------------------------------------
# 1) Metin yardÄ±mcÄ±larÄ±
#    - clean_text: metni biÃ§imsel olarak normalize eder.
#    - split_into_chunks: uzun metni kaygan pencere (sliding window) ile kÃ¼Ã§Ã¼k parÃ§alara bÃ¶ler.
# --------------------------------------------------------------------------------------------------
def clean_text(s: str) -> str:
    """
    Metin temizliÄŸi:
    - \x00 gibi bozuk karakterleri kaldÄ±rÄ±r.
    - Birden fazla boÅŸluk/tab'Ä± tek boÅŸluÄŸa indirir.
    - Ã‡oklu boÅŸ satÄ±rlarÄ± sadeleÅŸtirir (3+ â†’ 2).
    Bu temizleme, embedding kalitesini ve okunabilirliÄŸi iyileÅŸtirir.
    """
    s = (s or "").replace("\x00", " ")
    s = re.sub(r"[ \t]+", " ", s)
    s = re.sub(r"\n{3,}", "\n\n", s)
    return s.strip()


def split_into_chunks(text: str, size: int = 800, overlap: int = 120) -> List[str]:
    """
    Metni kelime bazlÄ± parÃ§alara bÃ¶ler.
    Parametreler:
      - size: hedef parÃ§a uzunluÄŸu (kelime)
      - overlap: art arda gelen parÃ§alar arasÄ±ndaki ortak kelime sayÄ±sÄ±
    Not:
      - RAG'de 512â€“800 kelime iyi bir baÅŸlangÄ±Ã§ aralÄ±ÄŸÄ±dÄ±r; overlap 80â€“120 Ã¶nerilir.
    """
    words = (text or "").split()
    if not words:
        return []
    chunks: List[str] = []
    i = 0
    while i < len(words):
        piece = " ".join(words[i:i + size]).strip()
        if piece:
            chunks.append(piece)
        nxt = i + size - overlap
        i = nxt if nxt > i else i + size
    return chunks


# YENÄ°: Basit tokenizasyon ve sorgudan anahtar Ã§Ä±karÄ±mÄ± (genel; sayfaya/konuya Ã¶zgÃ¼ deÄŸil)
def tokenize_for_keywords(text: str) -> list[str]:
    """
    YENÄ°:
    - KÃ¼Ã§Ã¼k harfe indir, TR karakterleri basit normalize et
    - Harf/rakam dÄ±ÅŸÄ±nÄ± boÅŸlukla deÄŸiÅŸtir
    - 1 karakterlik parÃ§alarÄ± ele
    """
    txt = (text or "").lower()
    tr_map = str.maketrans("Ã§ÄŸÄ±Ã¶ÅŸÃ¼Ã¢Ã®Ã»", "cgiosuaiu")
    txt = txt.translate(tr_map)
    txt = re.sub(r"[^a-z0-9ÄŸÃ¼ÅŸÄ±Ã¶Ã§ ]", " ", txt)
    tokens = [t for t in txt.split() if len(t) > 1]
    return tokens


# GÃœNCELLEME: Sadece sorgudan tÃ¼reyen anahtarlar (stop-words hariÃ§). Sayfaya/konuya Ã¶zgÃ¼ deÄŸil.
def build_query_keywords(query: str) -> set[str]:
    tokens = tokenize_for_keywords(query)
    stop = {
        "ve","ile","mi","nedir","nelerdir","hangi","temel","alan","olarak","da","de","bir","icin",
        "nasil","ne","kim","neydi","neye","hakkinda","uzerine","ileti","olan","midir","midirki"
    }
    return {t for t in tokens if t not in stop and len(t) > 2}


# YENÄ°: Sorgudan basit ikili ifadeler (bigram) Ã¼ret (genel eÅŸleÅŸmeyi gÃ¼Ã§lendirir)
def build_query_bigrams(query: str) -> set[str]:
    toks = [t for t in tokenize_for_keywords(query) if len(t) > 2]
    return {" ".join([toks[i], toks[i + 1]]) for i in range(len(toks) - 1)}


def is_valid_page(page_num: int) -> bool:
    """
    Sayfa filtreleme: PDF sayfa 13-104 arasÄ± tez iÃ§eriÄŸi
    (1-12: Ã¶n sayfalar, 105+: kaynakÃ§a/ekler)
    """
    return PDF_PAGE_START <= page_num <= PDF_PAGE_END


def pdf_to_thesis_page(pdf_page: int) -> int:
    """
    PDF sayfa numarasÄ±nÄ± tez sayfa numarasÄ±na Ã§evirir.
    Ã–rnek: PDF sayfa 1 â†’ Tez sayfa 1 (offset 0 ile)
    """
    return pdf_page + PDF_TO_THESIS_OFFSET


# --------------------------------------------------------------------------------------------------
# 2) Embedding saÄŸlayÄ±cÄ±sÄ± ve Chroma vektÃ¶r veritabanÄ±
#    - Embedding: TÃ¼rkÃ§e iÃ§in SentenceTransformers modeli (HF Ã¼zerinden Ã§ekilir).
#    - Chroma: .chroma klasÃ¶rÃ¼ne kalÄ±cÄ± olarak yazar (Space yeniden baÅŸlasa da veri korunur).
# --------------------------------------------------------------------------------------------------
# YENÄ°: CHROMA dizinini garantiye al
os.makedirs(CHROMA_PERSIST_DIR, exist_ok=True)

embeddings = HuggingFaceEmbeddings(model_name=EMBEDDINGS_MODEL)

chroma_client = chromadb.PersistentClient(path=CHROMA_PERSIST_DIR)
vectorstore = Chroma(
    client=chroma_client,
    collection_name="docs",
    embedding_function=embeddings
)


# --------------------------------------------------------------------------------------------------
# 3) Ingest fonksiyonlarÄ±
#    - JSONL: her satÄ±r bir kaydÄ± temsil eder â†’ {"content": "...", "meta": {...}}
#    - Parquet: tablo formatÄ±; "content" zorunlu, "meta" opsiyonel (dict) ya da sÃ¼tunlardan derlenir.
#    - Sayfa filtreleme: sadece PDF sayfa 13-104 arasÄ±ndaki parÃ§alar ingest edilir.
# --------------------------------------------------------------------------------------------------
def ingest_jsonl(file_obj) -> str:
    """
    JSONL dosyasÄ±nÄ± satÄ±r satÄ±r okuyup metin + metadata Ã§Ä±karÄ±r ve Chroma'ya ekler.
    """
    try:
        lines = file_obj.read().decode("utf-8").splitlines()
        texts, metas = [], []
        
        for ln in lines:
            row = json.loads(ln)
            content = clean_text(row.get("content", ""))
            if not content:
                continue
            meta = row.get("meta", {}) or {}
            
            page_num = meta.get("page_start")
            if page_num and not is_valid_page(int(page_num)):
                continue
                
            texts.append(content)
            metas.append(meta)
            
        if not texts:
            return "JSONL boÅŸ ya da geÃ§erli kayÄ±t bulunamadÄ±."
            
        vectorstore.add_texts(texts=texts, metadatas=metas)
        return f"JSONL ingest tamamlandÄ±: {len(texts)} parÃ§a eklendi."
    except Exception as e:
        return f"JSONL ingest hatasÄ±: {e}"


def ingest_parquet(file_obj) -> str:
    """
    Parquet dosyasÄ±nÄ± okuyup "content" ve (varsa) "meta" bilgilerini alÄ±r ve Chroma'ya ekler.
    """
    try:
        df = pd.read_parquet(file_obj)
        if "content" not in df.columns:
            return "Parquet: 'content' sÃ¼tunu bulunamadÄ±."
        texts, metas = [], []
        for _, r in df.iterrows():
            content = clean_text(str(r["content"]))
            if not content:
                continue
            meta = {}
            if "meta" in df.columns and isinstance(r.get("meta"), dict):
                meta = r.get("meta") or {}
            for key in ["title", "source", "page", "page_start", "page_end"]:
                if key in df.columns:
                    val = r.get(key)
                    if pd.notna(val) and val != "":
                        meta.setdefault(key, val)
            
            page_num = meta.get("page_start")
            if page_num and not is_valid_page(int(page_num)):
                continue
                
            texts.append(content)
            metas.append(meta)
        if not texts:
            return "Parquet: geÃ§erli satÄ±r bulunamadÄ±."
        vectorstore.add_texts(texts=texts, metadatas=metas)
        return f"Parquet ingest tamamlandÄ±: {len(texts)} parÃ§a eklendi."
    except Exception as e:
        return f"Parquet ingest hatasÄ±: {e}"


# --------------------------------------------------------------------------------------------------
# 4) Retrieval ve Prompt oluÅŸturma
#    - retrieve: Sorguya en Ã§ok benzeyen k metin parÃ§asÄ±nÄ± Chroma'dan getirir.
#    - build_prompt: Sistem talimatÄ± + baÄŸlam + soru birleÅŸiminden Gemini istemini kurar.
#    - generate_with_gemini: YanÄ±t Ã¼retir; metin dÃ¶ndÃ¼rÃ¼r.
# --------------------------------------------------------------------------------------------------
SYSTEM_MSG = (
    "AÅŸaÄŸÄ±daki baÄŸlam parÃ§alarÄ±nÄ± kullanarak yanÄ±t ver. "
    "BaÄŸlamda verilen TÃœM ilgili bilgileri, tarihleri, isimleri ve detaylarÄ± MUTLAKA dahil et. "
    "EÄŸer baÄŸlamda yeterli bilgi yoksa: 'Bu konu tezde yeterli detayla bulunamadÄ±.' de. "
    "YanÄ±tÄ± tam cÃ¼mlelerle bitir; maddeleri yarÄ±m bÄ±rakma."
)


def retrieve(query: str, k: int):
    """
    Sorgu embedding'i ile Chroma'dan en ilgili k belge parÃ§asÄ±nÄ± getirir.
    GÃœNCELLEME:
    - Skorlar distance/negatif olabilir â†’ 0..1 arasÄ± benzerliÄŸe normalize edilir.
    - Leksikal yeniden sÄ±ralama (re-rank) tÃ¼m sorgular iÃ§in uygulanÄ±r.
    """
    try:
        # Daha zengin havuz iÃ§in 2x sonuÃ§ Ã§ek (min 8)
        results = vectorstore.similarity_search_with_score(query, k=max(k * 2, 8))
        # results: List[(Document, score)]  -> score Ã§oÄŸunlukla distance (kÃ¼Ã§Ã¼k = iyi)
        query_keywords = build_query_keywords(query)
        query_bigrams  = build_query_bigrams(query)

        raw_items = []
        max_hits = 1
        for doc, raw_score in results:
            # 1) Embedding skorunu 0..1 benzerliÄŸe Ã§evir
            try:
                if raw_score is None:
                    emb_sim = 0.5
                elif float(raw_score) >= 0:
                    emb_sim = 1.0 / (1.0 + float(raw_score))   # distance -> similarity
                else:
                    emb_sim = 1.0 / (1.0 + abs(float(raw_score)))
            except Exception:
                emb_sim = 0.5

            # 2) Leksikal eÅŸleÅŸme (kelime + bigram)
            text = (doc.page_content or "").lower()
            text_tokens = tokenize_for_keywords(text)
            word_hits   = sum(1 for t in text_tokens if t in query_keywords)
            phrase_hits = sum(1 for bg in query_bigrams if bg in text)
            hits = word_hits + 2 * phrase_hits
            max_hits = max(max_hits, hits)

            raw_items.append((doc, emb_sim, hits))

        # 3) BirleÅŸtir ve sÄ±rala
        reranked = []
        for doc, emb_sim, hits in raw_items:
            lexical_norm = hits / max_hits if max_hits > 0 else 0.0
            combined = 0.85 * emb_sim + 0.15 * lexical_norm
            reranked.append((combined, doc))

        reranked.sort(key=lambda x: x[0], reverse=True)
        docs = [doc for _score, doc in reranked[:k]]
        if not docs:
            return vectorstore.similarity_search(query, k=k)
        return docs
    except Exception:
        return vectorstore.similarity_search(query, k=k)


def page_label(meta: dict) -> str:
    """
    Metadata'dan sayfa etiketini Ã§Ä±karÄ±r.
    Ã–ncelik sÄ±rasÄ±: page_label > logical_page > page_start/page_end > page (offset ile)
    """
    if meta.get("page_label"):
        return str(meta["page_label"])
    if meta.get("logical_page"):
        return str(meta["logical_page"])
    if meta.get("page_start") and meta.get("page_end"):
        start = str(meta["page_start"])
        end = str(meta["page_end"])
        return start if start == end else f"{start}-{end}"
    if meta.get("page_start"):
        return str(meta["page_start"])
    if meta.get("page") is not None:
        try:
            return str(int(meta["page"]) + PDF_TO_THESIS_OFFSET)
        except Exception:
            return str(meta["page"])
    return "?"


def build_prompt(query: str, docs, length_choice: str) -> str:
    """
    Gemini'ye verilecek istem (prompt):
      - Sistem talimatÄ± (modelin davranÄ±ÅŸ sÄ±nÄ±rlarÄ±)
      - BaÄŸlam: numaralÄ± satÄ±rlar; kaynak adÄ± ve sayfa bilgisi gÃ¶rÃ¼nÃ¼r
      - KullanÄ±cÄ± sorusu
      - YanÄ±t uzunluÄŸu talimatÄ±
    """
    length_instructions = {
        "KÄ±sa": "KÄ±sa ve Ã¶z bir yanÄ±t ver. Sadece temel bilgileri belirt.",
        "Orta": "DetaylÄ± ama Ã¶zlÃ¼ bir yanÄ±t ver. Ã–nemli noktalarÄ± aÃ§Ä±kla.",
        "Uzun": "KapsamlÄ± ve detaylÄ± bir yanÄ±t ver. TÃ¼m ilgili bilgileri, Ã¶rnekleri ve aÃ§Ä±klamalarÄ± dahil et."
    }
    ctx_lines = []
    for i, d in enumerate(docs, start=1):
        meta = d.metadata or {}
        src = meta.get("source", "unknown")
        page_display = page_label(meta)
        ctx_lines.append(f"[{i}] ({src} s.{page_display}) {d.page_content}")
    context = "\n\n".join(ctx_lines) if ctx_lines else "(baÄŸlam yok)"
    length_instruction = length_instructions.get(length_choice, length_instructions["Orta"])
    return f"{SYSTEM_MSG}\n\n{length_instruction}\n\nBaÄŸlam:\n{context}\n\nSoru: {query}\nYanÄ±t:"


def generate_with_gemini(prompt: str, max_tokens: int | None = None) -> str:
    """
    Gemini'den yanÄ±t Ã¼retir ve dÃ¼z metin olarak dÃ¶ndÃ¼rÃ¼r.
    """
    cfg = dict(GENERATION_CFG)
    if max_tokens is not None:
        cfg["max_output_tokens"] = int(max_tokens)
    model = genai.GenerativeModel(GENERATION_MODEL, generation_config=cfg)
    resp = model.generate_content(prompt)
    return (resp.text or "").strip()


def answer_fn(message: str, history: List[Tuple[str, str]], length_choice: str) -> str:
    """
    ChatInterface tarafÄ±ndan Ã§aÄŸrÄ±lÄ±r.
    RAG pipeline: retrieve â†’ build_prompt â†’ generate â†’ format_response
    GÃœNCELLEME:
    - "Tezde bulunamadÄ±" veya "yanÄ±t Ã¼retilemedi" durumlarÄ±nda kaynak/uyarÄ± GÃ–STERÄ°LMEZ.
    - UyarÄ± yalnÄ±zca kaynak bloÄŸu varsa eklenir.
    """
    try:
        simple_greetings = ["merhaba", "selam", "hello", "hi", "nasÄ±lsÄ±n", "iyi misin"]
        if message.lower().strip() in simple_greetings:
            return "Merhaba! Yapay ZekÃ¢ Dil Modelleri tezi hakkÄ±nda sorularÄ±nÄ±zÄ± sorabilirsiniz."

        docs = retrieve(message, k=CURRENT_TOP_K)
        if not docs:
            return "Bu konu tezde bulunamadÄ± veya sorunuzla yeterince ilgili deÄŸil."

        prompt = build_prompt(message, docs, length_choice)
        max_tokens = RESPONSE_LENGTH_TO_TOKENS.get(length_choice, RESPONSE_LENGTH_TO_TOKENS["Orta"])
        answer = generate_with_gemini(prompt, max_tokens=max_tokens)
        if not answer:
            return "YanÄ±t Ã¼retilemedi."

        # Model "bulunamadÄ±" vb. diyorsa aynen dÃ¶ndÃ¼r; kaynak/uyarÄ± ekleme.
        low_answer = answer.lower()
        if ("bulunamadÄ±" in low_answer) or ("yeterli detay" in low_answer):
            return answer

        # Kaynak sayfalarÄ± topla
        pages_by_source = {}
        for d in docs:
            m = d.metadata or {}
            display_name = "Yapay ZekÃ¢ Dil Modelleri"
            page_display = page_label(m)
            if page_display != "?":
                pages_by_source.setdefault(display_name, set()).add(page_display)

        # Kaynak bloÄŸu
        if pages_by_source:
            def sort_key(p: str):
                head = str(p).split("-")[0]
                return int(head) if head.isdigit() else 10**9
            items = []
            for src, pages in pages_by_source.items():
                ordered = ", ".join(sorted(pages, key=sort_key))
                items.append(f"- {src} s. {ordered}")
            sources_block = "Kaynak: " + items[0][2:] if len(items) == 1 else "Kaynaklar:\n" + "\n".join(items)

            # YalnÄ±zca kaynak bloÄŸu varsa uyarÄ± ekle
            warning_note = "\n\nâ„¹ï¸ Bu yanÄ±t birden fazla sayfadan derlenmiÅŸtir. Tam bilgi iÃ§in kaynak sayfalara gÃ¶z atÄ±n."
            final_answer = (answer or "YanÄ±t Ã¼retilemedi.").rstrip() + "\n\n" + sources_block + warning_note
            return final_answer

        # Kaynak yoksa sadece yanÄ±tÄ± dÃ¶ndÃ¼r
        return (answer or "YanÄ±t Ã¼retilemedi.").rstrip()

    except Exception as e:
        return f"Hata: {e}"


# --------------------------------------------------------------------------------------------------
# 5) Otomatik ingest (deploy esnasÄ±nda hiÃ§bir kullanÄ±cÄ± aksiyonu gerektirmeden veri yÃ¼kler)
#    - data/processed_docs.jsonl
#    - data/processed_docs.parquet
# --------------------------------------------------------------------------------------------------
def auto_ingest_from_repo() -> str:
    """
    Uygulama baÅŸlarken veri klasÃ¶rÃ¼ndeki dosyalarÄ± ingest eder.
    """
    logs = []
    try:
        p = "data/processed_docs.jsonl"
        if os.path.exists(p):
            with open(p, "rb") as f:
                result = ingest_jsonl(f)
                logs.append(result)
    except Exception as e:
        logs.append(f"AUTO JSONL hata: {e}")

    try:
        p = "data/processed_docs.parquet"
        if os.path.exists(p):
            with open(p, "rb") as f:
                result = ingest_parquet(f)
                logs.append(result)
    except Exception as e:
        logs.append(f"AUTO Parquet hata: {e}")

    # YENÄ°: ingest sonrasÄ± koleksiyon sayÄ±mÄ± logla (HF Spaces konsolu iÃ§in faydalÄ±)
    try:
        cnt = vectorstore._collection.count()
        logs.append(f"[Chroma] persist_dir={CHROMA_PERSIST_DIR} | collection='docs' | count={cnt}")
    except Exception as e:
        logs.append(f"[Chroma] count okunamadÄ±: {e}")

    return "\n".join([lg for lg in logs if lg])


# --------------------------------------------------------------------------------------------------
# 6) Gradio arayÃ¼zÃ¼
#    - Bu sÃ¼rÃ¼mde dosya yÃ¼kleme kapalÄ±dÄ±r; veri aÃ§Ä±lÄ±ÅŸta otomatik yÃ¼klenir.
#    - Sol panel: Tez indirme + estetik iÃ§indekiler + yanÄ±t uzunluÄŸu seÃ§imi
#    - SaÄŸ panel: Sohbet arayÃ¼zÃ¼
#    Yazar/DanÄ±ÅŸman/Kapsam baÅŸlÄ±kta kalÄ±cÄ± gÃ¶sterilir.
# --------------------------------------------------------------------------------------------------
EXAMPLES = [
    "Tezin temel problem tanÄ±mÄ± nedir?",
    "Transformer mimarisinin temel yapÄ±taÅŸlarÄ± nelerdir?",
    "Kendine dikkat (self-attention) nasÄ±l Ã§alÄ±ÅŸÄ±r?",
    "RNN/LSTM/GRU'nun karÅŸÄ±laÅŸtÄ±ÄŸÄ± temel sorunlar nelerdir?",
    "GPT ve BERT hangi gÃ¶revlerde daha baÅŸarÄ±lÄ±dÄ±r?",
    "Temel doÄŸal dil iÅŸleme teknikleri nelerdir?",
    "Yapay zekÃ¢ nasÄ±l tanÄ±mlanÄ±r? KapsadÄ±ÄŸÄ± alt alanlar nelerdir?",
    "Ã‡ok modlu modellerin Ã¶ne Ã§Ä±kan Ã¶rnekleri hangileri?",
    "Etik bÃ¶lÃ¼mÃ¼nde hangi riskler tartÄ±ÅŸÄ±lÄ±yor?",
    "Gelecek Ã§alÄ±ÅŸmalar iÃ§in Ã¶neriler nelerdir?",
    "Tezi bana anlatÄ±r mÄ±sÄ±n?",
]

# Tema: aÃ§Ä±k, yÃ¼ksek okunabilirlik ve sade anahtarlar (Gradio ile uyumlu)
theme = Base(
    primary_hue=colors.blue,
    secondary_hue=colors.violet,
    text_size=sizes.text_md,
    radius_size=sizes.radius_md,
).set(
    body_background_fill="#f7f9fc",
    block_background_fill="#ffffff",
    border_color_primary="#e3e8f0",
    body_text_color="#0f172a",
    block_title_text_color="#0b1220",
    link_text_color="#1d4ed8",
    button_primary_background_fill="#2563eb",
    button_primary_text_color="#ffffff",
    input_background_fill="#ffffff",
    input_border_color="#cbd5e1",
)

# CSS (estetik kartlar ve diÄŸer stiller)
css = """
.gr-example {
  background: #f1f5ff !important;
  color: #0f172a !important;
  border: 1px solid #dbe2f3 !important;
}
.gr-example:hover {
  background: #e6eeff !important;
  border-color: #c1cff5 !important;
}
input, textarea, .gr-text-input input, .gr-textbox textarea {
  color: #0f172a !important;
}
::placeholder {
  color: #6b7280 !important;
}
button.primary:hover {
  background-color: #1d4ed8 !important;
}
.gr-chatbot .message.user { background:#eef2ff !important; border-color:#dbe2f3 !important; }
.gr-chatbot .message.bot  { background:#ffffff !important; border-color:#e3e8f0 !important; }
.toc-container { 
  display: flex; 
  flex-direction: column; 
  gap: 8px; 
  margin: 10px 0; 
}
.toc-card {
  padding: 12px 14px;
  border-radius: 8px;
  background: linear-gradient(135deg, #ffffff 0%, #f8fafc 100%);
  border: 1px solid #e2e8f0;
  box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
  transition: all 0.2s ease;
}
.toc-card:hover {
  border-color: #3b82f6;
  box-shadow: 0 4px 12px rgba(59, 130, 246, 0.15);
  transform: translateY(-1px);
}
.toc-header {
  display: flex;
  justify-content: space-between;
  align-items: center;
  font-weight: 600;
  color: #1e293b;
  margin-bottom: 4px;
}
.toc-page {
  color: #64748b;
  font-size: 0.9em;
  font-weight: 500;
}
.toc-subtitle {
  color: #475569;
  font-size: 0.85em;
  line-height: 1.4;
  margin-top: 6px;
}
"""

def chat_step(user_message: str, history: list[tuple[str, str]], length_choice: str):
    """
    KullanÄ±cÄ± mesajÄ±nÄ± iÅŸleyip chatbot yanÄ±tÄ±nÄ± dÃ¶ndÃ¼rÃ¼r.
    """
    msg = (user_message or "").strip()
    if not msg:
        return history, ""
    bot_reply = answer_fn(msg, history=history or [], length_choice=length_choice)
    new_history = (history or []) + [(msg, bot_reply)]
    return new_history, ""


with gr.Blocks(title="Yapay ZekÃ¢ Dil Modelleri â€¢ KaynaklÄ± Soruâ€‘Cevap", theme=theme, css=css, fill_height=True) as demo:
    gr.Markdown(
        """
        <div style="padding:10px 0 4px 0;">
          <h2 style="margin:0;color:#0b1220;">Yapay ZekÃ¢ Dil Modelleri â€” Tez AsistanÄ±</h2>
          <div style="color:#334155;margin-top:8px;">
            Bu arayÃ¼z, 'Yapay ZekÃ¢ Dil Modelleri' tezi temel alÄ±narak sorularÄ±nÄ±za yanÄ±t verir; ilgili pasajlarÄ± bulur ve kaynak sayfalarÄ±yla birlikte sunar.
          </div>
          <div style="color:#64748b;font-size:0.9em;margin-top:8px;">
            ğŸ“ <strong>Yazar:</strong> YaÄŸmur Ã‡ORUM |
            ğŸ‘¨â€ğŸ« <strong>DanÄ±ÅŸman:</strong> Prof. Dr. Burak ORDÄ°N |
            ğŸ“š <strong>Kapsam:</strong> BÃ¶lÃ¼m 1-7 (Ana Ä°Ã§erik)
          </div>
        </div>
        """,
    )

    with gr.Row():
        with gr.Column(scale=1, min_width=400):
            gr.Markdown("### ğŸ“„ Tez DokÃ¼manÄ±")
            gr.DownloadButton(label="ğŸ“„ Tezi Ä°ndir (PDF)", value="data/tez.pdf")

            gr.Markdown("### ğŸ“š Ä°Ã§indekiler")
            gr.HTML(
                """
                <div class="toc-container">
                  <div class="toc-card"><div class="toc-header"><span>1. GÄ°RÄ°Å</span></div></div>
                  <div class="toc-card"><div class="toc-header"><span>2. YAPAY ZEKÃ‚ VE DOÄAL DÄ°L Ä°ÅLEME</span></div></div>
                  <div class="toc-card"><div class="toc-header"><span>3. DÄ°L MODELLEMEDE ML ve DL</span></div></div>
                  <div class="toc-card"><div class="toc-header"><span>4. DÄ°L MODELLERÄ°</span></div></div>
                  <div class="toc-card"><div class="toc-header"><span>5. TRANSFORMER TABANLI MODELLER</span></div></div>
                  <div class="toc-card"><div class="toc-header"><span>6. GÃœNCEL YÃ–NELÄ°MLER ve ETÄ°K</span></div></div>
                  <div class="toc-card"><div class="toc-header"><span>7. SONUÃ‡ ve DEÄERLENDÄ°RME</span></div></div>
                </div>
                """
            )

            length_choice = gr.Radio(
                choices=["KÄ±sa", "Orta", "Uzun"],
                value=DEFAULT_RESPONSE_LENGTH,
                label="YanÄ±t uzunluÄŸu"
            )

        with gr.Column(scale=2):
            chatbot = gr.Chatbot(height=500, avatar_images=(None, None), type="messages")
            input_box = gr.Textbox(placeholder="Sorunuzu yazÄ±n ve Enter'a basÄ±n...", scale=1)
            send_btn = gr.Button("GÃ¶nder", variant="primary")

            send_btn.click(
                chat_step,
                inputs=[input_box, chatbot, length_choice],
                outputs=[chatbot, input_box]
            )
            input_box.submit(
                chat_step,
                inputs=[input_box, chatbot, length_choice],
                outputs=[chatbot, input_box]
            )

            with gr.Row():
                for q in EXAMPLES:
                    gr.Button(q, variant="secondary", scale=1).click(
                        lambda s=q: s, outputs=input_box
                    ).then(
                        chat_step,
                        inputs=[input_box, chatbot, length_choice],
                        outputs=[chatbot, input_box]
                    )

    logs = auto_ingest_from_repo()
    if logs:
        print(logs)


if __name__ == "__main__":
    demo.launch(server_port=7860, share=False)