# app.py
# ==================================================================================================
# AkÄ±llÄ± Tez Rehberi â€” RAG Chatbot (LangChain + Chroma + Gemini + Gradio)

# AmaÃ§
# - Bu uygulama, belirli bir kaynak korpustan (bu projede: tek bir lisans tezi) "RAG" (Retrieval-
#   Augmented Generation) yÃ¶ntemiyle soru-cevap Ã¼retir.
# - Kaynak korpus, Kaggle'da hazÄ±rlanmÄ±ÅŸ JSONL/Parquet Ã§Ä±ktÄ±larÄ±dÄ±r. Uygulama aÃ§Ä±lÄ±ÅŸÄ±nda bu
#   dosyalarÄ± otomatik olarak yÃ¼kler ("auto-ingest") ve Chroma vektÃ¶r veritabanÄ±na kaydeder.
# - KullanÄ±cÄ±, Gradio arayÃ¼zÃ¼nden soru yazar. Sistem, soruya en Ã§ok benzeyen metin parÃ§alarÄ±nÄ±
#   bulur ("retrieve"), bu baÄŸlamÄ± bir istem (prompt) ile Gemini 2.0 modeline verir ve cevabÄ± Ã¼retir.
# - CevabÄ±n sonunda "Kaynaklar" baÅŸlÄ±ÄŸÄ± altÄ±nda hangi dosya/sayfa(lar)dan yararlanÄ±ldÄ±ÄŸÄ± listelenir.

# TasarÄ±m Ä°lkeleri
# - Veri, repo iÃ§indeki data/ klasÃ¶rÃ¼nden otomatik yÃ¼klenir.
# - Her adÄ±m (ingest â†’ retrieve â†’ prompt â†’ generate â†’ answer) aÃ§Ä±k ve izlenebilir ÅŸekilde
#   kodlanmÄ±ÅŸtÄ±r

# Ortam DeÄŸiÅŸkenleri (HF Spaces â†’ Settings â†’ Variables and secrets):
# - SECRET: GOOGLE_API_KEY        â†’ Gemini API anahtarÄ± (zorunlu)
# - VAR   : EMBEDDINGS_MODEL      â†’ trmteb/turkish-embedding-model
# - VAR   : GENERATION_MODEL      â†’ gemini-2.0-flash (eriÅŸim yoksa gemini-1.5-flash)
# - VAR   : CHROMA_PERSIST_DIR    â†’ .chroma (Chroma'nÄ±n kalÄ±cÄ± dizini; Space yeniden aÃ§Ä±ldÄ±ÄŸÄ±nda korunur)
#
# Gerekli Paketler (requirements.txt)
# ==================================================================================================

import os
import re
import json
from typing import List, Tuple

# 1) LLM: Gemini (Google Generative AI)
#    - Sadece yanÄ±t Ã¼retiminde kullanÄ±lÄ±r. Retrieval/embedding aÅŸamalarÄ± Space'in container'Ä±nda Ã§alÄ±ÅŸÄ±r.
import google.generativeai as genai

# 2) VektÃ¶r DB ve Embedding
#    - LangChain-Chroma: Chroma'nÄ±n resmi LangChain paketidir (deprecation sorunlarÄ± yaÅŸamaz).
#    - LangChain-HuggingFace: SentenceTransformers tabanlÄ± gÃ¶mlemeler iÃ§in gÃ¼ncel paket.
import chromadb
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings

# 3) Dosya okuma (yalnÄ±zca JSONL/Parquet)
import pandas as pd

# 4) Web arayÃ¼zÃ¼
import gradio as gr
from gradio.themes.base import Base
from gradio.themes.utils import colors, sizes


# --------------------------------------------------------------------------------------------------
# 0) Ortam deÄŸiÅŸkenleri ve LLM yapÄ±landÄ±rmasÄ±
#    - HF Spaces'te Secrets/Variables ile gelir; yerelde .env Ã¼zerinden verilebilir.
# --------------------------------------------------------------------------------------------------
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY", "")
EMBEDDINGS_MODEL = os.getenv("EMBEDDINGS_MODEL", "trmteb/turkish-embedding-model")
GENERATION_MODEL = os.getenv("GENERATION_MODEL", "gemini-2.0-flash")
CHROMA_PERSIST_DIR = os.getenv("CHROMA_PERSIST_DIR", ".chroma")

# STRICT RAG: aÃ§Ä±kâ€‘alan fallback kapalÄ± (yalnÄ±zca tezden yanÄ±t ver).
ALLOW_OPEN_DOMAIN_FALLBACK = os.getenv("ALLOW_OPEN_DOMAIN_FALLBACK", "false").lower() == "true"

# Sayfa filtreleme: PDF sayfa 13-104 arasÄ± tez iÃ§eriÄŸi (1-12: Ã¶n sayfalar, 105+: kaynakÃ§a/ekler)
PDF_PAGE_START = int(os.getenv("PDF_PAGE_START", "13"))
PDF_PAGE_END = int(os.getenv("PDF_PAGE_END", "104"))
# DÃœZELTME: Offset deÄŸeri -12'den 0'a deÄŸiÅŸtirildi (negatif sayfa numaralarÄ± Ã¶nlemek iÃ§in)
PDF_TO_THESIS_OFFSET = int(os.getenv("PDF_TO_THESIS_OFFSET", "0"))

# Gemini istemcisi; API anahtarÄ± zorunludur.
genai.configure(api_key=GOOGLE_API_KEY)

# Ãœretim (generate) parametreleri:
# - temperature: RAG'de dÃ¼ÅŸÃ¼k tutulur (0.0â€“0.3 aralÄ±ÄŸÄ±), kaynaÄŸa sadakat artar.
# - top_p/top_k: Ã–rnekleme Ã§eÅŸitliliÄŸi; varsayÄ±lanlar korunur, gerekirse ayarlanÄ±r.
# - max_output_tokens: CevabÄ±n Ã¼st uzunluÄŸu; kesilme yaÅŸanÄ±rsa artÄ±rÄ±labilir (Ã¶r. 768/1024).
# DÃœZELTME: Token limiti 512'den 1024'e artÄ±rÄ±ldÄ± (cÃ¼mlelerin yarÄ±m kalmasÄ±nÄ± Ã¶nlemek iÃ§in)
GENERATION_CFG = dict(
    temperature=0.25,
    top_p=0.95,
    top_k=40,
    max_output_tokens=1024  # DÃœZELTME: 512 â†’ 1024
)

# YanÄ±t uzunluÄŸu Ã¶n ayarlarÄ± (STRICT RAG) - GERÃ‡EKTEN FARK EDECEK DEÄERLER
RESPONSE_LENGTH_TO_TOKENS = {
    "KÄ±sa": 400,    # Ã‡ok kÄ±sa, Ã¶zet yanÄ±t
    "Orta": 800,    # Orta uzunluk, detaylÄ± yanÄ±t
    "Uzun": 1500    # Ã‡ok uzun, kapsamlÄ± yanÄ±t
}
DEFAULT_RESPONSE_LENGTH = os.getenv("DEFAULT_RESPONSE_LENGTH", "Orta")
# DÃœZELTME: Top-K deÄŸeri 10'dan 5'e dÃ¼ÅŸÃ¼rÃ¼ldÃ¼ - daha az gÃ¼rÃ¼ltÃ¼, daha alakalÄ± sonuÃ§lar
CURRENT_TOP_K = int(os.getenv("CURRENT_TOP_K", "5"))


# --------------------------------------------------------------------------------------------------
# 1) Metin yardÄ±mcÄ±larÄ±
#    - clean_text: metni biÃ§imsel olarak normalize eder.
#    - split_into_chunks: uzun metni kaygan pencere (sliding window) ile kÃ¼Ã§Ã¼k parÃ§alara bÃ¶ler.
# --------------------------------------------------------------------------------------------------
def clean_text(s: str) -> str:
    """
    Metin temizliÄŸi:
    - \x00 gibi bozuk karakterleri kaldÄ±rÄ±r.
    - Birden fazla boÅŸluk/tab'Ä± tek boÅŸluÄŸa indirir.
    - Ã‡oklu boÅŸ satÄ±rlarÄ± sadeleÅŸtirir (3+ â†’ 2).
    Bu temizleme, embedding kalitesini ve okunabilirliÄŸi iyileÅŸtirir.
    """
    s = (s or "").replace("\x00", " ")
    s = re.sub(r"[ \t]+", " ", s)
    s = re.sub(r"\n{3,}", "\n\n", s)
    return s.strip()


def split_into_chunks(text: str, size: int = 800, overlap: int = 120) -> List[str]:
    """
    Metni kelime bazlÄ± parÃ§alara bÃ¶ler.
    Parametreler:
      - size: hedef parÃ§a uzunluÄŸu (kelime)
      - overlap: art arda gelen parÃ§alar arasÄ±ndaki ortak kelime sayÄ±sÄ±
    Not:
      - RAG'de 512â€“800 kelime iyi bir baÅŸlangÄ±Ã§ aralÄ±ÄŸÄ±dÄ±r; overlap 80â€“120 Ã¶nerilir.
    """
    words = (text or "").split()
    if not words:
        return []
    chunks: List[str] = []
    i = 0
    while i < len(words):
        piece = " ".join(words[i:i + size]).strip()
        if piece:
            chunks.append(piece)
        nxt = i + size - overlap
        i = nxt if nxt > i else i + size
    return chunks


def is_valid_page(page_num: int) -> bool:
    """
    Sayfa filtreleme: PDF sayfa 13-104 arasÄ± tez iÃ§eriÄŸi
    (1-12: Ã¶n sayfalar, 105+: kaynakÃ§a/ekler)
    DÃœZELTME: Filtreleme aktif (Kaggle'da tÃ¼m PDF iÅŸlendiÄŸi iÃ§in burada filtre gerekli)
    """
    return PDF_PAGE_START <= page_num <= PDF_PAGE_END


def pdf_to_thesis_page(pdf_page: int) -> int:
    """
    PDF sayfa numarasÄ±nÄ± tez sayfa numarasÄ±na Ã§evirir.
    DÃœZELTME: Offset deÄŸeri 0 olarak ayarlandÄ± (negatif sayfa numaralarÄ± Ã¶nlemek iÃ§in)
    Ã–rnek: PDF sayfa 1 â†’ Tez sayfa 1 (offset 0 ile)
    """
    return pdf_page + PDF_TO_THESIS_OFFSET


# --------------------------------------------------------------------------------------------------
# 2) Embedding saÄŸlayÄ±cÄ±sÄ± ve Chroma vektÃ¶r veritabanÄ±
#    - Embedding: TÃ¼rkÃ§e iÃ§in SentenceTransformers modeli (HF Ã¼zerinden Ã§ekilir).
#    - Chroma: .chroma klasÃ¶rÃ¼ne kalÄ±cÄ± olarak yazar (Space yeniden baÅŸlasa da veri korunur).
# --------------------------------------------------------------------------------------------------
embeddings = HuggingFaceEmbeddings(model_name=EMBEDDINGS_MODEL)

chroma_client = chromadb.PersistentClient(path=CHROMA_PERSIST_DIR)
vectorstore = Chroma(
    client=chroma_client,
    collection_name="docs",
    embedding_function=embeddings
)


# --------------------------------------------------------------------------------------------------
# 3) Ingest fonksiyonlarÄ±
#    - JSONL: her satÄ±r bir kaydÄ± temsil eder â†’ {"content": "...", "meta": {...}}
#    - Parquet: tablo formatÄ±; "content" zorunlu, "meta" opsiyonel (dict) ya da sÃ¼tunlardan derlenir.
#    - Sayfa filtreleme: sadece PDF sayfa 13-104 arasÄ±ndaki parÃ§alar ingest edilir.
# --------------------------------------------------------------------------------------------------
def ingest_jsonl(file_obj) -> str:
    """
    JSONL dosyasÄ±nÄ± satÄ±r satÄ±r okuyup metin + metadata Ã§Ä±karÄ±r ve Chroma'ya ekler.
    DEBUG: Metadata formatÄ± kontrol edilir ve loglanÄ±r.
    DÃœZELTME: Sayfa filtreleme aktif (page_start field'Ä± kullanÄ±lÄ±yor)
    """
    try:
        lines = file_obj.read().decode("utf-8").splitlines()
        texts, metas = [], []
        print(f"ğŸ“„ {len(lines)} satÄ±r okundu")
        
        for ln in lines:
            row = json.loads(ln)
            content = clean_text(row.get("content", ""))
            if not content:
                continue
            meta = row.get("meta", {}) or {}
            
            # DEBUG: Ä°lk 3 metadata'yÄ± kontrol et
            if len(texts) < 3:
                print(f"ğŸ” Metadata {len(texts)+1}: {meta}")
            
            # DÃœZELTME: Sayfa filtreleme AÃ‡IK - page_start field'Ä±nÄ± kullan
            page_num = meta.get("page_start")
            if page_num and not is_valid_page(int(page_num)):
                continue
                
            texts.append(content)
            metas.append(meta)
            
        if not texts:
            return "JSONL boÅŸ ya da geÃ§erli kayÄ±t bulunamadÄ±."
            
        print(f"ğŸ“Š {len(texts)} parÃ§a eklenecek")
        vectorstore.add_texts(texts=texts, metadatas=metas)
        return f"JSONL ingest tamamlandÄ±: {len(texts)} parÃ§a eklendi."
    except Exception as e:
        return f"JSONL ingest hatasÄ±: {e}"


def ingest_parquet(file_obj) -> str:
    """
    Parquet dosyasÄ±nÄ± okuyup "content" ve (varsa) "meta" bilgilerini alÄ±r ve Chroma'ya ekler.
    DEBUG: Metadata formatÄ± kontrol edilir ve loglanÄ±r.
    DÃœZELTME: Sayfa filtreleme aktif (page_start field'Ä± kullanÄ±lÄ±yor)
    """
    try:
        df = pd.read_parquet(file_obj)
        if "content" not in df.columns:
            return "Parquet: 'content' sÃ¼tunu bulunamadÄ±."
        texts, metas = [], []
        for _, r in df.iterrows():
            content = clean_text(str(r["content"]))
            if not content:
                continue
            meta = {}
            if "meta" in df.columns and isinstance(r.get("meta"), dict):
                meta = r.get("meta") or {}
            for key in ["title", "source", "page", "page_start", "page_end"]:
                if key in df.columns:
                    val = r.get(key)
                    if pd.notna(val) and val != "":
                        meta.setdefault(key, val)
            
            # DÃœZELTME: Sayfa filtreleme AÃ‡IK - page_start field'Ä±nÄ± kullan
            page_num = meta.get("page_start")
            if page_num and not is_valid_page(int(page_num)):
                continue
                
            texts.append(content)
            metas.append(meta)
        if not texts:
            return "Parquet: geÃ§erli satÄ±r bulunamadÄ±."
        vectorstore.add_texts(texts=texts, metadatas=metas)
        return f"Parquet ingest tamamlandÄ±: {len(texts)} parÃ§a eklendi."
    except Exception as e:
        return f"Parquet ingest hatasÄ±: {e}"


# --------------------------------------------------------------------------------------------------
# 4) Retrieval ve Prompt oluÅŸturma
#    - retrieve: Sorguya en Ã§ok benzeyen k metin parÃ§asÄ±nÄ± Chroma'dan getirir.
#    - build_prompt: Sistem talimatÄ± + baÄŸlam + soru birleÅŸiminden Gemini istemini kurar.
#    - generate_with_gemini: YanÄ±t Ã¼retir; metin dÃ¶ndÃ¼rÃ¼r.
# --------------------------------------------------------------------------------------------------
# DÃœZELTME: Sistem mesajÄ± - baÄŸlamdaki bilgileri kullan ama Ã§ok katÄ± olma
SYSTEM_MSG = (
    "AÅŸaÄŸÄ±daki baÄŸlam parÃ§alarÄ±nÄ± kullanarak yanÄ±t ver. "
    "BaÄŸlamda verilen TÃœM ilgili bilgileri, tarihleri, isimleri ve detaylarÄ± MUTLAKA dahil et. "
    "EÄŸer baÄŸlamda yeterli bilgi yoksa: 'Bu konu tezde yeterli detayla bulunamadÄ±.' de. "
    "YanÄ±tÄ± tam cÃ¼mlelerle bitir; maddeleri yarÄ±m bÄ±rakma."
)


def retrieve(query: str, k: int):
    """
    Sorgu embedding'i ile Chroma'dan en ilgili k belge parÃ§asÄ±nÄ± getirir.
    DÃœZELTME: Relevance score filtresi eklendi - Ã§ok dÃ¼ÅŸÃ¼k skorlu belgeleri filtrele
    Threshold: 0.35 (daha sÄ±kÄ±, sadece gerÃ§ekten alakalÄ± belgeleri alÄ±r)
    """
    try:
        results = vectorstore.similarity_search_with_relevance_scores(query, k=k)
        # DÃœZELTME: 0.35'in altÄ±ndaki skorlarÄ± filtrele (alakasÄ±z belgeleri at)
        # Not: EÄŸer hiÃ§ belge kalmazsa, en yÃ¼ksek skorlu 2 belgeyi al
        filtered = [(doc, score) for doc, score in results if score >= 0.35]
        
        # EÄŸer hiÃ§bir belge 0.35'i geÃ§emezse, en iyi 2'yi al
        if not filtered and results:
            filtered = results[:2]
            print(f"âš ï¸ HiÃ§ yÃ¼ksek skorlu belge yok, en iyi {len(filtered)} belge kullanÄ±lÄ±yor")
        
        print(f"ğŸ” Toplam {len(results)} belge, {len(filtered)} belge seÃ§ildi")
        for doc, score in filtered[:3]:  # Ä°lk 3'Ã¼ logla
            meta = doc.metadata or {}
            page = page_label(meta)
            print(f"   ğŸ“„ Sayfa {page}, skor: {score:.3f}")
        docs = [doc for doc, _score in filtered]
        return docs
    except Exception as e:
        print(f"âŒ Relevance score hatasÄ±: {e}")
        # Fallback: score olmadan normal arama
        docs = vectorstore.similarity_search(query, k=k)
        return docs


def page_label(meta: dict) -> str:
    """
    Metadata'dan sayfa etiketini Ã§Ä±karÄ±r.
    Ã–ncelik sÄ±rasÄ±: page_label > logical_page > page_start/page_end > page (offset ile)
    """
    # 1) AÃ§Ä±k etiket
    if meta.get("page_label"): 
        return str(meta["page_label"])
    if meta.get("logical_page"):
        return str(meta["logical_page"])
    
    # 2) AralÄ±k (eÄŸer start ve end aynÄ±ysa tek sayfa gÃ¶ster)
    if meta.get("page_start") and meta.get("page_end"):
        start = str(meta["page_start"])
        end = str(meta["page_end"])
        # DÃœZELTME: AynÄ± sayfaysa "16-16" yerine "16" gÃ¶ster
        if start == end:
            return start
        return f"{start}-{end}"
    if meta.get("page_start"):
        return str(meta["page_start"])
    
    # 3) Fallback: PDF sayfasÄ±na offset uygula
    if meta.get("page") is not None:
        try:
            return str(int(meta["page"]) + PDF_TO_THESIS_OFFSET)
        except Exception:
            return str(meta["page"])
    
    return "?"


def build_prompt(query: str, docs, length_choice: str) -> str:
    """
    Gemini'ye verilecek istem (prompt):
      - Sistem talimatÄ± (modelin davranÄ±ÅŸ sÄ±nÄ±rlarÄ±)
      - BaÄŸlam: numaralÄ± satÄ±rlar; kaynak adÄ± ve sayfa bilgisi gÃ¶rÃ¼nÃ¼r
      - KullanÄ±cÄ± sorusu
      - YanÄ±t uzunluÄŸu talimatÄ±
    DÃœZELTME: Sayfa etiketleme sistemi eklendi (belge sayfa numaralarÄ±nÄ± gÃ¶stermek iÃ§in)
    """
    # YanÄ±t uzunluÄŸu talimatÄ±
    length_instructions = {
        "KÄ±sa": "KÄ±sa ve Ã¶z bir yanÄ±t ver. Sadece temel bilgileri belirt.",
        "Orta": "DetaylÄ± ama Ã¶zlÃ¼ bir yanÄ±t ver. Ã–nemli noktalarÄ± aÃ§Ä±kla.",
        "Uzun": "KapsamlÄ± ve detaylÄ± bir yanÄ±t ver. TÃ¼m ilgili bilgileri, Ã¶rnekleri ve aÃ§Ä±klamalarÄ± dahil et."
    }
    
    ctx_lines = []
    for i, d in enumerate(docs, start=1):
        meta = d.metadata or {}
        src = meta.get("source", "unknown")
        # DÃœZELTME: Sayfa etiketleme sistemi kullanÄ±lÄ±yor
        page_display = page_label(meta)
        ctx_lines.append(f"[{i}] ({src} s.{page_display}) {d.page_content}")
    
    context = "\n\n".join(ctx_lines) if ctx_lines else "(baÄŸlam yok)"
    length_instruction = length_instructions.get(length_choice, length_instructions["Orta"])
    
    return f"{SYSTEM_MSG}\n\n{length_instruction}\n\nBaÄŸlam:\n{context}\n\nSoru: {query}\nYanÄ±t:"


def generate_with_gemini(prompt: str, max_tokens: int | None = None) -> str:
    """
    Gemini'den yanÄ±t Ã¼retir ve dÃ¼z metin olarak dÃ¶ndÃ¼rÃ¼r.
    """
    cfg = dict(GENERATION_CFG)
    if max_tokens is not None:
        cfg["max_output_tokens"] = int(max_tokens)
    model = genai.GenerativeModel(GENERATION_MODEL, generation_config=cfg)
    resp = model.generate_content(prompt)
    return (resp.text or "").strip()


# DÃœZELTME: polish_style() fonksiyonu KALDIRILDI
# Sebep: Kaynak bloÄŸunu siliyordu ve yanÄ±tÄ± bozuyordu
# ArtÄ±k yanÄ±t doÄŸrudan kullanÄ±cÄ±ya sunuluyor (kaynak bilgisi korunuyor)


def answer_fn(message: str, history: List[Tuple[str, str]], length_choice: str) -> str:
    """
    ChatInterface tarafÄ±ndan Ã§aÄŸrÄ±lÄ±r.
    DEBUG: Metadata formatÄ± farklÄ± olduÄŸu iÃ§in page_start kullanÄ±lÄ±yor.
    DÃœZELTME: polish_style() kaldÄ±rÄ±ldÄ±, kaynak bloÄŸu artÄ±k korunuyor
    DÃœZELTME: Relevance score kontrolÃ¼ eklendi - alakasÄ±z belgeler filtreleniyor
    DÃœZELTME: BoÅŸ sonuÃ§ kontrolÃ¼ eklendi - baÄŸlamda bilgi yoksa uyarÄ± ver
    """
    try:
        # Basit selamlama ve tez dÄ±ÅŸÄ± sorular iÃ§in kontrol
        simple_greetings = ["merhaba", "selam", "hello", "hi", "nasÄ±lsÄ±n", "iyi misin"]
        if message.lower().strip() in simple_greetings:
            return "Merhaba! Yapay ZekÃ¢ Dil Modelleri tezi hakkÄ±nda sorularÄ±nÄ±zÄ± sorabilirsiniz."
        
        # DEBUG: Retrieve sonuÃ§larÄ±nÄ± kontrol et
        docs = retrieve(message, k=CURRENT_TOP_K)
        print(f"ğŸ” DEBUG: {len(docs)} yÃ¼ksek skorlu belge bulundu")
        
        # DÃœZELTME: EÄŸer hiÃ§ yÃ¼ksek skorlu belge yoksa, tezde bilgi yok demektir
        if not docs:
            return "Bu konu tezde bulunamadÄ± veya sorunuzla yeterince ilgili deÄŸil."

        # DEBUG: Metadata'larÄ± kontrol et
        for i, doc in enumerate(docs[:3]):  # Ä°lk 3 belgeyi kontrol et
            meta = doc.metadata or {}
            print(f"ğŸ“„ Belge {i+1}: page={meta.get('page')}, page_start={meta.get('page_start')}, source={meta.get('source')}")

        prompt = build_prompt(message, docs, length_choice)
        max_tokens = RESPONSE_LENGTH_TO_TOKENS.get(length_choice, RESPONSE_LENGTH_TO_TOKENS["Orta"])
        answer = generate_with_gemini(prompt, max_tokens=max_tokens)

        # DÃœZELTME: Sadece Ã§ok kesin "bulunamadÄ±" cevaplarÄ±nda kaynak gÃ¶sterme
        if not answer:
            return "YanÄ±t Ã¼retilemedi."

        # DEBUG: Kaynak oluÅŸturma
        pages_by_source = {}
        for d in docs:
            m = d.metadata or {}
            display_name = "Yapay ZekÃ¢ Dil Modelleri"
            # DÃœZELTME: Sayfa etiketleme sistemi kullanÄ±lÄ±yor
            page_display = page_label(m)
            print(f"ğŸ” Sayfa etiketi: {page_display}")
            
            # Sayfa numarasÄ±nÄ± kaynak listesine ekle (tekrarlarÄ± Ã¶nlemek iÃ§in set kullanÄ±lÄ±yor)
            if page_display != "?":
                pages_by_source.setdefault(display_name, set()).add(page_display)
                print(f"âœ… Kaynak sayfa: {page_display}")

        print(f"ğŸ“š Kaynak sayfalar: {pages_by_source}")

        # Kaynak bloÄŸu oluÅŸtur
        if pages_by_source:
            def sort_key(p: str):
                # Sayfa numarasÄ±nÄ± Ã§Ä±kar (Ã¶rn: "16" veya "16-17" â†’ "16")
                head = str(p).split("-")[0]
                return int(head) if head.isdigit() else 10**9

            items = []
            for src, pages in pages_by_source.items():
                ordered = ", ".join(sorted(pages, key=sort_key))
                # DÃœZELTME: "s." (sayfa) kÄ±saltmasÄ± kullanÄ±lÄ±yor
                items.append(f"- {src} s. {ordered}")
            
            # Tek kaynak varsa "Kaynak:", birden fazlaysa "Kaynaklar:"
            sources_block = "Kaynak: " + items[0][2:] if len(items) == 1 else "Kaynaklar:\n" + "\n".join(items)
            print(f"ğŸ“ Kaynak bloÄŸu: {sources_block}")
        else:
            # DÃœZELTME: EÄŸer hiÃ§ kaynak yoksa, kaynak bloÄŸu ekleme
            sources_block = ""
            print("âŒ HiÃ§ kaynak sayfasÄ± bulunamadÄ±!")

        # DÃœZELTME: polish_style() KALDIRILDI - kaynak bloÄŸu artÄ±k korunuyor
        # YanÄ±t + kaynak bloÄŸunu doÄŸrudan birleÅŸtir
        if sources_block:
            final_answer = (answer or "YanÄ±t Ã¼retilemedi.").rstrip() + "\n\n" + sources_block
        else:
            final_answer = (answer or "YanÄ±t Ã¼retilemedi.").rstrip()
        
        return final_answer

    except Exception as e:
        print(f"âŒ Hata: {e}")
        return f"Hata: {e}"


# --------------------------------------------------------------------------------------------------
# DEBUG FONKSIYONU - Metadata formatÄ±nÄ± kontrol eder
# --------------------------------------------------------------------------------------------------
def debug_metadata():
    """Startup'ta metadata formatÄ±nÄ± kontrol et"""
    print("\n" + "="*80)
    print("ğŸ” METADATA DEBUG BAÅLIYOR")
    print("="*80)
    
    # JSONL kontrolÃ¼
    print("\nğŸ“„ JSONL ANALÄ°ZÄ°:")
    try:
        with open("data/processed_docs.jsonl", "r", encoding="utf-8") as f:
            lines = f.readlines()[:3]  # Ä°lk 3 satÄ±r
            print(f"   Toplam satÄ±r: {len(lines)}")
            for i, line in enumerate(lines, 1):
                record = json.loads(line)
                meta = record.get("meta", {})
                content_preview = record.get("content", "")[:80]
                print(f"\n   KayÄ±t {i}:")
                print(f"   Ä°Ã§erik: {content_preview}...")
                print(f"   Metadata keys: {list(meta.keys())}")
                for k, v in meta.items():
                    print(f"      â€¢ {k}: {v} (type: {type(v).__name__})")
    except Exception as e:
        print(f"   âŒ JSONL hata: {e}")
    
    # Parquet kontrolÃ¼
    print("\nğŸ“Š PARQUET ANALÄ°ZÄ°:")
    try:
        df = pd.read_parquet("data/processed_docs.parquet")
        print(f"   SÃ¼tunlar: {list(df.columns)}")
        print(f"   Toplam satÄ±r: {len(df)}")
        
        if len(df) > 0:
            row = df.iloc[0]
            print(f"\n   Ä°lk satÄ±r:")
            
            # Meta sÃ¼tunu varsa
            if "meta" in df.columns:
                meta_val = row.get("meta")
                if isinstance(meta_val, dict):
                    print(f"   meta dict keys: {list(meta_val.keys())}")
                    for k, v in meta_val.items():
                        print(f"      â€¢ {k}: {v}")
                else:
                    print(f"   meta deÄŸeri dict deÄŸil: {type(meta_val)}")
            
            # DoÄŸrudan sÃ¼tunlar
            for col in ["page", "page_start", "page_end", "page_label", "source"]:
                if col in df.columns:
                    val = row.get(col)
                    if pd.notna(val):
                        print(f"   {col}: {val} (type: {type(val).__name__})")
                        
    except Exception as e:
        print(f"   âŒ Parquet hata: {e}")
    
    # PDF kontrolÃ¼
    print("\nğŸ“• PDF KONTROLÃœ:")
    try:
        import os
        pdf_path = "data/tez.pdf"
        if os.path.exists(pdf_path):
            size_mb = os.path.getsize(pdf_path) / (1024 * 1024)
            print(f"   âœ… PDF mevcut: {pdf_path}")
            print(f"   Boyut: {size_mb:.2f} MB")
            if size_mb < 0.1:
                print(f"   âš ï¸  UYARI: Dosya Ã§ok kÃ¼Ã§Ã¼k!")
        else:
            print(f"   âŒ PDF bulunamadÄ±: {pdf_path}")
    except Exception as e:
        print(f"   âŒ PDF kontrol hatasÄ±: {e}")
    
    print("\n" + "="*80)
    print("âœ… DEBUG TAMAMLANDI")
    print("="*80 + "\n")


# --------------------------------------------------------------------------------------------------
# 5) Otomatik ingest (deploy esnasÄ±nda hiÃ§bir kullanÄ±cÄ± aksiyonu gerektirmeden veri yÃ¼kler)
#    - data/processed_docs.jsonl
#    - data/processed_docs.parquet
# --------------------------------------------------------------------------------------------------
def auto_ingest_from_repo() -> str:
    """
    Uygulama baÅŸlarken veri klasÃ¶rÃ¼ndeki dosyalarÄ± ingest eder.
    DEBUG: Her adÄ±m loglanÄ±r ve kontrol edilir.
    """
    # DEBUG: Ã–nce metadata formatÄ±nÄ± kontrol et
    debug_metadata()
    
    logs = []
    print("ğŸš€ Auto ingest baÅŸlÄ±yor...")
    
    try:
        p = "data/processed_docs.jsonl"
        if os.path.exists(p):
            print(f"âœ… JSONL dosyasÄ± bulundu: {p}")
            with open(p, "rb") as f:
                result = ingest_jsonl(f)
                logs.append(result)
                print(f"ğŸ“Š JSONL sonucu: {result}")
        else:
            print(f"âŒ JSONL dosyasÄ± bulunamadÄ±: {p}")
    except Exception as e:
        logs.append(f"AUTO JSONL hata: {e}")
        print(f"âŒ JSONL hatasÄ±: {e}")

    try:
        p = "data/processed_docs.parquet"
        if os.path.exists(p):
            print(f"âœ… Parquet dosyasÄ± bulundu: {p}")
            with open(p, "rb") as f:
                result = ingest_parquet(f)
                logs.append(result)
                print(f"ğŸ“Š Parquet sonucu: {result}")
        else:
            print(f"âŒ Parquet dosyasÄ± bulunamadÄ±: {p}")
    except Exception as e:
        logs.append(f"AUTO Parquet hata: {e}")
        print(f"âŒ Parquet hatasÄ±: {e}")

    final_result = "\n".join([lg for lg in logs if lg])
    print(f"ğŸ¯ Final ingest sonucu: {final_result}")
    return final_result


# --------------------------------------------------------------------------------------------------
# 6) Gradio arayÃ¼zÃ¼
#    - Bu sÃ¼rÃ¼mde dosya yÃ¼kleme kapalÄ±dÄ±r; veri aÃ§Ä±lÄ±ÅŸta otomatik yÃ¼klenir.
#    - Sol panel: Tez indirme + estetik iÃ§indekiler + yanÄ±t uzunluÄŸu seÃ§imi
#    - SaÄŸ panel: Sohbet arayÃ¼zÃ¼
# --------------------------------------------------------------------------------------------------
EXAMPLES = [
    "Tezin temel problem tanÄ±mÄ± nedir?",
    "Transformer mimarisinin temel yapÄ±taÅŸlarÄ± nelerdir?",
    "Kendine dikkat (self-attention) nasÄ±l Ã§alÄ±ÅŸÄ±r?",
    "RNN/LSTM/GRU'nun karÅŸÄ±laÅŸtÄ±ÄŸÄ± temel sorunlar nelerdir?",
    "GPT ve BERT hangi gÃ¶revlerde daha baÅŸarÄ±lÄ±dÄ±r?",
    "Temel doÄŸal dil iÅŸleme teknikleri nelerdir?",
    "Yapay zekÃ¢ nasÄ±l tanÄ±mlanÄ±r? KapsadÄ±ÄŸÄ± alt alanlar nelerdir?",
    "Ã‡ok modlu modellerin Ã¶ne Ã§Ä±kan Ã¶rnekleri hangileri?",
    "Etik bÃ¶lÃ¼mÃ¼nde hangi riskler tartÄ±ÅŸÄ±lÄ±yor?",
    "Gelecek Ã§alÄ±ÅŸmalar iÃ§in Ã¶neriler nelerdir?",
    "Tezi bana anlatÄ±r mÄ±sÄ±n?",
]

# Tema: aÃ§Ä±k, yÃ¼ksek okunabilirlik ve sade anahtarlar (Gradio ile uyumlu)
theme = Base(
    primary_hue=colors.blue,
    secondary_hue=colors.violet,
    text_size=sizes.text_md,
    radius_size=sizes.radius_md,
).set(
    body_background_fill="#f7f9fc",
    block_background_fill="#ffffff",
    border_color_primary="#e3e8f0",
    body_text_color="#0f172a",
    block_title_text_color="#0b1220",
    link_text_color="#1d4ed8",
    button_primary_background_fill="#2563eb",
    button_primary_text_color="#ffffff",
    input_background_fill="#ffffff",
    input_border_color="#cbd5e1",
)

# CSS (estetik kartlar ve diÄŸer stiller)
css = """
.gr-example {
  background: #f1f5ff !important;
  color: #0f172a !important;
  border: 1px solid #dbe2f3 !important;
}
.gr-example:hover {
  background: #e6eeff !important;
  border-color: #c1cff5 !important;
}
input, textarea, .gr-text-input input, .gr-textbox textarea {
  color: #0f172a !important;
}
::placeholder {
  color: #6b7280 !important;
}
button.primary:hover {
  background-color: #1d4ed8 !important;
}
/* Sohbet balonlarÄ± */
.gr-chatbot .message.user { background:#eef2ff !important; border-color:#dbe2f3 !important; }
.gr-chatbot .message.bot  { background:#ffffff !important; border-color:#e3e8f0 !important; }
/* Ä°Ã§indekiler â€” estetik kartlar */
.toc-container { 
  display: flex; 
  flex-direction: column; 
  gap: 8px; 
  margin: 10px 0; 
}
.toc-card {
  padding: 12px 14px;
  border-radius: 8px;
  background: linear-gradient(135deg, #ffffff 0%, #f8fafc 100%);
  border: 1px solid #e2e8f0;
  box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
  transition: all 0.2s ease;
}
.toc-card:hover {
  border-color: #3b82f6;
  box-shadow: 0 4px 12px rgba(59, 130, 246, 0.15);
  transform: translateY(-1px);
}
.toc-header {
  display: flex;
  justify-content: space-between;
  align-items: center;
  font-weight: 600;
  color: #1e293b;
  margin-bottom: 4px;
}
.toc-page {
  color: #64748b;
  font-size: 0.9em;
  font-weight: 500;
}
.toc-subtitle {
  color: #475569;
  font-size: 0.85em;
  line-height: 1.4;
  margin-top: 6px;
}
"""

# ----- Basit sohbet adÄ±mÄ±: kullanÄ±cÄ± mesajÄ± â†’ yanÄ±t; input'u temizle -----
def chat_step(user_message: str, history: list[tuple[str, str]], length_choice: str):
    msg = (user_message or "").strip()
    if not msg:
        return history, ""
    bot_reply = answer_fn(msg, history=history or [], length_choice=length_choice)
    new_history = (history or []) + [(msg, bot_reply)]
    return new_history, ""


with gr.Blocks(title="Yapay ZekÃ¢ Dil Modelleri â€¢ KaynaklÄ± Soruâ€‘Cevap", theme=theme, css=css, fill_height=True) as demo:
    # Ana baÅŸlÄ±k
    gr.Markdown(
        """
        <div style="padding:10px 0 4px 0;">
          <h2 style="margin:0;color:#0b1220;">Yapay ZekÃ¢ Dil Modelleri â€” Tez AsistanÄ±</h2>
          <div style="color:#334155">
            Bu arayÃ¼z, 'Yapay ZekÃ¢ Dil Modelleri' tezi temel alÄ±narak sorularÄ±nÄ±za yanÄ±t verir; ilgili pasajlarÄ± bulur ve kaynak sayfalarÄ±yla birlikte sunar.
          </div>
        </div>
        """,
    )

    with gr.Row():
        # Sol panel: Tez indirme + estetik iÃ§indekiler + yanÄ±t uzunluÄŸu seÃ§imi
        with gr.Column(scale=1, min_width=400):
            gr.Markdown("### ğŸ“„ Tez DokÃ¼manÄ±")
            # DÃœZELTME: DoÄŸru dosya adÄ± (gerÃ§ek dosya adÄ±)
            gr.DownloadButton(label="ğŸ“„ Tezi Ä°ndir (PDF)", value="data/tez.pdf")

            gr.Markdown("### ğŸ“š Ä°Ã§indekiler")
            gr.HTML(
                """
                <div class="toc-container">
                  <div class="toc-card">
                    <div class="toc-header">
                      <span>1. GÄ°RÄ°Å</span>
                    </div>
                  </div>
                  <div class="toc-card">
                    <div class="toc-header">
                      <span>2. YAPAY ZEKÃ‚ VE DOÄAL DÄ°L Ä°ÅLEME</span>
                    </div>
                  </div>
                  <div class="toc-card">
                    <div class="toc-header">
                      <span>3. DÄ°L MODELLEMEDE ML ve DL</span>
                    </div>
                  </div>
                  <div class="toc-card">
                    <div class="toc-header">
                      <span>4. DÄ°L MODELLERÄ°</span>
                    </div>
                  </div>
                  <div class="toc-card">
                    <div class="toc-header">
                      <span>5. TRANSFORMER TABANLI MODELLER</span>
                    </div>
                  </div>
                  <div class="toc-card">
                    <div class="toc-header">
                      <span>6. GÃœNCEL YÃ–NELÄ°MLER ve ETÄ°K</span>
                    </div>
                  </div>
                  <div class="toc-card">
                    <div class="toc-header">
                      <span>7. SONUÃ‡ ve DEÄERLENDÄ°RME</span>
                    </div>
                  </div>
                </div>
                """
            )

            # YanÄ±t uzunluÄŸu seÃ§imi
            length_choice = gr.Radio(
                choices=["KÄ±sa", "Orta", "Uzun"], 
                value=DEFAULT_RESPONSE_LENGTH, 
                label="YanÄ±t uzunluÄŸu"
            )

        # SaÄŸ panel: Sohbet arayÃ¼zÃ¼
        with gr.Column(scale=2):
            chatbot = gr.Chatbot(height=500, avatar_images=(None, None))
            input_box = gr.Textbox(placeholder="Sorunuzu yazÄ±n ve Enter'a basÄ±n...", scale=1)
            send_btn = gr.Button("GÃ¶nder", variant="primary")

            # YanÄ±t uzunluÄŸu seÃ§imini chat_step'e parametre olarak geÃ§
            send_btn.click(
                chat_step, 
                inputs=[input_box, chatbot, length_choice], 
                outputs=[chatbot, input_box]
            )
            input_box.submit(
                chat_step, 
                inputs=[input_box, chatbot, length_choice], 
                outputs=[chatbot, input_box]
            )

            with gr.Row():
                for q in EXAMPLES:
                    gr.Button(q, variant="secondary", scale=1).click(
                        lambda s=q: s, outputs=input_box
                    ).then(
                        chat_step, 
                        inputs=[input_box, chatbot, length_choice], 
                        outputs=[chatbot, input_box]
                    )

    # Uygulama aÃ§Ä±lÄ±ÅŸÄ±nda otomatik ingest (logu UI'da gÃ¶stermiyoruz)
    _ = auto_ingest_from_repo()


# Yerel geliÅŸtirme iÃ§in (HF Spaces'te launch Ã§aÄŸrÄ±sÄ± gerekmez)
if __name__ == "__main__":
    demo.launch(server_port=7860, share=False)